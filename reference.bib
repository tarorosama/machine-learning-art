
@article{kingma_adam:_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for ﬁrst-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efﬁcient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the inﬁnity norm.},
	language = {en},
	urldate = {2019-12-29},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
	file = {Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:/Users/yisu/Zotero/storage/FGPDCFEG/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf},
}

@article{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://arxiv.org/abs/1502.03167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
	urldate = {2019-12-29},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv: 1502.03167},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/Y77SDB6S/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/EXE5ZABB/1502.html:text/html},
}

@article{ng_machine_nodate,
	title = {Machine {Learning} {Yearning}-{Draft}},
	language = {en},
	author = {Ng, Andrew},
	pages = {118},
	file = {Ng - Machine Learning Yearning-Draft.pdf:/Users/yisu/Zotero/storage/FMA43DB6/Ng - Machine Learning Yearning-Draft.pdf:application/pdf},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	language = {en},
	number = {7553},
	urldate = {2019-12-28},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444},
	file = {LeCun et al. - 2015 - Deep learning.pdf:/Users/yisu/Zotero/storage/6STAESIF/LeCun et al. - 2015 - Deep learning.pdf:application/pdf},
}

@article{dean_deep_nodate,
	title = {The {Deep} {Learning} {Revolution} and {Its} {Implications} for {Computer} {Architecture} and {Chip} {Design}},
	abstract = {The past decade has seen a remarkable series of advances in machine learning, and in particular deep learning approaches based on artificial neural networks, to improve our abilities to build more accurate systems across a broad range of areas, including computer vision, speech recognition, language translation, and natural language understanding tasks. This paper is a companion paper to a keynote talk at the 2020 International Solid-State Circuits Conference (ISSCC) discussing some of the advances in machine learning, and their implications on the kinds of computational devices we need to build, especially in the post-Moore’s Law-era. It also discusses some of the ways that machine learning may also be able to help with some aspects of the circuit design process. Finally, it provides a sketch of at least one interesting direction towards much larger-scale multi-task models that are sparsely activated and employ much more dynamic, example- and task-based routing than the machine learning models of today.},
	language = {en},
	author = {Dean, Jeffrey},
	pages = {17},
	file = {Dean - The Deep Learning Revolution and Its Implications .pdf:/Users/yisu/Zotero/storage/SNJL9S4U/Dean - The Deep Learning Revolution and Its Implications .pdf:application/pdf},
}

@article{he_automl_2021,
	title = {{AutoML}: {A} {Survey} of the {State}-of-the-{Art}},
	volume = {212},
	issn = {09507051},
	shorttitle = {{AutoML}},
	url = {http://arxiv.org/abs/1908.00709},
	doi = {10.1016/j.knosys.2020.106622},
	abstract = {Deep learning (DL) techniques have penetrated all aspects of our lives and brought us great convenience. However, building a high-quality DL system for a specific task highly relies on human expertise, hindering the applications of DL to more areas. Automated machine learning (AutoML) becomes a promising solution to build a DL system without human assistance, and a growing number of researchers focus on AutoML. In this paper, we provide a comprehensive and up-to-date review of the state-of-the-art (SOTA) in AutoML. First, we introduce AutoML methods according to the pipeline, covering data preparation, feature engineering, hyperparameter optimization, and neural architecture search (NAS). We focus more on NAS, as it is currently very hot sub-topic of AutoML. We summarize the performance of the representative NAS algorithms on the CIFAR-10 and ImageNet datasets and further discuss several worthy studying directions of NAS methods: one/two-stage NAS, one-shot NAS, and joint hyperparameter and architecture optimization. Finally, we discuss some open problems of the existing AutoML methods for future research.},
	urldate = {2022-04-24},
	journal = {Knowledge-Based Systems},
	author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
	month = jan,
	year = {2021},
	note = {arXiv: 1908.00709},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {106622},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/CNEPXZAN/He et al. - 2021 - AutoML A Survey of the State-of-the-Art.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/W6II9CCK/1908.html:text/html},
}

@article{dulac-arnold_challenges_nodate,
	title = {Challenges of {Real}-{World} {Reinforcement} {Learning}},
	abstract = {Reinforcement learning (RL) has proven its worth in a series of artiﬁcial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are often hard to leverage in real-world systems due to a series of assumptions that are rarely satisﬁed in practice. We present a set of nine unique challenges that must be addressed to productionize RL to real world problems. For each of these challenges, we specify the exact meaning of the challenge, present some approaches from the literature, and specify some metrics for evaluating that challenge. An approach that addresses all nine challenges would be applicable to a large number of real world problems. We also present an example domain that has been modiﬁed to present these challenges as a testbed for practical RL research.},
	language = {en},
	author = {Dulac-Arnold, Gabriel and Mankowitz, Daniel and Hester, Todd},
	pages = {14},
	file = {Dulac-Arnold et al. - Challenges of Real-World Reinforcement Learning.pdf:/Users/yisu/Zotero/storage/V9JURCLC/Dulac-Arnold et al. - Challenges of Real-World Reinforcement Learning.pdf:application/pdf},
}

@inproceedings{zhao_recommending_2019,
	address = {Copenhagen Denmark},
	title = {Recommending what video to watch next: a multitask ranking system},
	isbn = {978-1-4503-6243-6},
	shorttitle = {Recommending what video to watch next},
	url = {https://dl.acm.org/doi/10.1145/3298689.3346997},
	doi = {10.1145/3298689.3346997},
	abstract = {In this paper, we introduce a large scale multi-objective ranking system for recommending what video to watch next on an industrial video sharing platform. The system faces many real-world challenges, including the presence of multiple competing ranking objectives, as well as implicit selection biases in user feedback. To tackle these challenges, we explored a variety of soft-parameter sharing techniques such as Multi-gate Mixture-of-Experts so as to efciently optimize for multiple ranking objectives. Additionally, we mitigated the selection biases by adopting a Wide \& Deep framework. We demonstrated that our proposed techniques can lead to substantial improvements on recommendation quality on one of the world’s largest video sharing platforms.},
	language = {en},
	urldate = {2022-04-05},
	booktitle = {Proceedings of the 13th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Zhao, Zhe and Hong, Lichan and Wei, Li and Chen, Jilin and Nath, Aniruddh and Andrews, Shawn and Kumthekar, Aditee and Sathiamoorthy, Maheswaran and Yi, Xinyang and Chi, Ed},
	month = sep,
	year = {2019},
	pages = {43--51},
	file = {Zhao et al. - 2019 - Recommending what video to watch next a multitask.pdf:/Users/yisu/Zotero/storage/W7XNL4IL/Zhao et al. - 2019 - Recommending what video to watch next a multitask.pdf:application/pdf},
}

@article{humayun_no_2022,
	title = {No {More} {Than} 6ft {Apart}: {Robust} {K}-{Means} via {Radius} {Upper} {Bounds}},
	shorttitle = {No {More} {Than} 6ft {Apart}},
	url = {http://arxiv.org/abs/2203.02502},
	abstract = {Centroid based clustering methods such as k-means, k-medoids and k-centers are heavily applied as a go-to tool in exploratory data analysis. In many cases, those methods are used to obtain representative centroids of the data manifold for visualization or summarization of a dataset. Real world datasets often contain inherent abnormalities, e.g., repeated samples and sampling bias, that manifest imbalanced clustering. We propose to remedy such a scenario by introducing a maximal radius constraint \$r\$ on the clusters formed by the centroids, i.e., samples from the same cluster should not be more than \$2r\$ apart in terms of \${\textbackslash}ell\_2\$ distance. We achieve this constraint by solving a semi-definite program, followed by a linear assignment problem with quadratic constraints. Through qualitative results, we show that our proposed method is robust towards dataset imbalances and sampling artifacts. To the best of our knowledge, ours is the first constrained k-means clustering method with hard radius constraints. Codes at https://bit.ly/kmeans-constrained},
	language = {en},
	urldate = {2022-03-07},
	journal = {arXiv:2203.02502 [cs]},
	author = {Humayun, Ahmed Imtiaz and Balestriero, Randall and Kyrillidis, Anastasios and Baraniuk, Richard},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.02502},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Humayun et al. - 2022 - No More Than 6ft Apart Robust K-Means via Radius .pdf:/Users/yisu/Zotero/storage/ZFKBKGRS/Humayun et al. - 2022 - No More Than 6ft Apart Robust K-Means via Radius .pdf:application/pdf},
}

@article{muhamed_ctr-bert_nodate,
	title = {{CTR}-{BERT}: {Cost}-effective knowledge distillation for billion-parameter teacher models},
	abstract = {While pre-trained large language models (LLM) like BERT have achieved state-ofthe-art in several NLP tasks, their performance on tasks with additional grounding e.g. with numeric and categorical features is less studied. In this paper, we study the application of pre-trained LLM for Click-through-rate (CTR) prediction for product advertisement in e-commerce. This is challenging because the model needs to a) learn from language as well as tabular data features, b) maintain low-latency ({\textless}5 ms) at inference time, and c) adapt to constantly changing advertisement distribution. We ﬁrst show that scaling the pre-trained language model to 1.5 billion parameters signiﬁcantly improves performance over conventional CTR baselines. We then present CTR-BERT, a novel lightweight cache-friendly factorized model for CTR prediction that consists of twin-structured BERT-like encoders for text with a mechanism for late fusion for text and tabular features. We train the CTR-BERT model using cross-architecture knowledge distillation (KD) and empirically study the interaction between KD and distribution shift in this setting, by a) experimenting with pre-training, distillation pre-ﬁnetuning and ﬁne-tuning strategies b) factorizing features based on their distribution shift time scales, that allows the model to readily adapt and be re-trained. Finally, we show that CTR-BERT signiﬁcantly outperforms a traditional CTR baseline with a 2.3\% relative ROC-AUC lift in ofﬂine experiments and a 2\% CTR lift in an online experiment.},
	language = {en},
	author = {Muhamed, Aashiq and Keivanloo, Iman and Perera, Sujan and Mracek, James and Xu, Yi and Cui, Qingjun and Rajagopalan, Santosh and Zeng, Belinda and Chilimbi, Trishul},
	pages = {7},
	file = {Muhamed et al. - CTR-BERT Cost-effective knowledge distillation fo.pdf:/Users/yisu/Zotero/storage/IUGBEUUX/Muhamed et al. - CTR-BERT Cost-effective knowledge distillation fo.pdf:application/pdf},
}

@inproceedings{dallmann_case_2021,
	address = {Amsterdam Netherlands},
	title = {A {Case} {Study} on {Sampling} {Strategies} for {Evaluating} {Neural} {Sequential} {Item} {Recommendation} {Models}},
	isbn = {978-1-4503-8458-2},
	url = {https://dl.acm.org/doi/10.1145/3460231.3475943},
	doi = {10.1145/3460231.3475943},
	abstract = {At the present time, sequential item recommendation models are compared by calculating metrics on a small item subset (target set) to speed up computation. The target set contains the relevant item and a set of negative items that are sampled from the full item set. Two well-known strategies to sample negative items are uniform random sampling and sampling by popularity to better approximate the item frequency distribution in the dataset. Most recently published papers on sequential item recommendation rely on sampling by popularity to compare the evaluated models. However, recent work has already shown that an evaluation with uniform random sampling may not be consistent with the full ranking, that is, the model ranking obtained by evaluating a metric using the full item set as target set, which raises the question whether the ranking obtained by sampling by popularity is equal to the full ranking. In this work, we re-evaluate current state-of-the-art sequential recommender models from the point of view, whether these sampling strategies have an impact on the final ranking of the models. We therefore train four recently proposed sequential recommendation models on five widely known datasets. For each dataset and model, we employ three evaluation strategies. First, we compute the full model ranking. Then we evaluate all models on a target set sampled by the two different sampling strategies, uniform random sampling and sampling by popularity with the commonly used target set size of 100, compute the model ranking for each strategy and compare them with each other. Additionally, we vary the size of the sampled target set. Overall, we find that both sampling strategies can produce inconsistent rankings compared with the full ranking of the models. Furthermore, both sampling by popularity and uniform random sampling do not consistently produce the same ranking when compared over different sample sizes. Our results suggest that like uniform random sampling, rankings obtained by sampling by popularity do not equal the full ranking of recommender models and therefore both should be avoided in favor of the full ranking when establishing state-of-the-art.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {Fifteenth {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Dallmann, Alexander and Zoller, Daniel and Hotho, Andreas},
	month = sep,
	year = {2021},
	pages = {505--514},
	file = {Dallmann et al. - 2021 - A Case Study on Sampling Strategies for Evaluating.pdf:/Users/yisu/Zotero/storage/ZTVJU5I6/Dallmann et al. - 2021 - A Case Study on Sampling Strategies for Evaluating.pdf:application/pdf},
}

@inproceedings{peng_learning_2021,
	address = {Amsterdam Netherlands},
	title = {Learning an {Adaptive} {Meta} {Model}-{Generator} for {Incrementally} {Updating} {Recommender} {Systems}},
	isbn = {978-1-4503-8458-2},
	url = {https://dl.acm.org/doi/10.1145/3460231.3474239},
	doi = {10.1145/3460231.3474239},
	abstract = {Recommender Systems (RSs) in real-world applications often deal with billions of user interactions daily. To capture the most recent trends effectively, it is common to update the model incrementally using only the newly arrived data. However, this may impede the model’s ability to retain long-term information due to the potential overfitting and forgetting issues. To address this problem, we propose a novel Adaptive Sequential Model Generation (ASMG) framework, which generates a better serving model from a sequence of historical models via a meta generator. For the design of the meta generator, we propose to employ Gated Recurrent Units (GRUs) to leverage its ability to capture the long-term dependencies. We further introduce some novel strategies to apply together with the GRU meta generator, which not only improve its computational efficiency but also enable more accurate sequential modeling. By instantiating the model-agnostic framework on a general deep learning-based RS model, we demonstrate that our method achieves state-of-the-art performance on three public datasets and one industrial dataset.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {Fifteenth {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Peng, Danni and Pan, Sinno Jialin and Zhang, Jie and Zeng, Anxiang},
	month = sep,
	year = {2021},
	pages = {411--421},
	file = {Peng et al. - 2021 - Learning an Adaptive Meta Model-Generator for Incr.pdf:/Users/yisu/Zotero/storage/EFZ8JKUM/Peng et al. - 2021 - Learning an Adaptive Meta Model-Generator for Incr.pdf:application/pdf},
}

@inproceedings{tagliabue_you_2021,
	address = {Amsterdam Netherlands},
	title = {You {Do} {Not} {Need} a {Bigger} {Boat}: {Recommendations} at {Reasonable} {Scale} in a ({Mostly}) {Serverless} and {Open} {Stack}},
	isbn = {978-1-4503-8458-2},
	shorttitle = {You {Do} {Not} {Need} a {Bigger} {Boat}},
	url = {https://dl.acm.org/doi/10.1145/3460231.3474604},
	doi = {10.1145/3460231.3474604},
	abstract = {We argue that immature data pipelines are preventing a large portion of industry practitioners from leveraging the latest research on recommender systems. We propose our template data stack for machine learning at “reasonable scale”, and show how many challenges are solved by embracing a serverless paradigm. Leveraging our experience, we detail how modern open source tools can provide a pipeline processing terabytes of data with minimal infrastructure work.},
	language = {en},
	urldate = {2021-10-04},
	booktitle = {Fifteenth {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Tagliabue, Jacopo},
	month = sep,
	year = {2021},
	pages = {598--600},
	file = {Tagliabue - 2021 - You Do Not Need a Bigger Boat Recommendations at .pdf:/Users/yisu/Zotero/storage/43CC8Y67/Tagliabue - 2021 - You Do Not Need a Bigger Boat Recommendations at .pdf:application/pdf},
}

@article{coviello_scalable_nodate,
	title = {A scalable model for online contextual music recommendations},
	language = {en},
	author = {Coviello, Emanuele and Ellis, Katherine and Moerchen, Fabian},
	pages = {9},
	file = {Coviello et al. - A scalable model for online contextual music recom.pdf:/Users/yisu/Zotero/storage/EKRHVQ6D/Coviello et al. - A scalable model for online contextual music recom.pdf:application/pdf},
}

@inproceedings{villermet_follow_2021,
	address = {Amsterdam Netherlands},
	title = {Follow the guides: disentangling human and algorithmic curation in online music consumption},
	isbn = {978-1-4503-8458-2},
	shorttitle = {Follow the guides},
	url = {https://dl.acm.org/doi/10.1145/3460231.3474269},
	doi = {10.1145/3460231.3474269},
	language = {en},
	urldate = {2021-10-03},
	booktitle = {Fifteenth {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Villermet, Quentin and Poiroux, Jérémie and Moussallam, Manuel and Louail, Thomas and Roth, Camille},
	month = sep,
	year = {2021},
	pages = {380--389},
	file = {Villermet et al. - 2021 - Follow the guides disentangling human and algorit.pdf:/Users/yisu/Zotero/storage/HU42C43I/Villermet et al. - 2021 - Follow the guides disentangling human and algorit.pdf:application/pdf},
}

@inproceedings{gutierrez_granada_recommendations_2021,
	address = {Amsterdam Netherlands},
	title = {Recommendations at {Videoland}},
	isbn = {978-1-4503-8458-2},
	url = {https://dl.acm.org/doi/10.1145/3460231.3474617},
	doi = {10.1145/3460231.3474617},
	abstract = {With the largest commercial TV channels in the Netherlands, RTL plays an important role in society. We reach over 85\% of Dutch people on a weekly basis, spending on average around 45 minutes a day with us and our content. We feel a part of all of the Netherlands. We actively pick up the challenge to show a diverse and representative view in the media.},
	language = {en},
	urldate = {2021-10-03},
	booktitle = {Fifteenth {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Gutierrez Granada, Mateo and Odijk, Daan},
	month = sep,
	year = {2021},
	pages = {580--582},
	file = {Gutierrez Granada and Odijk - 2021 - Recommendations at Videoland.pdf:/Users/yisu/Zotero/storage/5HCZTAPV/Gutierrez Granada and Odijk - 2021 - Recommendations at Videoland.pdf:application/pdf},
}

@inproceedings{zhou_impact_2010,
	address = {Melbourne, Australia},
	title = {The impact of {YouTube} recommendation system on video views},
	isbn = {978-1-4503-0483-2},
	url = {http://portal.acm.org/citation.cfm?doid=1879141.1879193},
	doi = {10.1145/1879141.1879193},
	abstract = {Hosting a collection of millions of videos, YouTube oﬀers several features to help users discover the videos of their interest. For example, YouTube provides video search, related video recommendation and front page highlight. The understanding of how these features drive video views is useful for creating a strategy to drive video popularity. In this paper, we perform a measurement study on data sets crawled from YouTube and ﬁnd that the related video recommendation, which recommends the videos that are related to the video a user is watching, is one of the most important view sources of videos. Despite the fact that the YouTube video search is the number one source of views in aggregation, the related video recommendation is the main source of views for the majority of the videos on YouTube. Furthermore, our results reveal that there is a strong correlation between the view count of a video and the average view count of its top referrer videos. This implies that a video has a higher chance to become popular when it is placed on the related video recommendation lists of popular videos. We also ﬁnd that the click through rate from a video to its related videos is high and the position of a video in a related video list plays a critical role in the click through rate. Finally, our evaluation of the impact of the related video recommendation system on the diversity of video views indicates that the current recommendation system helps to increase the diversity of video views in aggregation.},
	language = {en},
	urldate = {2021-10-03},
	booktitle = {Proceedings of the 10th annual conference on {Internet} measurement - {IMC} '10},
	publisher = {ACM Press},
	author = {Zhou, Renjie and Khemmarat, Samamon and Gao, Lixin},
	year = {2010},
	pages = {404},
	file = {Zhou et al. - 2010 - The impact of YouTube recommendation system on vid.pdf:/Users/yisu/Zotero/storage/7H353VJN/Zhou et al. - 2010 - The impact of YouTube recommendation system on vid.pdf:application/pdf},
}

@misc{noauthor_aws_nodate,
	title = {{AWS} {Management} {Console}},
	url = {https://console.aws.amazon.com/console/home?region=us-east-1#},
	urldate = {2021-09-29},
	file = {AWS Management Console:/Users/yisu/Zotero/storage/25WJGRNK/home.html:text/html},
}

@inproceedings{breck_ml_2017,
	address = {Boston, MA},
	title = {The {ML} test score: {A} rubric for {ML} production readiness and technical debt reduction},
	isbn = {978-1-5386-2715-0},
	shorttitle = {The {ML} test score},
	url = {http://ieeexplore.ieee.org/document/8258038/},
	doi = {10.1109/BigData.2017.8258038},
	abstract = {Creating reliable, production-level machine learning systems brings on a host of concerns not found in small toy examples or even large ofﬂine research experiments. Testing and monitoring are key considerations for ensuring the production-readiness of an ML system, and for reducing technical debt of ML systems. But it can be difﬁcult to formulate speciﬁc tests, given that the actual prediction behavior of any given model is difﬁcult to specify a priori. In this paper, we present 28 speciﬁc tests and monitoring needs, drawn from experience with a wide range of production ML systems to help quantify these issues and present an easy to follow road-map to improve production readiness and pay down ML technical debt.},
	language = {en},
	urldate = {2021-09-27},
	booktitle = {2017 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	publisher = {IEEE},
	author = {Breck, Eric and Cai, Shanqing and Nielsen, Eric and Salib, Michael and Sculley, D.},
	month = dec,
	year = {2017},
	pages = {1123--1132},
	file = {Breck et al. - 2017 - The ML test score A rubric for ML production read.pdf:/Users/yisu/Zotero/storage/H3IFRATD/Breck et al. - 2017 - The ML test score A rubric for ML production read.pdf:application/pdf},
}

@inproceedings{xian_ex3_2021,
	address = {Amsterdam Netherlands},
	title = {{EX3}: {Explainable} {Attribute}-aware {Item}-set {Recommendations}},
	isbn = {978-1-4503-8458-2},
	shorttitle = {{EX3}},
	url = {https://dl.acm.org/doi/10.1145/3460231.3474240},
	doi = {10.1145/3460231.3474240},
	language = {en},
	urldate = {2021-09-16},
	booktitle = {Fifteenth {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Xian, Yikun and Zhao, Tong and Li, Jin and Chan, Jim and Kan, Andrey and Ma, Jun and Dong, Xin Luna and Faloutsos, Christos and Karypis, George and Muthukrishnan, S. and Zhang, Yongfeng},
	month = sep,
	year = {2021},
	pages = {484--494},
	file = {Xian et al. - 2021 - EX3 Explainable Attribute-aware Item-set Recommen.pdf:/Users/yisu/Zotero/storage/7PUD4ESV/Xian et al. - 2021 - EX3 Explainable Attribute-aware Item-set Recommen.pdf:application/pdf},
}

@article{stucchio_bayesian_nodate,
	title = {Bayesian {A}/{B} {Testing} at {VWO}},
	abstract = {Before joining Visual Website Optimizer, I ran A/B testing at AOL Patch and acted as an statistical consultant. Whenever an A/B test concluded, people would approach me and ask questions about the results. One of the most common questions I was asked was “what is the probability that version B is better than version A?” Unfortunately, at the time I was using frequentist testing methods, and I was completely unable to provide an answer to that question. In fact, I was unable to answer most of the questions I was asked, and instead had to give unintuitive alternative statistics that didn’t really address the question.},
	language = {en},
	author = {Stucchio, Chris},
	pages = {33},
	file = {Stucchio - Bayesian AB Testing at VWO.pdf:/Users/yisu/Zotero/storage/5TBHPFMS/Stucchio - Bayesian AB Testing at VWO.pdf:application/pdf},
}

@inproceedings{burges_learning_2005,
	address = {Bonn, Germany},
	title = {Learning to rank using gradient descent},
	isbn = {978-1-59593-180-1},
	url = {http://portal.acm.org/citation.cfm?doid=1102351.1102363},
	doi = {10.1145/1102351.1102363},
	abstract = {We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce RankNet, an implementation of these ideas using a neural network to model the underlying ranking function. We present test results on toy data and on data from a commercial internet search engine.},
	language = {en},
	urldate = {2021-09-13},
	booktitle = {Proceedings of the 22nd international conference on {Machine} learning  - {ICML} '05},
	publisher = {ACM Press},
	author = {Burges, Chris and Shaked, Tal and Renshaw, Erin and Lazier, Ari and Deeds, Matt and Hamilton, Nicole and Hullender, Greg},
	year = {2005},
	pages = {89--96},
	file = {Burges et al. - 2005 - Learning to rank using gradient descent.pdf:/Users/yisu/Zotero/storage/YV8C6T5E/Burges et al. - 2005 - Learning to rank using gradient descent.pdf:application/pdf},
}

@article{chapelle_empirical_nodate,
	title = {An {Empirical} {Evaluation} of {Thompson} {Sampling}},
	abstract = {Thompson sampling is one of oldest heuristic to address the exploration / exploitation trade-off, but it is surprisingly unpopular in the literature. We present here some empirical results using Thompson sampling on simulated and real data, and show that it is highly competitive. And since this heuristic is very easy to implement, we argue that it should be part of the standard baselines to compare against.},
	language = {en},
	author = {Chapelle, Olivier and Li, Lihong},
	pages = {9},
	file = {Chapelle and Li - An Empirical Evaluation of Thompson Sampling.pdf:/Users/yisu/Zotero/storage/7U8VMQ8N/Chapelle and Li - An Empirical Evaluation of Thompson Sampling.pdf:application/pdf},
}

@article{sambasivan_everyone_2021,
	title = {``{Everyone} wants to do the model work, not the data work'': {Data} {Cascades} in {High}-{Stakes} {AI}},
	abstract = {AI models are increasingly applied in high-stakes domains like health and conservation. Data quality carries an elevated significance in high-stakes AI due to its heightened downstream impact, impacting predictions like cancer detection, wildlife poaching, and loan allocations. Paradoxically, data is the most under-valued and de-glamorised aspect of AI. In this paper, we report on data practices in high-stakes AI, from interviews with 53 AI practitioners in India, East and West African countries, and USA. We define, identify, and present empirical evidence on Data Cascades—compounding events causing negative, downstream effects from data issues—triggered by conventional AI/ML practices that undervalue data quality. Data cascades are pervasive (92\% prevalence), invisible, delayed, but often avoidable. We discuss HCI opportunities in designing and incentivizing data excellence as a first-class citizen of AI, resulting in safer and more robust systems for all.},
	language = {en},
	author = {Sambasivan, Nithya and Kapania, Shivani and Highfill, Hannah and Akrong, Diana and Paritosh, Praveen and Aroyo, Lora},
	year = {2021},
	pages = {15},
	file = {Sambasivan et al. - 2021 - ``Everyone wants to do the model work, not the dat.pdf:/Users/yisu/Zotero/storage/LCUHTYVC/Sambasivan et al. - 2021 - ``Everyone wants to do the model work, not the dat.pdf:application/pdf},
}

@article{li_federated_2020,
	title = {Federated {Learning}: {Challenges}, {Methods}, and {Future} {Directions}},
	volume = {37},
	issn = {1053-5888, 1558-0792},
	shorttitle = {Federated {Learning}},
	url = {http://arxiv.org/abs/1908.07873},
	doi = {10.1109/MSP.2020.2975749},
	abstract = {Federated learning involves training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data localized. Training in heterogeneous and potentially massive networks introduces novel challenges that require a fundamental departure from standard approaches for large-scale machine learning, distributed optimization, and privacy-preserving data analysis. In this article, we discuss the unique characteristics and challenges of federated learning, provide a broad overview of current approaches, and outline several directions of future work that are relevant to a wide range of research communities.},
	language = {en},
	number = {3},
	urldate = {2021-04-28},
	journal = {IEEE Signal Processing Magazine},
	author = {Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
	month = may,
	year = {2020},
	note = {arXiv: 1908.07873},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	pages = {50--60},
	file = {Li et al. - 2020 - Federated Learning Challenges, Methods, and Futur.pdf:/Users/yisu/Zotero/storage/3ZPCUBU5/Li et al. - 2020 - Federated Learning Challenges, Methods, and Futur.pdf:application/pdf},
}

@article{lin_structured_2017,
	title = {A {Structured} {Self}-attentive {Sentence} {Embedding}},
	url = {http://arxiv.org/abs/1703.03130},
	abstract = {This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what speciﬁc parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author proﬁling, sentiment classiﬁcation and textual entailment. Results show that our model yields a signiﬁcant performance gain compared to other sentence embedding methods in all of the 3 tasks.},
	language = {en},
	urldate = {2021-04-27},
	journal = {arXiv:1703.03130 [cs]},
	author = {Lin, Zhouhan and Feng, Minwei and Santos, Cicero Nogueira dos and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio, Yoshua},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.03130},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {Lin et al. - 2017 - A Structured Self-attentive Sentence Embedding.pdf:/Users/yisu/Zotero/storage/QDYLNFDK/Lin et al. - 2017 - A Structured Self-attentive Sentence Embedding.pdf:application/pdf},
}

@article{stergiou_distributed_nodate,
	title = {Distributed {Negative} {Sampling} for {Word} {Embeddings}},
	abstract = {Word2Vec recently popularized dense vector word representations as ﬁxed-length features for machine learning algorithms and is in widespread use today. In this paper we investigate one of its core components, Negative Sampling, and propose efﬁcient distributed algorithms that allow us to scale to vocabulary sizes of more than 1 billion unique words and corpus sizes of more than 1 trillion words.},
	language = {en},
	author = {Stergiou, Stergios and Straznickas, Zygimantas and Wu, Rolina and Tsioutsiouliklis, Kostas},
	pages = {7},
	file = {Stergiou et al. - Distributed Negative Sampling for Word Embeddings.pdf:/Users/yisu/Zotero/storage/6VXXFVFL/Stergiou et al. - Distributed Negative Sampling for Word Embeddings.pdf:application/pdf},
}

@inproceedings{shi_attention-based_2018,
	address = {Torino Italy},
	title = {Attention-based {Adaptive} {Model} to {Unify} {Warm} and {Cold} {Starts} {Recommendation}},
	isbn = {978-1-4503-6014-2},
	url = {https://dl.acm.org/doi/10.1145/3269206.3271710},
	doi = {10.1145/3269206.3271710},
	abstract = {Nowadays, recommender systems provide essential web services on the Internet. There are mainly two categories of traditional recommendation algorithms: Content-Based (CB) and Collaborative Filtering (CF). CF methods make recommendations mainly according to the historical feedback information. They usually perform better when there is sufficient feedback information but less successful on new users and items, which is called the “cold-start” problem. However, CB methods help in this scenario because of using content information. To take both advantages of CF and CB, how to combine them is a challenging issue. To the best of our knowledge, little previous work has been done to solve the problem in one unified recommendation model. In this work, we study how to integrate CF and CB, which utilizes both types of information in model-level but not in result-level and makes recommendations adaptively. A novel attention-based model named Attentional Content\&Collaborate Model (ACCM) is proposed. Attention mechanism helps adaptively adjust for each user-item pair from which source information the recommendation is made. Especially, a “cold sampling” learning strategy is designed to handle the cold-start problem. Experimental results on two benchmark datasets show that the ACCM performs better on both warm and cold tests compared to the state-of-the-art algorithms.},
	language = {en},
	urldate = {2020-12-11},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {ACM},
	author = {Shi, Shaoyun and Zhang, Min and Liu, Yiqun and Ma, Shaoping},
	month = oct,
	year = {2018},
	pages = {127--136},
	file = {Shi et al. - 2018 - Attention-based Adaptive Model to Unify Warm and C.pdf:/Users/yisu/Zotero/storage/C29K3WBM/Shi et al. - 2018 - Attention-based Adaptive Model to Unify Warm and C.pdf:application/pdf},
}

@article{yao_online_2020,
	title = {Online {Structured} {Meta}-learning},
	url = {http://arxiv.org/abs/2010.11545},
	abstract = {Learning quickly is of great importance for machine intelligence deployed in online platforms. With the capability of transferring knowledge from learned tasks, meta-learning has shown its effectiveness in online scenarios by continuously updating the model with the learned prior. However, current online meta-learning algorithms are limited to learn a globally-shared meta-learner, which may lead to sub-optimal results when the tasks contain heterogeneous information that are distinct by nature and difﬁcult to share. We overcome this limitation by proposing an online structured meta-learning (OSML) framework. Inspired by the knowledge organization of human and hierarchical feature representation, OSML explicitly disentangles the meta-learner as a meta-hierarchical graph with different knowledge blocks. When a new task is encountered, it constructs a meta-knowledge pathway by either utilizing the most relevant knowledge blocks or exploring new blocks. Through the meta-knowledge pathway, the model is able to quickly adapt to the new task. In addition, new knowledge is further incorporated into the selected blocks. Experiments on three datasets demonstrate the effectiveness and interpretability of our proposed framework in the context of both homogeneous and heterogeneous tasks.},
	language = {en},
	urldate = {2020-12-06},
	journal = {arXiv:2010.11545 [cs]},
	author = {Yao, Huaxiu and Zhou, Yingbo and Mahdavi, Mehrdad and Li, Zhenhui and Socher, Richard and Xiong, Caiming},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.11545},
	keywords = {Computer Science - Machine Learning},
	file = {Yao et al. - 2020 - Online Structured Meta-learning.pdf:/Users/yisu/Zotero/storage/JN7MMBXT/Yao et al. - 2020 - Online Structured Meta-learning.pdf:application/pdf},
}

@article{mcinerney_explore_2018,
	title = {Explore, {Exploit}, and {Explain}: {Personalizing} {Explainable} {Recommendations} with {Bandits}},
	abstract = {The multi-armed bandit is an important framework for balancing exploration with exploitation in recommendation. Exploitation recommends content (e.g., products, movies, music playlists) with the highest predicted user engagement and has traditionally been the focus of recommender systems. Exploration recommends content with uncertain predicted user engagement for the purpose of gathering more information. The importance of exploration has been recognized in recent years, particularly in settings with new users, new items, non-stationary preferences and attributes. In parallel, explaining recommendations (“recsplanations”) is crucial if users are to understand their recommendations. Existing work has looked at bandits and explanations independently. We provide the first method that combines both in a principled manner. In particular, our method is able to jointly (1) learn which explanations each user responds to; (2) learn the best content to recommend for each user; and (3) balance exploration with exploitation to deal with uncertainty. Experiments with historical log data and tests with live production traffic in a large-scale music recommendation service show a significant improvement in user engagement.},
	language = {en},
	author = {McInerney, James and Lacker, Benjamin and Hansen, Samantha and Higley, Karl and Bouchard, Hugues and Gruson, Alois and Mehrotra, Rishabh},
	year = {2018},
	pages = {9},
	file = {McInerney et al. - 2018 - Explore, Exploit, and Explain Personalizing Expla.pdf:/Users/yisu/Zotero/storage/USLS9CTG/McInerney et al. - 2018 - Explore, Exploit, and Explain Personalizing Expla.pdf:application/pdf},
}

@inproceedings{mcinerney_explore_2018-1,
	address = {Vancouver British Columbia Canada},
	title = {Explore, exploit, and explain: personalizing explainable recommendations with bandits},
	isbn = {978-1-4503-5901-6},
	shorttitle = {Explore, exploit, and explain},
	url = {https://dl.acm.org/doi/10.1145/3240323.3240354},
	doi = {10.1145/3240323.3240354},
	abstract = {The multi-armed bandit is an important framework for balancing exploration with exploitation in recommendation. Exploitation recommends content (e.g., products, movies, music playlists) with the highest predicted user engagement and has traditionally been the focus of recommender systems. Exploration recommends content with uncertain predicted user engagement for the purpose of gathering more information. The importance of exploration has been recognized in recent years, particularly in settings with new users, new items, non-stationary preferences and attributes. In parallel, explaining recommendations (“recsplanations”) is crucial if users are to understand their recommendations. Existing work has looked at bandits and explanations independently. We provide the first method that combines both in a principled manner. In particular, our method is able to jointly (1) learn which explanations each user responds to; (2) learn the best content to recommend for each user; and (3) balance exploration with exploitation to deal with uncertainty. Experiments with historical log data and tests with live production traffic in a large-scale music recommendation service show a significant improvement in user engagement.},
	language = {en},
	urldate = {2020-11-01},
	booktitle = {Proceedings of the 12th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {McInerney, James and Lacker, Benjamin and Hansen, Samantha and Higley, Karl and Bouchard, Hugues and Gruson, Alois and Mehrotra, Rishabh},
	month = sep,
	year = {2018},
	pages = {31--39},
}

@inproceedings{ribeiro_why_2016,
	address = {San Francisco California USA},
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939778},
	doi = {10.1145/2939672.2939778},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classiﬁer in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the ﬂexibility of these methods by explaining diﬀerent models for text (e.g. random forests) and image classiﬁcation (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classiﬁer, and identifying why a classiﬁer should not be trusted.},
	language = {en},
	urldate = {2020-10-30},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = aug,
	year = {2016},
	pages = {1135--1144},
	file = {Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:/Users/yisu/Zotero/storage/W7B4MRVK/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:application/pdf},
}

@incollection{chapelle_empirical_2011,
	title = {An {Empirical} {Evaluation} of {Thompson} {Sampling}},
	url = {http://papers.nips.cc/paper/4321-an-empirical-evaluation-of-thompson-sampling.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 24},
	publisher = {Curran Associates, Inc.},
	author = {Chapelle, Olivier and Li, Lihong},
	editor = {Shawe-Taylor, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
	year = {2011},
	pages = {2249--2257},
}

@inproceedings{guo_deep_2020,
	address = {Virtual Event Brazil},
	title = {Deep {Bayesian} {Bandits}: {Exploring} in {Online} {Personalized} {Recommendations}},
	isbn = {978-1-4503-7583-2},
	shorttitle = {Deep {Bayesian} {Bandits}},
	url = {https://dl.acm.org/doi/10.1145/3383313.3412214},
	doi = {10.1145/3383313.3412214},
	language = {en},
	urldate = {2020-10-21},
	booktitle = {Fourteenth {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Guo, Dalin and Ktena, Sofia Ira and Myana, Pranay Kumar and Huszar, Ferenc and Shi, Wenzhe and Tejani, Alykhan and Kneier, Michael and Das, Sourav},
	month = sep,
	year = {2020},
	pages = {456--461},
	file = {Guo et al. - 2020 - Deep Bayesian Bandits Exploring in Online Persona.pdf:/Users/yisu/Zotero/storage/ABDSBMDN/Guo et al. - 2020 - Deep Bayesian Bandits Exploring in Online Persona.pdf:application/pdf},
}

@inproceedings{covington_deep_2016,
	address = {Boston Massachusetts USA},
	title = {Deep {Neural} {Networks} for {YouTube} {Recommendations}},
	isbn = {978-1-4503-4035-9},
	url = {https://dl.acm.org/doi/10.1145/2959100.2959190},
	doi = {10.1145/2959100.2959190},
	abstract = {YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: ﬁrst, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous userfacing impact.},
	language = {en},
	urldate = {2020-10-14},
	booktitle = {Proceedings of the 10th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Covington, Paul and Adams, Jay and Sargin, Emre},
	month = sep,
	year = {2016},
	pages = {191--198},
	file = {Covington et al. - 2016 - Deep Neural Networks for YouTube Recommendations.pdf:/Users/yisu/Zotero/storage/N7CGGXQN/Covington et al. - 2016 - Deep Neural Networks for YouTube Recommendations.pdf:application/pdf},
}

@article{lee_learning_nodate,
	title = {Learning with {Positive} and {Unlabeled} {Examples} {Using} {Weighted} {Logistic} {Regression}},
	abstract = {The problem of learning with positive and unlabeled examples arises frequently in retrieval applications. We transform the problem into a problem of learning with noise by labeling all unlabeled examples as negative and use a linear function to learn from the noisy examples. To learn a linear function with noise, we perform logistic regression after weighting the examples to handle noise rates of greater than a half. With appropriate regularization, the cost function of the logistic regression problem is convex, allowing the problem to be solved efﬁciently. We also propose a performance measure that can be estimated from positive and unlabeled examples for evaluating retrieval performance. The measure, which is proportional to the product of precision and recall, can be used with a validation set to select regularization parameters for logistic regression. Experiments on a text classiﬁcation corpus show that the methods proposed are effective.},
	language = {en},
	author = {Lee, Wee Sun and Liu, Bing},
	pages = {8},
	file = {Lee and Liu - Learning with Positive and Unlabeled Examples Usin.pdf:/Users/yisu/Zotero/storage/FLV7V8YB/Lee and Liu - Learning with Positive and Unlabeled Examples Usin.pdf:application/pdf},
}

@inproceedings{hu_collaborative_2008,
	address = {Pisa, Italy},
	title = {Collaborative {Filtering} for {Implicit} {Feedback} {Datasets}},
	isbn = {978-0-7695-3502-9},
	url = {http://ieeexplore.ieee.org/document/4781121/},
	doi = {10.1109/ICDM.2008.22},
	abstract = {A common task of recommender systems is to improve customer experience through personalized recommendations based on prior implicit feedback. These systems passively track different sorts of user behavior, such as purchase history, watching habits and browsing activity, in order to model user preferences. Unlike the much more extensively researched explicit feedback, we do not have any direct input from the users regarding their preferences. In particular, we lack substantial evidence on which products consumer dislike. In this work we identify unique properties of implicit feedback datasets. We propose treating the data as indication of positive and negative preference associated with vastly varying conﬁdence levels. This leads to a factor model which is especially tailored for implicit feedback recommenders. We also suggest a scalable optimization procedure, which scales linearly with the data size. The algorithm is used successfully within a recommender system for television shows. It compares favorably with well tuned implementations of other known methods. In addition, we offer a novel way to give explanations to recommendations given by this factor model.},
	language = {en},
	urldate = {2020-09-18},
	booktitle = {2008 {Eighth} {IEEE} {International} {Conference} on {Data} {Mining}},
	publisher = {IEEE},
	author = {Hu, Yifan and Koren, Yehuda and Volinsky, Chris},
	month = dec,
	year = {2008},
	pages = {263--272},
	file = {Hu et al. - 2008 - Collaborative Filtering for Implicit Feedback Data.pdf:/Users/yisu/Zotero/storage/VJK6LXHR/Hu et al. - 2008 - Collaborative Filtering for Implicit Feedback Data.pdf:application/pdf},
}

@article{wang_collaborative_2015,
	title = {Collaborative {Deep} {Learning} for {Recommender} {Systems}},
	url = {http://arxiv.org/abs/1409.2944},
	abstract = {Collaborative ﬁltering (CF) is a successful approach commonly used by many recommender systems. Conventional CF-based methods use the ratings given to items by users as the sole source of information for learning to make recommendation. However, the ratings are often very sparse in many applications, causing CF-based methods to degrade signiﬁcantly in their recommendation performance. To address this sparsity problem, auxiliary information such as item content information may be utilized. Collaborative topic regression (CTR) is an appealing recent method taking this approach which tightly couples the two components that learn from two diﬀerent sources of information. Nevertheless, the latent representation learned by CTR may not be very eﬀective when the auxiliary information is very sparse. To address this problem, we generalize recent advances in deep learning from i.i.d. input to non-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian model called collaborative deep learning (CDL), which jointly performs deep representation learning for the content information and collaborative ﬁltering for the ratings (feedback) matrix. Extensive experiments on three real-world datasets from diﬀerent domains show that CDL can signiﬁcantly advance the state of the art.},
	language = {en},
	urldate = {2020-09-18},
	journal = {arXiv:1409.2944 [cs, stat]},
	author = {Wang, Hao and Wang, Naiyan and Yeung, Dit-Yan},
	month = jun,
	year = {2015},
	note = {arXiv: 1409.2944},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing, Computer Science - Information Retrieval},
	file = {Wang et al. - 2015 - Collaborative Deep Learning for Recommender System.pdf:/Users/yisu/Zotero/storage/4CHHIHF8/Wang et al. - 2015 - Collaborative Deep Learning for Recommender System.pdf:application/pdf},
}

@article{blundell_weight_2015,
	title = {Weight {Uncertainty} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1505.05424},
	abstract = {We introduce a new, efﬁcient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classiﬁcation. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
	language = {en},
	urldate = {2020-08-18},
	journal = {arXiv:1505.05424 [cs, stat]},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	month = may,
	year = {2015},
	note = {arXiv: 1505.05424},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Bayesian Neural Network},
	file = {Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf:/Users/yisu/Zotero/storage/5PM4UH25/Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf:application/pdf},
}

@article{cheng_wide_2016,
	title = {Wide \& {Deep} {Learning} for {Recommender} {Systems}},
	url = {http://arxiv.org/abs/1606.07792},
	abstract = {Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classiﬁcation problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are eﬀective and interpretable, while generalization requires more feature engineering eﬀort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide \& Deep learning—jointly trained wide linear models and deep neural networks—to combine the beneﬁts of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide \& Deep signiﬁcantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.},
	language = {en},
	urldate = {2020-08-10},
	journal = {arXiv:1606.07792 [cs, stat]},
	author = {Cheng, Heng-Tze and Koc, Levent and Harmsen, Jeremiah and Shaked, Tal and Chandra, Tushar and Aradhye, Hrishi and Anderson, Glen and Corrado, Greg and Chai, Wei and Ispir, Mustafa and Anil, Rohan and Haque, Zakaria and Hong, Lichan and Jain, Vihan and Liu, Xiaobing and Shah, Hemal},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.07792},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval},
	file = {Cheng et al. - 2016 - Wide & Deep Learning for Recommender Systems.pdf:/Users/yisu/Zotero/storage/DVYSY6MN/Cheng et al. - 2016 - Wide & Deep Learning for Recommender Systems.pdf:application/pdf},
}

@article{thompson_likelihood_1933,
	title = {On the {Likelihood} that {One} {Unknown} {Probability} {Exceeds} {Another} in {View} of the {Evidence} of {Two} {Samples}},
	volume = {25},
	issn = {00063444},
	url = {https://www.jstor.org/stable/2332286?origin=crossref},
	doi = {10.2307/2332286},
	language = {en},
	number = {3/4},
	urldate = {2020-08-04},
	journal = {Biometrika},
	author = {Thompson, William R.},
	month = dec,
	year = {1933},
	pages = {285},
	file = {Thompson - 1933 - On the Likelihood that One Unknown Probability Exc.pdf:/Users/yisu/Zotero/storage/Y6FK6K5H/Thompson - 1933 - On the Likelihood that One Unknown Probability Exc.pdf:application/pdf},
}

@article{tibshirani_valerie_nodate,
	title = {Valerie and {Patrick} {Hastie}},
	language = {en},
	author = {Tibshirani, Sami and Friedman, Harry},
	pages = {764},
	file = {Tibshirani and Friedman - Valerie and Patrick Hastie.pdf:/Users/yisu/Zotero/storage/IKLN6FVC/Tibshirani and Friedman - Valerie and Patrick Hastie.pdf:application/pdf},
}

@inproceedings{swaminathan_counterfactual_2015,
	address = {Florence, Italy},
	title = {Counterfactual {Risk} {Minimization}},
	isbn = {978-1-4503-3473-0},
	url = {http://dl.acm.org/citation.cfm?doid=2740908.2742564},
	doi = {10.1145/2740908.2742564},
	abstract = {We develop a learning principle and an eﬃcient algorithm for batch learning from logged bandit feedback. This learning setting is ubiquitous in online systems (e.g., ad placement, web search, recommendation), where an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads). We ﬁrst address the counterfactual nature of the learning problem (Bottou et al., 2013) through propensity scoring. Next, we prove generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator. In analogy to the Structural Risk Minimization principle of Wapnik and Tscherwonenkis (1979), these constructive bounds give rise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM can be used to derive a new learning method—called Policy Optimizer for Exponential Models (POEM)—for learning stochastic linear rules for structured output prediction. We present a decomposition of the POEM objective that enables eﬃcient stochastic gradient optimization. The eﬀectiveness and eﬃciency of POEM is evaluated on several simulated multi-label classiﬁcation problems, as well as on a real-world information retrieval problem. The empirical results show that the CRM objective implemented in POEM provides improved robustness and generalization performance compared to the state-of-the-art.},
	language = {en},
	urldate = {2020-07-29},
	booktitle = {Proceedings of the 24th {International} {Conference} on {World} {Wide} {Web} - {WWW} '15 {Companion}},
	publisher = {ACM Press},
	author = {Swaminathan, Adith and Joachims, Thorsten},
	year = {2015},
	pages = {939--941},
	file = {Swaminathan and Joachims - 2015 - Counterfactual Risk Minimization.pdf:/Users/yisu/Zotero/storage/TNUJRPDN/Swaminathan and Joachims - 2015 - Counterfactual Risk Minimization.pdf:application/pdf},
}

@article{swaminathan_self-normalized_nodate,
	title = {The {Self}-{Normalized} {Estimator} for {Counterfactual} {Learning}},
	abstract = {This paper identiﬁes a severe problem of the counterfactual risk estimator typically used in batch learning from logged bandit feedback (BLBF), and proposes the use of an alternative estimator that avoids this problem. In the BLBF setting, the learner does not receive full-information feedback like in supervised learning, but observes feedback only for the actions taken by a historical policy. This makes BLBF algorithms particularly attractive for training online systems (e.g., ad placement, web search, recommendation) using their historical logs. The Counterfactual Risk Minimization (CRM) principle [1] offers a general recipe for designing BLBF algorithms. It requires a counterfactual risk estimator, and virtually all existing works on BLBF have focused on a particular unbiased estimator. We show that this conventional estimator suffers from a propensity overﬁtting problem when used for learning over complex hypothesis spaces. We propose to replace the risk estimator with a self-normalized estimator, showing that it neatly avoids this problem. This naturally gives rise to a new learning algorithm – Normalized Policy Optimizer for Exponential Models (Norm-POEM) – for structured output prediction using linear rules. We evaluate the empirical effectiveness of NormPOEM on several multi-label classiﬁcation problems, ﬁnding that it consistently outperforms the conventional estimator.},
	language = {en},
	author = {Swaminathan, Adith and Joachims, Thorsten},
	pages = {9},
	file = {Swaminathan and Joachims - The Self-Normalized Estimator for Counterfactual L.pdf:/Users/yisu/Zotero/storage/4LYH2MVW/Swaminathan and Joachims - The Self-Normalized Estimator for Counterfactual L.pdf:application/pdf},
}

@article{gutierrez_causal_nodate,
	title = {Causal {Inference} and {Uplift} {Modeling} {A} review of the literature},
	abstract = {Uplift modeling refers to the set of techniques used to model the incremental impact of an action or treatment on a customer outcome. Uplift modeling is therefore both a Causal Inference problem and a Machine Learning one. The literature on uplift is split into 3 main approaches–the Two-Model approach, the Class Transformation approach and modeling uplift directly. Unfortunately, in the absence of a common framework of causal inference and notation, it can be quite di cult to assess those three methods. In this paper, we use the Rubin (1974) model of causal inference and its modern “econometrics” notation to provide a clear comparison of the three approaches and generalize one of them. To our knowledge, this is the ﬁrst paper that provides a uniﬁed review of the uplift literature. Moreover, our paper contributes to the literature by showing that, in the limit, minimizing the Mean Square Error (MSE) formula with respect to a causal e↵ect estimator is equivalent to minimizing the MSE in which the unobserved treatment e↵ect is replaced by a modiﬁed target variable. Finally, we hope that our paper will be of use to researchers interested in applying Machine Learning techniques to causal inference problems in a business context as well as in other ﬁelds: medicine, sociology or economics.},
	language = {en},
	author = {Gutierrez, Pierre and Gerardy, Jean-Yves},
	pages = {14},
	file = {Gutierrez and Gerardy - Causal Inference and Uplift Modeling A review of t.pdf:/Users/yisu/Zotero/storage/WP6I9ACL/Gutierrez and Gerardy - Causal Inference and Uplift Modeling A review of t.pdf:application/pdf},
}

@article{rubin_estimating_1974,
	title = {Estimating causal effects of treatments in randomized and nonrandomized studies.},
	volume = {66},
	issn = {0022-0663},
	url = {http://content.apa.org/journals/edu/66/5/688},
	doi = {10.1037/h0037350},
	language = {en},
	number = {5},
	urldate = {2020-07-27},
	journal = {Journal of Educational Psychology},
	author = {Rubin, Donald B.},
	year = {1974},
	pages = {688--701},
	file = {Rubin - 1974 - Estimating causal effects of treatments in randomi.pdf:/Users/yisu/Zotero/storage/33MR4S9P/Rubin - 1974 - Estimating causal effects of treatments in randomi.pdf:application/pdf},
}

@article{welling_bayesian_nodate,
	title = {Bayesian {Learning} via {Stochastic} {Gradient} {Langevin} {Dynamics}},
	abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overﬁtting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a “sampling threshold” and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
	language = {en},
	author = {Welling, Max and Teh, Yee Whye},
	pages = {8},
	file = {Welling and Teh - Bayesian Learning via Stochastic Gradient Langevin.pdf:/Users/yisu/Zotero/storage/V9G4HL84/Welling and Teh - Bayesian Learning via Stochastic Gradient Langevin.pdf:application/pdf},
}

@article{belnap_propensities_2007,
	title = {Propensities and probabilities},
	volume = {38},
	issn = {13552198},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1355219807000469},
	doi = {10.1016/j.shpsb.2006.09.003},
	abstract = {Popper’s introduction of ‘‘propensity’’ was intended to provide a solid conceptual foundation for objective single-case probabilities. By considering the partly opposed contributions of Humphreys and Miller and Salmon, it is argued that when properly understood, propensities can in fact be understood as objective single-case causal probabilities of transitions between concrete events. The chief claim is that propensities are well-explicated by describing how they ﬁt into the existing formal theory of branching space-times, which is simultaneously indeterministic and causal. Several problematic examples, some commonsense and some quantum-mechanical, are used to make clear the advantages of invoking branching space-times theory in coming to understand propensities.},
	language = {en},
	number = {3},
	urldate = {2020-07-24},
	journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
	author = {Belnap, Nuel},
	month = sep,
	year = {2007},
	pages = {593--625},
	file = {Belnap - 2007 - Propensities and probabilities.pdf:/Users/yisu/Zotero/storage/PR3DA2JB/Belnap - 2007 - Propensities and probabilities.pdf:application/pdf},
}

@article{rosenbaum_central_nodate,
	title = {The central role of the propensity score in observational studies for causal effects},
	abstract = {The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a twodimensional plot.},
	language = {en},
	author = {Rosenbaum, Paul R and Rubin, Donald B},
	pages = {15},
	file = {Rosenbaum and Rubin - The central role of the propensity score in observ.pdf:/Users/yisu/Zotero/storage/4GZPCIQR/Rosenbaum and Rubin - The central role of the propensity score in observ.pdf:application/pdf},
}

@article{rubin_estimating_2006,
	title = {Estimating the {Causal} {Effects} of {Marketing} {Interventions} {Using} {Propensity} {Score} {Methodology}},
	volume = {21},
	issn = {0883-4237},
	url = {http://arxiv.org/abs/math/0609201},
	doi = {10.1214/088342306000000259},
	abstract = {Propensity score methods were proposed by Rosenbaum and Rubin [Biometrika 70 (1983) 41–55] as central tools to help assess the causal eﬀects of interventions. Since their introduction more than two decades ago, they have found wide application in a variety of areas, including medical research, economics, epidemiology and education, especially in those situations where randomized experiments are either diﬃcult to perform, or raise ethical questions, or would require extensive delays before answers could be obtained. In the past few years, the number of published applications using propensity score methods to evaluate medical and epidemiological interventions has increased dramatically. Nevertheless, thus far, we believe that there have been few applications of propensity score methods to evaluate marketing interventions (e.g., advertising, promotions), where the tradition is to use generally inappropriate techniques, which focus on the prediction of an outcome from background characteristics and an indicator for the intervention using statistical tools such as least-squares regression, data mining, and so on. With these techniques, an estimated parameter in the model is used to estimate some global “causal” eﬀect. This practice can generate grossly incorrect answers that can be self-perpetuating: polishing the Ferraris rather than the Jeeps “causes” them to continue to win more races than the Jeeps ⇔ visiting the high-prescribing doctors rather than the low-prescribing doctors “causes” them to continue to write more prescriptions. This presentation will take “causality” seriously, not just as a casual concept implying some predictive association in a data set, and will illustrate why propensity score methods are generally superior in practice to the standard predictive approaches for estimating causal eﬀects.},
	language = {en},
	number = {2},
	urldate = {2020-07-24},
	journal = {Statistical Science},
	author = {Rubin, Donald B. and Waterman, Richard P.},
	month = may,
	year = {2006},
	note = {arXiv: math/0609201},
	keywords = {Mathematics - Statistics Theory},
	pages = {206--222},
	file = {Rubin and Waterman - 2006 - Estimating the Causal Effects of Marketing Interve.pdf:/Users/yisu/Zotero/storage/3KIWUWJ5/Rubin and Waterman - 2006 - Estimating the Causal Effects of Marketing Interve.pdf:application/pdf},
}

@inproceedings{zhao_uplift_2019,
	address = {Washington, DC, USA},
	title = {Uplift {Modeling} for {Multiple} {Treatments} with {Cost} {Optimization}},
	isbn = {978-1-72814-493-1},
	url = {https://ieeexplore.ieee.org/document/8964199/},
	doi = {10.1109/DSAA.2019.00057},
	abstract = {Uplift modeling is an emerging machine learning approach for estimating the treatment effect at an individual or subgroup level. It can be used for optimizing the performance of interventions such as marketing campaigns and product designs. Uplift modeling can be used to estimate which users are likely to beneﬁt from a treatment and then prioritize delivering or promoting the preferred experience to those users. An important but so far neglected use case for uplift modeling is an experiment with multiple treatment groups that have different costs, such as for example when different communication channels and promotion types are tested simultaneously. In this paper, we extend standard uplift models to support multiple treatment groups with different costs. We evaluate the performance of the proposed models using both synthetic and real data. We also describe a production implementation of the approach.},
	language = {en},
	urldate = {2020-07-24},
	booktitle = {2019 {IEEE} {International} {Conference} on {Data} {Science} and {Advanced} {Analytics} ({DSAA})},
	publisher = {IEEE},
	author = {Zhao, Zhenyu and Harinen, Totte},
	month = oct,
	year = {2019},
	pages = {422--431},
	file = {Zhao and Harinen - 2019 - Uplift Modeling for Multiple Treatments with Cost .pdf:/Users/yisu/Zotero/storage/MDLMFGZ8/Zhao and Harinen - 2019 - Uplift Modeling for Multiple Treatments with Cost .pdf:application/pdf},
}

@article{kveton_perturbed-history_2019,
	title = {Perturbed-{History} {Exploration} in {Stochastic} {Linear} {Bandits}},
	url = {http://arxiv.org/abs/1903.09132},
	abstract = {We propose a new online algorithm for minimizing the cumulative regret in stochastic linear bandits. The key idea is to build a perturbed history, which mixes the history of observed rewards with a pseudo-history of randomly generated i.i.d. pseudo-rewards. Our algorithm, perturbed-history exploration in a linear bandit (LinPHE), estimates a linear model from its perturbed history and pulls the arm We pwroitvhethaeO˜h(igdh√enst)vgaalupe-fruenedebrotuhnadt model.},
	language = {en},
	urldate = {2020-07-23},
	journal = {arXiv:1903.09132 [cs, stat]},
	author = {Kveton, Branislav and Szepesvari, Csaba and Ghavamzadeh, Mohammad and Boutilier, Craig},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.09132},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Kveton et al. - 2019 - Perturbed-History Exploration in Stochastic Linear.pdf:/Users/yisu/Zotero/storage/NFYVF2FC/Kveton et al. - 2019 - Perturbed-History Exploration in Stochastic Linear.pdf:application/pdf},
}

@article{riquelme_deep_2018,
	title = {Deep {Bayesian} {Bandits} {Showdown}: {An} {Empirical} {Comparison} of {Bayesian} {Deep} {Networks} for {Thompson} {Sampling}},
	shorttitle = {Deep {Bayesian} {Bandits} {Showdown}},
	url = {http://arxiv.org/abs/1802.09127},
	abstract = {Recent advances in deep reinforcement learning have made signiﬁcant strides in performance on applications such as Go and Atari games. However, developing practical methods to balance exploration and exploitation in complex domains remains largely unsolved. Thompson Sampling and its extension to reinforcement learning provide an elegant approach to exploration that only requires access to posterior samples of the model. At the same time, advances in approximate Bayesian methods have made posterior approximation for ﬂexible neural network models practical. Thus, it is attractive to consider approximate Bayesian neural networks in a Thompson Sampling framework. To understand the impact of using an approximate posterior on Thompson Sampling, we benchmark well-established and recently developed methods for approximate posterior sampling combined with Thompson Sampling over a series of contextual bandit problems. We found that many approaches that have been successful in the supervised learning setting underperformed in the sequential decision-making scenario. In particular, we highlight the challenge of adapting slowly converging uncertainty estimates to the online setting.},
	language = {en},
	urldate = {2020-07-23},
	journal = {arXiv:1802.09127 [cs, stat]},
	author = {Riquelme, Carlos and Tucker, George and Snoek, Jasper},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.09127},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Riquelme et al. - 2018 - Deep Bayesian Bandits Showdown An Empirical Compa.pdf:/Users/yisu/Zotero/storage/PP3N6FBG/Riquelme et al. - 2018 - Deep Bayesian Bandits Showdown An Empirical Compa.pdf:application/pdf},
}

@article{kawale_efficient_nodate,
	title = {Efficient {Thompson} {Sampling} for {Online} ￼{Matrix}-{Factorization} {Recommendation}},
	abstract = {Matrix factorization (MF) collaborative ﬁltering is an effective and widely used method in recommendation systems. However, the problem of ﬁnding an optimal trade-off between exploration and exploitation (otherwise known as the bandit problem), a crucial problem in collaborative ﬁltering from cold-start, has not been previously addressed. In this paper, we present a novel algorithm for online MF recommendation that automatically combines ﬁnding the most relevant items with exploring new or less-recommended items. Our approach, called Particle Thompson sampling for MF (PTS), is based on the general Thompson sampling framework, but augmented with a novel efﬁcient online Bayesian probabilistic matrix factorization method based on the Rao-Blackwellized particle ﬁlter. Extensive experiments in collaborative ﬁltering using several real-world datasets demonstrate that PTS signiﬁcantly outperforms the current state-of-the-arts.},
	language = {en},
	author = {Kawale, Jaya and Bui, Hung H and Kveton, Branislav and Tran-Thanh, Long and Chawla, Sanjay},
	pages = {9},
	file = {Kawale et al. - Efficient Thompson Sampling for Online ￼Matrix-Fac.pdf:/Users/yisu/Zotero/storage/FIFUEV3R/Kawale et al. - Efficient Thompson Sampling for Online ￼Matrix-Fac.pdf:application/pdf},
}

@article{swaminathan_counterfactual_nodate,
	title = {Counterfactual {Risk} {Minimization}: {Learning} from {Logged} {Bandit} {Feedback}},
	abstract = {We develop a learning principle and an efﬁcient algorithm for batch learning from logged bandit feedback. This learning setting is ubiquitous in online systems (e.g., ad placement, web search, recommendation), where an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads). We ﬁrst address the counterfactual nature of the learning problem through propensity scoring. Next, we prove generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator. These constructive bounds give rise to the Counterfactual Risk Minimization (CRM) principle. We show how CRM can be used to derive a new learning method – called Policy Optimizer for Exponential Models (POEM) – for learning stochastic linear rules for structured output prediction. We present a decomposition of the POEM objective that enables efﬁcient stochastic gradient optimization. POEM is evaluated on several multi-label classiﬁcation problems showing substantially improved robustness and generalization performance compared to the state-of-the-art.},
	language = {en},
	author = {Swaminathan, Adith and Joachims, Thorsten},
	pages = {10},
	file = {Swaminathan and Joachims - Counterfactual Risk Minimization Learning from Lo.pdf:/Users/yisu/Zotero/storage/QMRSTNBX/Swaminathan and Joachims - Counterfactual Risk Minimization Learning from Lo.pdf:application/pdf},
}

@article{zhou_neural_2020,
	title = {Neural {Contextual} {Bandits} with {UCB}-based {Exploration}},
	url = {http://arxiv.org/abs/1911.04462},
	abstract = {We study the stochastic contextual bandit problem, where the reward is generated from an unknown function with additive noise. No assumption is made about the reward function other than boundedness. We propose a new algorithm, NeuralUCB, which leverages the representation power of deep neural networks and uses a neural network-based random feature mapping to construct an upper conﬁdence bound (UCB) of reward for efﬁcient exploration. We prove that, under√standard assumptions, NeuralUCB achieves O( T ) regret, where T is the number of rounds. To the best of our knowledge, it is the ﬁrst neural network-based contextual bandit algorithm with a near-optimal regret guarantee. We also show the algorithm is empirically competitive against representative baselines in a number of benchmarks.},
	language = {en},
	urldate = {2020-07-21},
	journal = {arXiv:1911.04462 [cs, stat]},
	author = {Zhou, Dongruo and Li, Lihong and Gu, Quanquan},
	month = jul,
	year = {2020},
	note = {arXiv: 1911.04462},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Zhou et al. - 2020 - Neural Contextual Bandits with UCB-based Explorati.pdf:/Users/yisu/Zotero/storage/7WTWBSGP/Zhou et al. - 2020 - Neural Contextual Bandits with UCB-based Explorati.pdf:application/pdf},
}

@book{karau_learning_2015,
	address = {Beijing ; Sebastopol},
	edition = {First edition},
	title = {Learning {Spark}},
	isbn = {978-1-4493-5862-4},
	abstract = {This book introduces Apache Spark, the open source cluster computing system that makes data analytics fast to write and fast to run. You'll learn how to express parallel jobs with just a few lines of code, and cover applications from simple batch jobs to stream processing and machine learning.--},
	language = {en},
	publisher = {O'Reilly},
	author = {Karau, Holden and Konwinski, Andy and Wendell, Patrick and Zaharia, Matei},
	year = {2015},
	note = {OCLC: ocn844872440},
	keywords = {Big data, Computer programs, Data mining, Spark (Electronic resource : Apache Software Foundation)},
	file = {Karau et al. - 2015 - Learning Spark.pdf:/Users/yisu/Zotero/storage/IF9JR9D8/Karau et al. - 2015 - Learning Spark.pdf:application/pdf},
}

@article{strehl_learning_2010,
	title = {Learning from {Logged} {Implicit} {Exploration} {Data}},
	url = {http://arxiv.org/abs/1003.0120},
	abstract = {We provide a sound and consistent foundation for the use of nonrandom exploration data in “contextual bandit” or “partially labeled” settings where only the value of a chosen action is learned.},
	language = {en},
	urldate = {2020-07-13},
	journal = {arXiv:1003.0120 [cs]},
	author = {Strehl, Alex and Langford, John and Kakade, Sham and Li, Lihong},
	month = jun,
	year = {2010},
	note = {arXiv: 1003.0120},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Strehl et al. - 2010 - Learning from Logged Implicit Exploration Data.pdf:/Users/yisu/Zotero/storage/SUVTM75Y/Strehl et al. - 2010 - Learning from Logged Implicit Exploration Data.pdf:application/pdf},
}

@inproceedings{wu_recurrent_2017,
	address = {Cambridge, United Kingdom},
	title = {Recurrent {Recommender} {Networks}},
	isbn = {978-1-4503-4675-7},
	url = {http://dl.acm.org/citation.cfm?doid=3018661.3018689},
	doi = {10.1145/3018661.3018689},
	abstract = {Recommender systems traditionally assume that user proﬁles and movie attributes are static. Temporal dynamics are purely reactive, that is, they are inferred after they are observed, e.g. after a user’s taste has changed or based on handengineered temporal bias corrections for movies. We propose Recurrent Recommender Networks (RRN) that are able to predict future behavioral trajectories. This is achieved by endowing both users and movies with a Long Short-Term Memory (LSTM) [14] autoregressive model that captures dynamics, in addition to a more traditional low-rank factorization. On multiple real-world datasets, our model oﬀers excellent prediction accuracy and it is very compact, since we need not learn latent state but rather just the state transition function.},
	language = {en},
	urldate = {2020-07-13},
	booktitle = {Proceedings of the {Tenth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining} - {WSDM} '17},
	publisher = {ACM Press},
	author = {Wu, Chao-Yuan and Ahmed, Amr and Beutel, Alex and Smola, Alexander J. and Jing, How},
	year = {2017},
	pages = {495--503},
	file = {Wu et al. - 2017 - Recurrent Recommender Networks.pdf:/Users/yisu/Zotero/storage/CBZN6VKS/Wu et al. - 2017 - Recurrent Recommender Networks.pdf:application/pdf},
}

@article{russo_tutorial_nodate,
	title = {A {Tutorial} on {Thompson} {Sampling}},
	abstract = {Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally eﬃcient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, product assortment, recommendation, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not eﬀective and relations to alternative algorithms.},
	language = {en},
	author = {Russo, Daniel J and Roy, Benjamin Van and Kazerouni, Abbas and Osband, Ian and Wen, Zheng},
	pages = {43},
	file = {Russo et al. - A Tutorial on Thompson Sampling.pdf:/Users/yisu/Zotero/storage/N8PKH5F2/Russo et al. - A Tutorial on Thompson Sampling.pdf:application/pdf},
}

@inproceedings{ding_whole_2019,
	address = {Anchorage AK USA},
	title = {Whole {Page} {Optimization} with {Global} {Constraints}},
	isbn = {978-1-4503-6201-6},
	url = {https://dl.acm.org/doi/10.1145/3292500.3330675},
	doi = {10.1145/3292500.3330675},
	abstract = {Homepage is a primary gateway for users of many online services like Amazon, Facebook, Netflix etc. Most of these homepages present contents organized in widgets. Ranking of widgets in the homepage involves three primary aspects: 1) Present relevant widget higher in the page, 2) Compose the page holistically with diverse contents and 3) Satisfy any business or awareness targets. In this paper, we present a unified framework that can incorporate wholepage model along with business constraints. Proposed whole-page model captures widget interaction through sub-modular features. We derive a primal-dual algorithm for optimizing whole-page model with global constraints imposed based on widget impressions in the homepage. Through extensive offline studies and online experiment, we show that our proposed method achieves significantly higher performance with guaranteeing constraints compared to existing frameworks. We improved customer streaming minutes by 0.42\% and customer distinct streaming says by 0.11\% over state-of-the-art submodular diversity model in online A/B testing.},
	language = {en},
	urldate = {2020-07-07},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Ding, Weicong and Govindaraj, Dinesh and Vishwanathan, S V N},
	month = jul,
	year = {2019},
	pages = {3153--3161},
	file = {Ding et al. - 2019 - Whole Page Optimization with Global Constraints.pdf:/Users/yisu/Zotero/storage/PDAVPNSJ/Ding et al. - 2019 - Whole Page Optimization with Global Constraints.pdf:application/pdf},
}

@inproceedings{sedhain_autorec_2015,
	address = {Florence, Italy},
	title = {{AutoRec}: {Autoencoders} {Meet} {Collaborative} {Filtering}},
	isbn = {978-1-4503-3473-0},
	shorttitle = {{AutoRec}},
	url = {http://dl.acm.org/citation.cfm?doid=2740908.2742726},
	doi = {10.1145/2740908.2742726},
	abstract = {This paper proposes AutoRec, a novel autoencoder framework for collaborative ﬁltering (CF). Empirically, AutoRec’s compact and eﬃciently trainable model outperforms stateof-the-art CF techniques (biased matrix factorization, RBMCF and LLORMA) on the Movielens and Netﬂix datasets.},
	language = {en},
	urldate = {2020-06-29},
	booktitle = {Proceedings of the 24th {International} {Conference} on {World} {Wide} {Web} - {WWW} '15 {Companion}},
	publisher = {ACM Press},
	author = {Sedhain, Suvash and Menon, Aditya Krishna and Sanner, Scott and Xie, Lexing},
	year = {2015},
	pages = {111--112},
	file = {Sedhain et al. - 2015 - AutoRec Autoencoders Meet Collaborative Filtering.pdf:/Users/yisu/Zotero/storage/V8JHJE4V/Sedhain et al. - 2015 - AutoRec Autoencoders Meet Collaborative Filtering.pdf:application/pdf},
}

@article{auer_nonstochastic_2002,
	title = {The {Nonstochastic} {Multiarmed} {Bandit} {Problem}},
	volume = {32},
	issn = {0097-5397, 1095-7111},
	url = {http://epubs.siam.org/doi/10.1137/S0097539701398375},
	doi = {10.1137/S0097539701398375},
	abstract = {In the multi-armed bandit problem, a gambler must decide which arm of non-identical slot machines to play in a sequence of trials so as to maximize his reward. This classical problem has received much attention because of the simple model it provides of the trade-off between exploration (trying out each arm to ﬁnd the best one) and exploitation (playing the arm believed to give the best payoff). Past solutions for the bandit problem have almost always relied on assumptions about the statistics of the slot machines.},
	language = {en},
	number = {1},
	urldate = {2020-06-29},
	journal = {SIAM Journal on Computing},
	author = {Auer, Peter and Cesa-Bianchi, Nicolò and Freund, Yoav and Schapire, Robert E.},
	month = jan,
	year = {2002},
	pages = {48--77},
	file = {Auer et al. - 2002 - The Nonstochastic Multiarmed Bandit Problem.pdf:/Users/yisu/Zotero/storage/9DK84AR3/Auer et al. - 2002 - The Nonstochastic Multiarmed Bandit Problem.pdf:application/pdf},
}

@article{osband_bootstrapped_2015,
	title = {Bootstrapped {Thompson} {Sampling} and {Deep} {Exploration}},
	url = {http://arxiv.org/abs/1507.00300},
	abstract = {This technical note presents a new approach to carrying out the kind of exploration achieved by Thompson sampling, but without explicitly maintaining or sampling from posterior distributions. The approach is based on a bootstrap technique that uses a combination of observed and artiﬁcially generated data. The latter serves to induce a prior distribution which, as we will demonstrate, is critical to eﬀective exploration. We explain how the approach can be applied to multi-armed bandit and reinforcement learning problems and how it relates to Thompson sampling. The approach is particularly well-suited for contexts in which exploration is coupled with deep learning, since in these settings, maintaining or generating samples from a posterior distribution becomes computationally infeasible.},
	language = {en},
	urldate = {2020-06-19},
	journal = {arXiv:1507.00300 [cs, stat]},
	author = {Osband, Ian and Van Roy, Benjamin},
	month = jul,
	year = {2015},
	note = {arXiv: 1507.00300},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Osband and Van Roy - 2015 - Bootstrapped Thompson Sampling and Deep Exploratio.pdf:/Users/yisu/Zotero/storage/FSDLWEDV/Osband and Van Roy - 2015 - Bootstrapped Thompson Sampling and Deep Exploratio.pdf:application/pdf},
}

@article{li_unbiased_nodate,
	title = {An {Unbiased} {Ofﬂine} {Evaluation} of {Contextual} {Bandit} {Algorithms} with {Generalized} {Linear} {Models}},
	abstract = {Contextual bandit algorithms have become popular tools in online recommendation and advertising systems. Ofﬂine evaluation of the effectiveness of new algorithms in these applications is critical for protecting online user experiences but very challenging due to their “partial-label” nature. A common practice is to create a simulator which simulates the online environment for the problem at hand and then run an algorithm against this simulator. However, creating the simulator itself is often difﬁcult and modeling bias is usually unavoidably introduced.},
	language = {en},
	author = {Li, Lihong and Chu, Wei and Langford, John and Moon, Taesup and Wang, Xuanhui},
	pages = {18},
	file = {Li et al. - An Unbiased Ofﬂine Evaluation of Contextual Bandit.pdf:/Users/yisu/Zotero/storage/P46FBCGP/Li et al. - An Unbiased Ofﬂine Evaluation of Contextual Bandit.pdf:application/pdf},
}

@article{elmachtoub_practical_nodate,
	title = {A {Practical} {Method} for {Solving} {Contextual} {Bandit} {Problems} {Using} {Decision} {Trees}},
	abstract = {Many efﬁcient algorithms with strong theoretical guarantees have been proposed for the contextual multi-armed bandit problem. However, applying these algorithms in practice can be difﬁcult because they require domain expertise to build appropriate features and to tune their parameters. We propose a new method for the contextual bandit problem that is simple, practical, and can be applied with little or no domain expertise. Our algorithm relies on decision trees to model the context-reward relationship. Decision trees are non-parametric, interpretable, and work well without handcrafted features. To guide the explorationexploitation trade-off, we use a bootstrapping approach which abstracts Thompson sampling to non-Bayesian settings. We also discuss several computational heuristics and demonstrate the performance of our method on several datasets.},
	language = {en},
	author = {Elmachtoub, Adam N and McNellis, Ryan and Oh, Sechan and Petrik, Marek},
	pages = {10},
	file = {Elmachtoub et al. - A Practical Method for Solving Contextual Bandit P.pdf:/Users/yisu/Zotero/storage/WFZV5RVK/Elmachtoub et al. - A Practical Method for Solving Contextual Bandit P.pdf:application/pdf},
}

@article{graepel_web-scale_nodate,
	title = {Web-{Scale} {Bayesian} {Click}-{Through} {Rate} {Prediction} for {Sponsored} {Search} {Advertising} in {Microsoft}’s {Bing} {Search} {Engine}},
	abstract = {We describe a new Bayesian click-through rate (CTR) prediction algorithm used for Sponsored Search in Microsoft’s Bing search engine. The algorithm is based on a probit regression model that maps discrete or real-valued input features to probabilities. It maintains Gaussian beliefs over weights of the model and performs Gaussian online updates derived from approximate message passing. Scalability of the algorithm is ensured through a principled weight pruning procedure and an approximate parallel implementation. We discuss the challenges arising from evaluating and tuning the predictor as part of the complex system of sponsored search where the predictions made by the algorithm decide about future training sample composition. Finally, we show experimental results from the production system and compare to a calibrated Naïve Bayes algorithm.},
	language = {en},
	author = {Graepel, Thore and Candela, Joaquin Quiñonero and Borchert, Thomas and Herbrich, Ralf},
	pages = {8},
	file = {Graepel et al. - Web-Scale Bayesian Click-Through Rate Prediction f.pdf:/Users/yisu/Zotero/storage/YBPG5ERY/Graepel et al. - Web-Scale Bayesian Click-Through Rate Prediction f.pdf:application/pdf},
}

@article{auer_finite-time_nodate,
	title = {Finite-time {Analysis} of the {Multiarmed} {Bandit} {Problem}},
	abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to ﬁnd proﬁtable actions while taking the empirically best action as often as possible. A popular measure of a policy’s success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the ﬁrst ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efﬁcient policies, and for all reward distributions with bounded support.},
	language = {en},
	author = {Auer, Peter and Cesa-Bianchi, Nicolo},
	pages = {22},
	file = {Auer and Cesa-Bianchi - Finite-time Analysis of the Multiarmed Bandit Prob.pdf:/Users/yisu/Zotero/storage/HQKHSCXU/Auer and Cesa-Bianchi - Finite-time Analysis of the Multiarmed Bandit Prob.pdf:application/pdf},
}

@article{li_contextual-bandit_2010,
	title = {A {Contextual}-{Bandit} {Approach} to {Personalized} {News} {Article} {Recommendation}},
	url = {http://arxiv.org/abs/1003.0146},
	doi = {10.1145/1772690.1772758},
	abstract = {Personalized web services strive to adapt their services (advertisements, news articles, etc.) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative ﬁltering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation.},
	language = {en},
	urldate = {2020-06-08},
	journal = {Proceedings of the 19th international conference on World wide web - WWW '10},
	author = {Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E.},
	year = {2010},
	note = {arXiv: 1003.0146},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, H.3.5, I.2.6},
	pages = {661},
	file = {Li et al. - 2010 - A Contextual-Bandit Approach to Personalized News .pdf:/Users/yisu/Zotero/storage/4GQ5FASY/Li et al. - 2010 - A Contextual-Bandit Approach to Personalized News .pdf:application/pdf},
}

@book{sutton_reinforcement_2018,
	address = {Cambridge, MA London},
	edition = {Second edition},
	series = {Adaptive computation and machine learning},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	language = {en},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew},
	year = {2018},
	note = {OCLC: 1050693967},
	file = {Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf:/Users/yisu/Zotero/storage/HFF6SIQZ/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf:application/pdf},
}

@article{koren_matrix_2009,
	title = {Matrix {Factorization} {Techniques} for {Recommender} {Systems}},
	volume = {42},
	issn = {0018-9162},
	url = {http://ieeexplore.ieee.org/document/5197422/},
	doi = {10.1109/MC.2009.263},
	language = {en},
	number = {8},
	urldate = {2020-05-11},
	journal = {Computer},
	author = {Koren, Yehuda and Bell, Robert and Volinsky, Chris},
	month = aug,
	year = {2009},
	pages = {30--37},
	file = {Koren et al. - 2009 - Matrix Factorization Techniques for Recommender Sy.pdf:/Users/yisu/Zotero/storage/NK4U5Q8Z/Koren et al. - 2009 - Matrix Factorization Techniques for Recommender Sy.pdf:application/pdf},
}

@article{rong_word2vec_2016,
	title = {word2vec {Parameter} {Learning} {Explained}},
	url = {http://arxiv.org/abs/1411.2738},
	abstract = {The word2vec model and application by Mikolov et al. have attracted a great amount of attention in recent two years. The vector representations of words learned by word2vec models have been shown to carry semantic meanings and are useful in various NLP tasks. As an increasing number of researchers would like to experiment with word2vec or similar techniques, I notice that there lacks a material that comprehensively explains the parameter learning process of word embedding models in details, thus preventing researchers that are non-experts in neural networks from understanding the working mechanism of such models.},
	language = {en},
	urldate = {2020-05-08},
	journal = {arXiv:1411.2738 [cs]},
	author = {Rong, Xin},
	month = jun,
	year = {2016},
	note = {arXiv: 1411.2738},
	keywords = {Computer Science - Computation and Language},
	file = {Rong - 2016 - word2vec Parameter Learning Explained.pdf:/Users/yisu/Zotero/storage/B5SDPMRC/Rong - 2016 - word2vec Parameter Learning Explained.pdf:application/pdf},
}

@inproceedings{chen_xgboost_2016,
	address = {San Francisco, California, USA},
	title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {{XGBoost}},
	url = {http://dl.acm.org/citation.cfm?doid=2939672.2939785},
	doi = {10.1145/2939672.2939785},
	abstract = {Tree boosting is a highly eﬀective and widely used machine learning method. In this paper, we describe a scalable endto-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
	language = {en},
	urldate = {2020-04-28},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining} - {KDD} '16},
	publisher = {ACM Press},
	author = {Chen, Tianqi and Guestrin, Carlos},
	year = {2016},
	pages = {785--794},
	file = {Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf:/Users/yisu/Zotero/storage/WDIEPV45/Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf:application/pdf},
}

@inproceedings{tin_kam_ho_random_1995,
	address = {Montreal, Que., Canada},
	title = {Random decision forests},
	volume = {1},
	isbn = {978-0-8186-7128-9},
	url = {http://ieeexplore.ieee.org/document/598994/},
	doi = {10.1109/ICDAR.1995.598994},
	abstract = {Decision trees are attractive classifiers due to their high execution speed. But trees derived with traditional methods often cannot be grown t o arbitrary complexity for possible loss of generalization accuracy o n unseen data. The limitation on complexity usually means suboptimal accuracy o n training data. Following the principles of stochastic modeling, we propose a method t o construct tree-based classifiers whose capacity can be arbitrarily expanded for increases in accuracy f o r both training and unseen data. T h e essence of the method is t o build multiple trees in randomly selected subspaces of the feature space. Trees in different subspaces generalize their classification in complementary ways, and their combined classification can be monotonically improved. T h e validity of the method is demonstrated through experiments o n the recognition of handwritten digits.},
	language = {en},
	urldate = {2020-04-28},
	booktitle = {Proceedings of 3rd {International} {Conference} on {Document} {Analysis} and {Recognition}},
	publisher = {IEEE Comput. Soc. Press},
	author = {{Tin Kam Ho}},
	year = {1995},
	pages = {278--282},
	file = {Tin Kam Ho - 1995 - Random decision forests.pdf:/Users/yisu/Zotero/storage/4FKYKG6K/Tin Kam Ho - 1995 - Random decision forests.pdf:application/pdf},
}

@incollection{cochran_formulating_2011,
	address = {Hoboken, NJ, USA},
	title = {Formulating {Good} {MILP} {Models}},
	isbn = {978-0-470-40053-1},
	url = {http://doi.wiley.com/10.1002/9780470400531.eorms0333},
	abstract = {Writing good models is essential to harnessing MILP solution technology. Due to an underlying theorem that characterizes MILP representability, MILP modeling involves identifying two types of structure in a problem: knapsack constraints and choices between discrete alternatives. This article illustrates the modeling process with a series of examples. They include capital budgeting, freight transport, set packing/covering/partitioning, airline crew rostering, facility location, ﬁxed charges, lot sizing, and package delivery. Piecewise linear modeling, logical conditions, symmetry, special ordered sets, indicator constraints, and semi-continuous variables are also discussed.},
	language = {en},
	urldate = {2020-04-20},
	booktitle = {Wiley {Encyclopedia} of {Operations} {Research} and {Management} {Science}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Hooker, John N.},
	collaborator = {Cochran, James J. and Cox, Louis A. and Keskinocak, Pinar and Kharoufeh, Jeffrey P. and Smith, J. Cole},
	month = feb,
	year = {2011},
	doi = {10.1002/9780470400531.eorms0333},
	pages = {eorms0333},
	file = {Hooker - 2011 - Formulating Good MILP Models.pdf:/Users/yisu/Zotero/storage/BXYKY47K/Hooker - 2011 - Formulating Good MILP Models.pdf:application/pdf},
}

@article{chawla_smote_2002,
	title = {{SMOTE}: {Synthetic} {Minority} {Over}-sampling {Technique}},
	volume = {16},
	issn = {1076-9757},
	shorttitle = {{SMOTE}},
	url = {https://www.jair.org/index.php/jair/article/view/10302},
	doi = {10.1613/jair.953},
	abstract = {An approach to the construction of classiﬁers from imbalanced datasets is described. A dataset is imbalanced if the classiﬁcation categories are not approximately equally represented. Often real-world data sets are predominately composed of “normal” examples with only a small percentage of “abnormal” or “interesting” examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classiﬁer to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classiﬁer performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classiﬁer performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classiﬁer. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
	language = {en},
	urldate = {2020-04-17},
	journal = {Journal of Artificial Intelligence Research},
	author = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
	month = jun,
	year = {2002},
	pages = {321--357},
	file = {Chawla et al. - 2002 - SMOTE Synthetic Minority Over-sampling Technique.pdf:/Users/yisu/Zotero/storage/ZW3FP3JP/Chawla et al. - 2002 - SMOTE Synthetic Minority Over-sampling Technique.pdf:application/pdf},
}

@article{dai_learning_2018,
	title = {Learning {Combinatorial} {Optimization} {Algorithms} over {Graphs}},
	url = {http://arxiv.org/abs/1704.01665},
	abstract = {The design of good heuristics or approximation algorithms for NP-hard combinatorial optimization problems often requires signiﬁcant specialized knowledge and trial-and-error. Can we automate this challenging, tedious process, and learn the algorithms instead? In many real-world applications, it is typically the case that the same optimization problem is solved again and again on a regular basis, maintaining the same problem structure but differing in the data. This provides an opportunity for learning heuristic algorithms that exploit the structure of such recurring problems. In this paper, we propose a unique combination of reinforcement learning and graph embedding to address this challenge. The learned greedy policy behaves like a meta-algorithm that incrementally constructs a solution, and the action is determined by the output of a graph embedding network capturing the current state of the solution. We show that our framework can be applied to a diverse range of optimization problems over graphs, and learns effective algorithms for the Minimum Vertex Cover, Maximum Cut and Traveling Salesman problems.},
	language = {en},
	urldate = {2020-03-14},
	journal = {arXiv:1704.01665 [cs, stat]},
	author = {Dai, Hanjun and Khalil, Elias B. and Zhang, Yuyu and Dilkina, Bistra and Song, Le},
	month = feb,
	year = {2018},
	note = {arXiv: 1704.01665},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Dai et al. - 2018 - Learning Combinatorial Optimization Algorithms ove.pdf:/Users/yisu/Zotero/storage/5PS5XDVR/Dai et al. - 2018 - Learning Combinatorial Optimization Algorithms ove.pdf:application/pdf},
}

@article{he_delving_2015,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	url = {http://arxiv.org/abs/1502.01852},
	abstract = {Rectiﬁed activation units (rectiﬁers) are essential for state-of-the-art neural networks. In this work, we study rectiﬁer neural networks for image classiﬁcation from two aspects. First, we propose a Parametric Rectiﬁed Linear Unit (PReLU) that generalizes the traditional rectiﬁed unit. PReLU improves model ﬁtting with nearly zero extra computational cost and little overﬁtting risk. Second, we derive a robust initialization method that particularly considers the rectiﬁer nonlinearities. This method enables us to train extremely deep rectiﬁed models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classiﬁcation dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [29]). To our knowledge, our result is the ﬁrst to surpass human-level performance (5.1\%, [22]) on this visual recognition challenge.},
	language = {en},
	urldate = {2020-03-09},
	journal = {arXiv:1502.01852 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.01852},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf:/Users/yisu/Zotero/storage/F2LIQEC8/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf:application/pdf},
}

@article{glorot_understanding_nodate,
	title = {Understanding the difﬁculty of training deep feedforward neural networks},
	abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
	language = {en},
	author = {Glorot, Xavier and Bengio, Yoshua},
	pages = {8},
	file = {Glorot and Bengio - Understanding the difﬁculty of training deep feedf.pdf:/Users/yisu/Zotero/storage/DJSXQ863/Glorot and Bengio - Understanding the difﬁculty of training deep feedf.pdf:application/pdf},
}

@article{smith_disciplined_2018,
	title = {A disciplined approach to neural network hyper-parameters: {Part} 1 -- learning rate, batch size, momentum, and weight decay},
	shorttitle = {A disciplined approach to neural network hyper-parameters},
	url = {http://arxiv.org/abs/1803.09820},
	abstract = {Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efﬁcient ways to set the hyper-parameters that signiﬁcantly reduce training time and improves performance. Speciﬁcally, this report shows how to examine the training validation/test loss function for subtle clues of underﬁtting and overﬁtting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase/decrease the learning rate/momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentum. Files to help replicate the results reported here are available at https://github.com/lnsmith54/hyperParam1.},
	language = {en},
	urldate = {2020-02-23},
	journal = {arXiv:1803.09820 [cs, stat]},
	author = {Smith, Leslie N.},
	month = apr,
	year = {2018},
	note = {arXiv: 1803.09820},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {1803.09820.pdf:/Users/yisu/Zotero/storage/DSL4XS38/1803.09820.pdf:application/pdf},
}

@article{rumelhart_learning_nodate,
	title = {Learning {Internal} {Representations} by {Error} {Propagation}},
	language = {en},
	author = {Rumelhart, D E and Hinton, G E and Williams, R J},
	pages = {23},
	file = {Rumelhart et al. - Learning Internal Representations by Error Propaga.pdf:/Users/yisu/Zotero/storage/XKGGBNA5/Rumelhart et al. - Learning Internal Representations by Error Propaga.pdf:application/pdf},
}

@article{tibshirani_elements_nodate,
	title = {The {Elements} of {Statistical} {Learning}},
	language = {en},
	author = {Tibshirani, Sami and Friedman, Harry},
	pages = {764},
	file = {Tibshirani and Friedman - Valerie and Patrick Hastie.pdf:/Users/yisu/Zotero/storage/R8XPXKNH/Tibshirani and Friedman - Valerie and Patrick Hastie.pdf:application/pdf},
}

@book{bishop_pattern_2006,
	address = {New York},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {978-0-387-31073-2},
	language = {en},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
	keywords = {Machine learning, Pattern perception},
	file = {Bishop - 2006 - Pattern recognition and machine learning.pdf:/Users/yisu/Zotero/storage/HUGXBHZR/Bishop - 2006 - Pattern recognition and machine learning.pdf:application/pdf},
}

@article{boser_training_nodate,
	title = {A {Training} {Algorithm} for {Optimal} {Margin} {Classifiers}},
	abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of classi action functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.},
	language = {en},
	author = {Boser, Bernhard E and Guyon, Isabelle M and Vapnik, Vladimir N},
	pages = {9},
	file = {Boser et al. - OAptTimraainl iMngarAglignoCritlahsmsi feorrs.pdf:/Users/yisu/Zotero/storage/ICICE3HF/Boser et al. - OAptTimraainl iMngarAglignoCritlahsmsi feorrs.pdf:application/pdf},
}

@book{boyd_introduction_2018,
	edition = {1},
	title = {Introduction to {Applied} {Linear} {Algebra}: {Vectors}, {Matrices}, and {Least} {Squares}},
	isbn = {978-1-316-51896-0 978-1-108-58366-4},
	shorttitle = {Introduction to {Applied} {Linear} {Algebra}},
	url = {https://www.cambridge.org/core/product/identifier/9781108583664/type/book},
	language = {en},
	urldate = {2020-01-07},
	publisher = {Cambridge University Press},
	author = {Boyd, Stephen and Vandenberghe, Lieven},
	month = jun,
	year = {2018},
	doi = {10.1017/9781108583664},
	file = {Boyd and Vandenberghe - 2018 - Introduction to Applied Linear Algebra Vectors, M.pdf:/Users/yisu/Zotero/storage/94Z73G9W/Boyd and Vandenberghe - 2018 - Introduction to Applied Linear Algebra Vectors, M.pdf:application/pdf},
}

@book{rawlings_model_2017,
	address = {Madison, Wisconsin},
	edition = {2nd edition},
	title = {Model predictive control: theory, computation, and design},
	isbn = {978-0-9759377-3-0},
	shorttitle = {Model predictive control},
	language = {en},
	publisher = {Nob Hill Publishing},
	author = {Rawlings, James B. and Mayne, David Q. and Diehl, Moritz M.},
	year = {2017},
	note = {OCLC: 1020170256},
	file = {Rawlings et al. - 2017 - Model predictive control theory, computation, and.pdf:/Users/yisu/Zotero/storage/6X4IZ69L/Rawlings et al. - 2017 - Model predictive control theory, computation, and.pdf:application/pdf},
}

@inproceedings{simard_best_2003,
	address = {Edinburgh, UK},
	title = {Best practices for convolutional neural networks applied to visual document analysis},
	volume = {1},
	isbn = {978-0-7695-1960-9},
	url = {http://ieeexplore.ieee.org/document/1227801/},
	doi = {10.1109/ICDAR.2003.1227801},
	abstract = {Neural networks are a powerful technology for classification of visual inputs arising from documents. However, there is a confusing plethora of different neural network methods that are used in the literature and in industry. This paper describes a set of concrete best practices that document analysis researchers can use to get good results with neural networks. The most important practice is getting a training set as large as possible: we expand the training set by adding a new form of distorted data. The next most important practice is that convolutional neural networks are better suited for visual document tasks than fully connected networks. We propose that a simple “do-it-yourself” implementation of convolution with a flexible architecture is suitable for many visual document problems. This simple convolutional neural network does not require complex methods, such as momentum, weight decay, structuredependent learning rates, averaging layers, tangent prop, or even finely-tuning the architecture. The end result is a very simple yet general architecture which can yield state-of-the-art performance for document analysis. We illustrate our claims on the MNIST set of English digit images.},
	language = {en},
	urldate = {2020-01-06},
	booktitle = {Seventh {International} {Conference} on {Document} {Analysis} and {Recognition}, 2003. {Proceedings}.},
	publisher = {IEEE Comput. Soc},
	author = {Simard, P.Y. and Steinkraus, D. and Platt, J.C.},
	year = {2003},
	pages = {958--963},
	file = {Simard et al. - 2003 - Best practices for convolutional neural networks a.pdf:/Users/yisu/Zotero/storage/6S32VKAQ/Simard et al. - 2003 - Best practices for convolutional neural networks a.pdf:application/pdf},
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {00010782},
	url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	number = {6},
	urldate = {2020-01-06},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	pages = {84--90},
	file = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:/Users/yisu/Zotero/storage/LFLXQQFZ/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf},
}

@article{srivastava_dropout:_nodate,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overﬁtting}},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overﬁtting is a serious problem in such networks. Large networks are also slow to use, making it diﬃcult to deal with overﬁtting by combining the predictions of many diﬀerent large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of diﬀerent “thinned” networks. At test time, it is easy to approximate the eﬀect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This signiﬁcantly reduces overﬁtting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classiﬁcation and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	language = {en},
	author = {Srivastava, Nitish and Hinton, Geoﬀrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	pages = {30},
	file = {Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:/Users/yisu/Zotero/storage/F9GQ5AU6/Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:application/pdf},
}

@article{scott_modern_2010,
	title = {A modern {Bayesian} look at the multi-armed bandit},
	volume = {26},
	issn = {15241904},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/asmb.874},
	doi = {10.1002/asmb.874},
	abstract = {A multi-armed bandit is an experiment with the goal of accumulating rewards from a payoff distribution with unknown parameters that are to be learned sequentially. This article describes a heuristic for managing multi-armed bandits called randomized probability matching, which randomly allocates observations to arms according the Bayesian posterior probability that each arm is optimal. Advances in Bayesian computation have made randomized probability matching easy to apply to virtually any payoff distribution. This ﬂexibility frees the experimenter to work with payoff distributions that correspond to certain classical experimental designs that have the potential to outperform methods that are ‘optimal’ in simpler contexts. I summarize the relationships between randomized probability matching and several related heuristics that have been used in the reinforcement learning literature. Copyright q 2010 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {6},
	urldate = {2022-04-24},
	journal = {Applied Stochastic Models in Business and Industry},
	author = {Scott, Steven L.},
	month = nov,
	year = {2010},
	pages = {639--658},
	file = {Scott - 2010 - A modern Bayesian look at the multi-armed bandit.pdf:/Users/yisu/Zotero/storage/APXFCIS7/Scott - 2010 - A modern Bayesian look at the multi-armed bandit.pdf:application/pdf},
}

@techreport{tay_long_2020,
	title = {Long {Range} {Arena}: {A} {Benchmark} for {Efficient} {Transformers}},
	shorttitle = {Long {Range} {Arena}},
	url = {http://arxiv.org/abs/2011.04006},
	abstract = {Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from \$1K\$ to \$16K\$ tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. LRA paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle. Our benchmark code will be released at https://github.com/google-research/long-range-arena.},
	number = {arXiv:2011.04006},
	urldate = {2022-05-29},
	institution = {arXiv},
	author = {Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
	month = nov,
	year = {2020},
	note = {arXiv:2011.04006 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/GEQQC59V/Tay et al. - 2020 - Long Range Arena A Benchmark for Efficient Transf.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/X7XL5GYA/2011.html:text/html},
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2022-05-03},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/4TWJPJXE/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/WUJ9NVF7/1512.html:text/html},
}

@article{baevski_data2vec_nodate,
	title = {data2vec: {A} {General} {Framework} for {Self}-supervised {Learning} in {Speech}, {Vision} and {Language}},
	abstract = {While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a selfdistillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches. Models and code are available at www.github.com/pytorch/fairseq/ tree/master/examples/data2vec.},
	language = {en},
	author = {Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
	pages = {13},
	file = {Baevski et al. - data2vec A General Framework for Self-supervised .pdf:/Users/yisu/Zotero/storage/B4TEKQPG/Baevski et al. - data2vec A General Framework for Self-supervised .pdf:application/pdf},
}

@techreport{weinberger_feature_2010,
	title = {Feature {Hashing} for {Large} {Scale} {Multitask} {Learning}},
	url = {http://arxiv.org/abs/0902.2206},
	abstract = {Empirical evidence suggests that hashing is an effective strategy for dimensionality reduction and practical nonparametric estimation. In this paper we provide exponential tail bounds for feature hashing and show that the interaction between random subspaces is negligible with high probability. We demonstrate the feasibility of this approach with experimental results for a new use case -- multitask learning with hundreds of thousands of tasks.},
	number = {arXiv:0902.2206},
	urldate = {2022-06-06},
	institution = {arXiv},
	author = {Weinberger, Kilian and Dasgupta, Anirban and Attenberg, Josh and Langford, John and Smola, Alex},
	month = feb,
	year = {2010},
	note = {arXiv:0902.2206 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/ZU3852RX/Weinberger et al. - 2010 - Feature Hashing for Large Scale Multitask Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/MPL2V5RD/0902.html:text/html},
}

@techreport{li_align_2021,
	title = {Align and {Prompt}: {Video}-and-{Language} {Pre}-training with {Entity} {Prompts}},
	shorttitle = {Align and {Prompt}},
	url = {http://arxiv.org/abs/2112.09583},
	abstract = {Video-and-language pre-training has shown promising improvements on various downstream tasks. Most previous methods capture cross-modal interactions with a transformer-based multimodal encoder, not fully addressing the misalignment between unimodal video and text features. Besides, learning fine-grained visual-language alignment usually requires off-the-shelf object detectors to provide object information, which is bottlenecked by the detector's limited vocabulary and expensive computation cost. We propose Align and Prompt: an efficient and effective video-and-language pre-training framework with better cross-modal alignment. First, we introduce a video-text contrastive (VTC) loss to align unimodal video-text features at the instance level, which eases the modeling of cross-modal interactions. Then, we propose a new visually-grounded pre-training task, prompting entity modeling (PEM), which aims to learn fine-grained region-entity alignment. To achieve this, we first introduce an entity prompter module, which is trained with VTC to produce the similarity between a video crop and text prompts instantiated with entity names. The PEM task then asks the model to predict the entity pseudo-labels (i.e{\textasciitilde}normalized similarity scores) for randomly-selected video crops. The resulting pre-trained model achieves state-of-the-art performance on both text-video retrieval and videoQA, outperforming prior work by a substantial margin. Our code and pre-trained models are available at https://github.com/salesforce/ALPRO.},
	number = {arXiv:2112.09583},
	urldate = {2022-06-06},
	institution = {arXiv},
	author = {Li, Dongxu and Li, Junnan and Li, Hongdong and Niebles, Juan Carlos and Hoi, Steven C. H.},
	month = dec,
	year = {2021},
	note = {arXiv:2112.09583 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/7QCDXTXX/Li et al. - 2021 - Align and Prompt Video-and-Language Pre-training .pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/KY5HU2H8/2112.html:text/html},
}

@article{wang_knowledge_2017,
	title = {Knowledge {Graph} {Embedding}: {A} {Survey} of {Approaches} and {Applications}},
	volume = {29},
	issn = {1041-4347},
	shorttitle = {Knowledge {Graph} {Embedding}},
	url = {http://ieeexplore.ieee.org/document/8047276/},
	doi = {10.1109/TKDE.2017.2754499},
	abstract = {Knowledge graph (KG) embedding is to embed components of a KG including entities and relations into continuous vector spaces, so as to simplify the manipulation while preserving the inherent structure of the KG. It can beneﬁt a variety of downstream tasks such as KG completion and relation extraction, and hence has quickly gained massive attention. In this article, we provide a systematic review of existing techniques, including not only the state-of-the-arts but also those with latest trends. Particularly, we make the review based on the type of information used in the embedding task. Techniques that conduct embedding using only facts observed in the KG are ﬁrst introduced. We describe the overall framework, speciﬁc model design, typical training procedures, as well as pros and cons of such techniques. After that, we discuss techniques that further incorporate additional information besides facts. We focus speciﬁcally on the use of entity types, relation paths, textual descriptions, and logical rules. Finally, we brieﬂy introduce how KG embedding can be applied to and beneﬁt a wide variety of downstream tasks such as KG completion, relation extraction, question answering, and so forth.},
	language = {en},
	number = {12},
	urldate = {2022-06-06},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Wang, Quan and Mao, Zhendong and Wang, Bin and Guo, Li},
	month = dec,
	year = {2017},
	pages = {2724--2743},
	file = {Wang et al. - 2017 - Knowledge Graph Embedding A Survey of Approaches .pdf:/Users/yisu/Zotero/storage/MMSMF3AE/Wang et al. - 2017 - Knowledge Graph Embedding A Survey of Approaches .pdf:application/pdf},
}

@techreport{rendle_neural_2020,
	title = {Neural {Collaborative} {Filtering} vs. {Matrix} {Factorization} {Revisited}},
	url = {http://arxiv.org/abs/2005.09683},
	abstract = {Embedding based models have been the state of the art in collaborative filtering for over a decade. Traditionally, the dot product or higher order equivalents have been used to combine two or more embeddings, e.g., most notably in matrix factorization. In recent years, it was suggested to replace the dot product with a learned similarity e.g. using a multilayer perceptron (MLP). This approach is often referred to as neural collaborative filtering (NCF). In this work, we revisit the experiments of the NCF paper that popularized learned similarities using MLPs. First, we show that with a proper hyperparameter selection, a simple dot product substantially outperforms the proposed learned similarities. Second, while a MLP can in theory approximate any function, we show that it is non-trivial to learn a dot product with an MLP. Finally, we discuss practical issues that arise when applying MLP based similarities and show that MLPs are too costly to use for item recommendation in production environments while dot products allow to apply very efficient retrieval algorithms. We conclude that MLPs should be used with care as embedding combiner and that dot products might be a better default choice.},
	number = {arXiv:2005.09683},
	urldate = {2022-06-06},
	institution = {arXiv},
	author = {Rendle, Steffen and Krichene, Walid and Zhang, Li and Anderson, John},
	month = jun,
	year = {2020},
	note = {arXiv:2005.09683 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/GQKAJEYH/Rendle et al. - 2020 - Neural Collaborative Filtering vs. Matrix Factoriz.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/6IYRNQK9/2005.html:text/html},
}

@article{ai_distributed_2022,
	title = {Distributed {Hybrid} {CPU} and {GPU} training for {Graph} {Neural} {Networks} on {Billion}-{Scale} {Heterogeneous} {Graphs}},
	abstract = {Graph neural networks (GNN) have shown great success in learning from graph-structured data. They are widely used in various applications, such as recommendation, fraud detection, and search. In these domains, the graphs are typically large and heterogeneous, containing many millions or billions of vertices and edges of different types. To tackle this challenge, we develop DistDGLv2, a system that extends DistDGL for training GNNs on massive heterogeneous graphs in a mini-batch fashion, using distributed hybrid CPU/GPU training. DistDGLv2 places graph data in distributed CPU memory and performs mini-batch computation in GPUs. For ease of use, DistDGLv2 adopts API compatible with Deep Graph Library (DGL)’s mini-batch training and heterogeneous graph API, which enables distributed training with almost no code modification. To ensure model accuracy, DistDGLv2 follows a synchronous training approach and allows ego-networks forming mini-batches to include non-local vertices. To ensure data locality and load balancing, DistDGLv2 partitions heterogeneous graphs by using a multi-level partitioning algorithm with min-edge cut and multiple balancing constraints. DistDGLv2 deploys an asynchronous minibatch generation pipeline that makes computation and data access asynchronous to fully utilize all hardware (CPU, GPU, network, PCIe). We demonstrate DistDGLv2 on various GNN workloads. Our results show that DistDGLv2 achieves 2 − 3× speedup over DistDGL and 18× speedup over Euler. It takes only 5 − 10 seconds to complete an epoch on graphs with hundreds of millions of vertices on a cluster with 64 GPUs.},
	language = {en},
	author = {Ai, Aws},
	year = {2022},
	pages = {10},
	file = {Ai - 2022 - Distributed Hybrid CPU and GPU training for Graph .pdf:/Users/yisu/Zotero/storage/HIQJE7KC/Ai - 2022 - Distributed Hybrid CPU and GPU training for Graph .pdf:application/pdf},
}

@techreport{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {http://arxiv.org/abs/2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	number = {arXiv:2103.00020},
	urldate = {2022-06-12},
	institution = {arXiv},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {arXiv:2103.00020 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/QLWNMBCL/Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/4JBBGLKW/2103.html:text/html},
}

@article{daulton_robust_nodate,
	title = {Robust {Multi}-{Objective} {Bayesian} {Optimization} {Under} {Input} {Noise}},
	abstract = {Bayesian optimization (BO) is a sample-efﬁcient approach for tuning design parameters to optimize expensive-to-evaluate, black-box performance metrics. In many manufacturing processes, the design parameters are subject to random input noise, resulting in a product that is often less performant than expected. Although BO methods have been proposed for optimizing a single objective under input noise, no existing method addresses the practical scenario where there are multiple objectives that are sensitive to input perturbations. In this work, we propose the ﬁrst multi-objective BO method that is robust to input noise. We formalize our goal as optimizing the multivariate value-at-risk (MVAR), a risk measure of the uncertain objectives. Since directly optimizing MVAR is computationally infeasible in many settings, we propose a scalable, theoretically-grounded approach for optimizing MVAR using random scalarizations. Empirically, we ﬁnd that our approach signiﬁcantly outperforms alternative methods and efﬁciently identiﬁes optimal robust designs that will satisfy speciﬁcations across multiple metrics with high probability.},
	language = {en},
	author = {Daulton, Samuel and Cakmak, Sait and Balandat, Maximilian and Osborne, Michael A and Zhou, Enlu and Bakshy, Eytan},
	pages = {36},
	file = {Daulton et al. - Robust Multi-Objective Bayesian Optimization Under.pdf:/Users/yisu/Zotero/storage/6ALDZUKY/Daulton et al. - Robust Multi-Objective Bayesian Optimization Under.pdf:application/pdf},
}

@incollection{stuckenschmidt_toward_2015,
	address = {Cham},
	title = {Toward {Building} a {Content}-{Based} {Video} {Recommendation} {System} {Based} on {Low}-{Level} {Features}},
	volume = {239},
	isbn = {978-3-319-27728-8 978-3-319-27729-5},
	url = {http://link.springer.com/10.1007/978-3-319-27729-5_4},
	abstract = {One of the challenges in video recommendation systems is the New Item problem, which happens when the system is unable to recommend video items, that no information is available about them. For example, in the popular movie-sharing websites, such as Youtube, everyday, hundred millions of hours of videos are uploaded and big portion of these videos may not contain any meta-data, to be used by the system to generate recommendations.},
	language = {en},
	urldate = {2022-06-12},
	booktitle = {E-{Commerce} and {Web} {Technologies}},
	publisher = {Springer International Publishing},
	author = {Deldjoo, Yashar and Elahi, Mehdi and Quadrana, Massimo and Cremonesi, Paolo},
	editor = {Stuckenschmidt, Heiner and Jannach, Dietmar},
	year = {2015},
	doi = {10.1007/978-3-319-27729-5_4},
	note = {Series Title: Lecture Notes in Business Information Processing},
	pages = {45--56},
	file = {Deldjoo et al. - 2015 - Toward Building a Content-Based Video Recommendati.pdf:/Users/yisu/Zotero/storage/WHUSSSVP/Deldjoo et al. - 2015 - Toward Building a Content-Based Video Recommendati.pdf:application/pdf},
}

@techreport{hejazinia_fel_2022,
	title = {{FEL}: {High} {Capacity} {Learning} for {Recommendation} and {Ranking} via {Federated} {Ensemble} {Learning}},
	shorttitle = {{FEL}},
	url = {http://arxiv.org/abs/2206.03852},
	abstract = {Federated learning (FL) has emerged as an effective approach to address consumer privacy needs. FL has been successfully applied to certain machine learning tasks, such as training smart keyboard models and keyword spotting. Despite FL's initial success, many important deep learning use cases, such as ranking and recommendation tasks, have been limited from on-device learning. One of the key challenges faced by practical FL adoption for DL-based ranking and recommendation is the prohibitive resource requirements that cannot be satisfied by modern mobile systems. We propose Federated Ensemble Learning (FEL) as a solution to tackle the large memory requirement of deep learning ranking and recommendation tasks. FEL enables large-scale ranking and recommendation model training on-device by simultaneously training multiple model versions on disjoint clusters of client devices. FEL integrates the trained sub-models via an over-arch layer into an ensemble model that is hosted on the server. Our experiments demonstrate that FEL leads to 0.43-2.31\% model quality improvement over traditional on-device federated learning - a significant improvement for ranking and recommendation system use cases.},
	number = {arXiv:2206.03852},
	urldate = {2022-06-12},
	institution = {arXiv},
	author = {Hejazinia, Meisam and Huba, Dzmitry and Leontiadis, Ilias and Maeng, Kiwan and Malek, Mani and Melis, Luca and Mironov, Ilya and Nasr, Milad and Wang, Kaikai and Wu, Carole-Jean},
	month = jun,
	year = {2022},
	note = {arXiv:2206.03852 [cs]
type: article},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/3MPAI9A8/Hejazinia et al. - 2022 - FEL High Capacity Learning for Recommendation and.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/RBRI5F5I/2206.html:text/html},
}

@misc{block_counterfactual_2022,
	title = {Counterfactual {Learning} {To} {Rank} for {Utility}-{Maximizing} {Query} {Autocompletion}},
	url = {http://arxiv.org/abs/2204.10936},
	doi = {10.1145/3477495.3531958},
	abstract = {Conventional methods for query autocompletion aim to predict which completed query a user will select from a list. A shortcoming of this approach is that users often do not know which query will provide the best retrieval performance on the current information retrieval system, meaning that any query autocompletion methods trained to mimic user behavior can lead to suboptimal query suggestions. To overcome this limitation, we propose a new approach that explicitly optimizes the query suggestions for downstream retrieval performance. We formulate this as a problem of ranking a set of rankings, where each query suggestion is represented by the downstream item ranking it produces. We then present a learning method that ranks query suggestions by the quality of their item rankings. The algorithm is based on a counterfactual learning approach that is able to leverage feedback on the items (e.g., clicks, purchases) to evaluate query suggestions through an unbiased estimator, thus avoiding the assumption that users write or select optimal queries. We establish theoretical support for the proposed approach and provide learning-theoretic guarantees. We also present empirical results on publicly available datasets, and demonstrate real-world applicability using data from an online shopping store.},
	language = {en},
	urldate = {2022-06-12},
	author = {Block, Adam and Kidambi, Rahul and Hill, Daniel N. and Joachims, Thorsten and Dhillon, Inderjit S.},
	month = apr,
	year = {2022},
	note = {arXiv:2204.10936 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval},
	file = {Block et al. - 2022 - Counterfactual Learning To Rank for Utility-Maximi.pdf:/Users/yisu/Zotero/storage/DDV35VEK/Block et al. - 2022 - Counterfactual Learning To Rank for Utility-Maximi.pdf:application/pdf},
}

@techreport{zheng_hien_2022,
	title = {{HIEN}: {Hierarchical} {Intention} {Embedding} {Network} for {Click}-{Through} {Rate} {Prediction}},
	shorttitle = {{HIEN}},
	url = {http://arxiv.org/abs/2206.00510},
	abstract = {Click-through rate (CTR) prediction plays an important role in online advertising and recommendation systems, which aims at estimating the probability of a user clicking on a specific item. Feature interaction modeling and user interest modeling methods are two popular domains in CTR prediction, and they have been studied extensively in recent years. However, these methods still suffer from two limitations. First, traditional methods regard item attributes as ID features, while neglecting structure information and relation dependencies among attributes. Second, when mining user interests from user-item interactions, current models ignore user intents and item intents for different attributes, which lacks interpretability. Based on this observation, in this paper, we propose a novel approach Hierarchical Intention Embedding Network (HIEN), which considers dependencies of attributes based on bottom-up tree aggregation in the constructed attribute graph. HIEN also captures user intents for different item attributes as well as item intents based on our proposed hierarchical attention mechanism. Extensive experiments on both public and production datasets show that the proposed model significantly outperforms the state-of-the-art methods. In addition, HIEN can be applied as an input module to state-of-the-art CTR prediction methods, bringing further performance lift for these existing models that might already be intensively used in real systems.},
	urldate = {2022-06-12},
	author = {Zheng, Zuowu and Zhang, Changwang and Gao, Xiaofeng and Chen, Guihai},
	month = jun,
	year = {2022},
	doi = {10.1145/3477495.3531988},
	note = {arXiv:2206.00510 [cs]},
	keywords = {Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/38I4IELJ/Zheng et al. - 2022 - HIEN Hierarchical Intention Embedding Network for.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/V8HPW8BN/2206.html:text/html},
}

@techreport{liu_convnet_2022,
	title = {A {ConvNet} for the 2020s},
	url = {http://arxiv.org/abs/2201.03545},
	abstract = {The "Roaring 20s" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually "modernize" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8\% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.},
	number = {arXiv:2201.03545},
	urldate = {2022-06-14},
	institution = {arXiv},
	author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
	month = mar,
	year = {2022},
	note = {arXiv:2201.03545 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/9FMD6Q8H/Liu et al. - 2022 - A ConvNet for the 2020s.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/KGI5R675/2201.html:text/html},
}

@article{sinha_multi-armed_nodate,
	title = {Multi-armed {Bandits} with {Cost} {Subsidy}},
	abstract = {In this paper, we consider a novel variant of the multi-armed bandit (MAB) problem, MAB with cost subsidy, which models many real-life applications where the learning agent has to pay to select an arm and is concerned about optimizing cumulative costs and rewards. We present two applications, intelligent SMS routing problem and ad audience optimization problem faced by several businesses (especially online platforms) and show how our problem uniquely captures key features of these applications. We show that naive generalizations of existing MAB algorithms like Upper Conﬁdence Bound and Thompson Sampling do not perform well for this problem. We then establish fundamental lower bound of Ω(K1/3T 2/3) on the performance of any online learning algorithm for this problem, highlighting the hardness of our problem in comparison to the classical MAB problem (where T is the time horizon and K is the number of arms). We also present a simple variant of explore-then-commit and establish near-optimal regret bounds for this algorithm. Lastly, we perform extensive numerical simulations to understand the behavior of a suite of algorithms for various instances and recommend a practical guide to employ diﬀerent algorithms.},
	language = {en},
	author = {Sinha, Deeksha and Avadhanula, Vashist},
	pages = {21},
	file = {Sinha and Avadhanula - Multi-armed Bandits with Cost Subsidy.pdf:/Users/yisu/Zotero/storage/U62KDCKQ/Sinha and Avadhanula - Multi-armed Bandits with Cost Subsidy.pdf:application/pdf},
}

@techreport{lin_survey_2021,
	title = {A {Survey} of {Transformers}},
	url = {http://arxiv.org/abs/2106.04554},
	abstract = {Transformers have achieved great success in many artificial intelligence fields, such as natural language processing, computer vision, and audio processing. Therefore, it is natural to attract lots of interest from academic and industry researchers. Up to the present, a great variety of Transformer variants (a.k.a. X-formers) have been proposed, however, a systematic and comprehensive literature review on these Transformer variants is still missing. In this survey, we provide a comprehensive review of various X-formers. We first briefly introduce the vanilla Transformer and then propose a new taxonomy of X-formers. Next, we introduce the various X-formers from three perspectives: architectural modification, pre-training, and applications. Finally, we outline some potential directions for future research.},
	number = {arXiv:2106.04554},
	urldate = {2022-06-14},
	institution = {arXiv},
	author = {Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
	month = jun,
	year = {2021},
	note = {arXiv:2106.04554 [cs]
type: article},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/SWTWFTT4/Lin et al. - 2021 - A Survey of Transformers.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/PKR3SMZJ/2106.html:text/html},
}

@article{hernan_causal_nodate,
	title = {Causal {Inference}: {What} {If}},
	language = {en},
	author = {Hernán, Miguel A and Robins, James M},
	pages = {311},
	file = {Hernán and Robins - Causal Inference What If.pdf:/Users/yisu/Zotero/storage/Z24TM8W5/Hernán and Robins - Causal Inference What If.pdf:application/pdf},
}

@misc{scholkopf_towards_2021,
	title = {Towards {Causal} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2102.11107},
	abstract = {The two fields of machine learning and graphical causality arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.},
	urldate = {2022-07-03},
	publisher = {arXiv},
	author = {Schölkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
	month = feb,
	year = {2021},
	note = {arXiv:2102.11107 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/4HWUY5B8/Schölkopf et al. - 2021 - Towards Causal Representation Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/WKFXDCSQ/2102.html:text/html},
}

@inproceedings{wang_adaptive_2022,
	title = {Adaptive {Experimentation} with {Delayed} {Binary} {Feedback}},
	url = {http://arxiv.org/abs/2202.00846},
	doi = {10.1145/3485447.3512097},
	abstract = {Conducting experiments with objectives that take significant delays to materialize (e.g. conversions, add-to-cart events, etc.) is challenging. Although the classical "split sample testing" is still valid for the delayed feedback, the experiment will take longer to complete, which also means spending more resources on worse-performing strategies due to their fixed allocation schedules. Alternatively, adaptive approaches such as "multi-armed bandits" are able to effectively reduce the cost of experimentation. But these methods generally cannot handle delayed objectives directly out of the box. This paper presents an adaptive experimentation solution tailored for delayed binary feedback objectives by estimating the real underlying objectives before they materialize and dynamically allocating variants based on the estimates. Experiments show that the proposed method is more efficient for delayed feedback compared to various other approaches and is robust in different settings. In addition, we describe an experimentation product powered by this algorithm. This product is currently deployed in the online experimentation platform of JD.com, a large e-commerce company and a publisher of digital ads.},
	urldate = {2022-07-04},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2022},
	author = {Wang, Zenan and Carrion, Carlos and Lin, Xiliang and Ji, Fuhua and Bao, Yongjun and Yan, Weipeng},
	month = apr,
	year = {2022},
	note = {arXiv:2202.00846 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval},
	pages = {2247--2255},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/A87J396M/Wang et al. - 2022 - Adaptive Experimentation with Delayed Binary Feedb.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/TYPG53T5/2202.html:text/html},
}

@misc{radford_learning_2021-1,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {http://arxiv.org/abs/2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	urldate = {2022-07-04},
	publisher = {arXiv},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {arXiv:2103.00020 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/9FLJRBBN/Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/TIXAUYG6/2103.html:text/html},
}

@misc{noauthor_optimal_nodate,
	title = {Optimal {Learning}},
	url = {https://pubsonline.informs.org/doi/epdf/10.1287/educ.1080.0039},
	language = {en},
	urldate = {2022-07-04},
	doi = {10.1287/educ.1080.0039},
	file = {Full Text:/Users/yisu/Zotero/storage/TLYSQ7BG/Optimal Learning.pdf:application/pdf;Snapshot:/Users/yisu/Zotero/storage/8VBNI8D6/educ.1080.html:text/html},
}

@book{murphy_probabilistic_2022,
	address = {Cambridge, Massachusetts},
	series = {Adaptive computation and machine learning series},
	title = {Probabilistic machine learning: an introduction},
	isbn = {978-0-262-04682-4},
	shorttitle = {Probabilistic machine learning},
	abstract = {"This book provides a detailed and up-to-date coverage of machine learning. It is unique in that it unifies approaches based on deep learning with approaches based on probabilistic modeling and inference. It provides mathematical background (e.g. linear algebra, optimization), basic topics (e.g., linear and logistic regression, deep neural networks), as well as more advanced topics (e.g., Gaussian processes). It provides a perfect introduction for people who want to understand cutting edge work in top machine learning conferences such as NeurIPS, ICML and ICLR"--},
	language = {en},
	publisher = {The MIT Press},
	author = {Murphy, Kevin P.},
	year = {2022},
	keywords = {Machine learning, Probabilities},
	file = {Murphy - 2022 - Probabilistic machine learning an introduction.pdf:/Users/yisu/Zotero/storage/5MVD4EVS/Murphy - 2022 - Probabilistic machine learning an introduction.pdf:application/pdf},
}

@misc{riquelme_scaling_2021,
	title = {Scaling {Vision} with {Sparse} {Mixture} of {Experts}},
	url = {http://arxiv.org/abs/2106.05974},
	abstract = {Sparsely-gated Mixture of Experts networks (MoEs) have demonstrated excellent scalability in Natural Language Processing. In Computer Vision, however, almost all performant networks are "dense", that is, every input is processed by every parameter. We present a Vision MoE (V-MoE), a sparse version of the Vision Transformer, that is scalable and competitive with the largest dense networks. When applied to image recognition, V-MoE matches the performance of state-of-the-art networks, while requiring as little as half of the compute at inference time. Further, we propose an extension to the routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per-image compute. This allows V-MoE to trade-off performance and compute smoothly at test-time. Finally, we demonstrate the potential of V-MoE to scale vision models, and train a 15B parameter model that attains 90.35\% on ImageNet.},
	urldate = {2022-07-05},
	publisher = {arXiv},
	author = {Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Pinto, André Susano and Keysers, Daniel and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2106.05974 [cs, stat]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/22Y4TB7I/Riquelme et al. - 2021 - Scaling Vision with Sparse Mixture of Experts.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/7XNRH5NE/2106.html:text/html},
}

@misc{he_masked_2021,
	title = {Masked {Autoencoders} {Are} {Scalable} {Vision} {Learners}},
	url = {http://arxiv.org/abs/2111.06377},
	abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.},
	urldate = {2022-07-08},
	publisher = {arXiv},
	author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
	month = dec,
	year = {2021},
	note = {arXiv:2111.06377 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/4TY35I7T/He et al. - 2021 - Masked Autoencoders Are Scalable Vision Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/JEEJDNFG/2111.html:text/html},
}

@misc{noauthor_banditnet_nodate,
	title = {{BanditNet}: {Deep} {Learning} with {Logged} {Bandit} {Feedback}},
	url = {https://www.cs.cornell.edu/people/tj/banditnet/},
	urldate = {2022-07-09},
	file = {BanditNet\: Deep Learning with Logged Bandit Feedback:/Users/yisu/Zotero/storage/6BLAUDBL/banditnet.html:text/html},
}

@article{li_dissertation_nodate,
	title = {A dissertation submitted in partial satisfaction of the requirements for the degree {Doctor} of {Philosophy} in {Computer} {Science}},
	language = {en},
	author = {Li, Zeyu},
	pages = {183},
	file = {Li - A dissertation submitted in partial satisfaction o.pdf:/Users/yisu/Zotero/storage/AFXQL9R5/Li - A dissertation submitted in partial satisfaction o.pdf:application/pdf},
}

@misc{li_pytorch_2020,
	title = {{PyTorch} {Distributed}: {Experiences} on {Accelerating} {Data} {Parallel} {Training}},
	shorttitle = {{PyTorch} {Distributed}},
	url = {http://arxiv.org/abs/2006.15704},
	abstract = {This paper presents the design, implementation, and evaluation of the PyTorch distributed data parallel module. PyTorch is a widely-adopted scientific computing package used in deep learning research and applications. Recent advances in deep learning argue for the value of large datasets and large models, which necessitates the ability to scale out model training to more computational resources. Data parallelism has emerged as a popular solution for distributed training thanks to its straightforward principle and broad applicability. In general, the technique of distributed data parallelism replicates the model on every computational resource to generate gradients independently and then communicates those gradients at each iteration to keep model replicas consistent. Despite the conceptual simplicity of the technique, the subtle dependencies between computation and communication make it non-trivial to optimize the distributed training efficiency. As of v1.5, PyTorch natively provides several techniques to accelerate distributed data parallel, including bucketing gradients, overlapping computation with communication, and skipping gradient synchronization. Evaluations show that, when configured appropriately, the PyTorch distributed data parallel module attains near-linear scalability using 256 GPUs.},
	urldate = {2022-07-09},
	publisher = {arXiv},
	author = {Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and Chintala, Soumith},
	month = jun,
	year = {2020},
	note = {arXiv:2006.15704 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/DA34U5LK/Li et al. - 2020 - PyTorch Distributed Experiences on Accelerating D.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/DJKX9DPE/2006.html:text/html},
}

@misc{he_momentum_2020,
	title = {Momentum {Contrast} for {Unsupervised} {Visual} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1911.05722},
	abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
	urldate = {2022-07-10},
	publisher = {arXiv},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	month = mar,
	year = {2020},
	note = {arXiv:1911.05722 [cs]
version: 3},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/SIJYUWM5/He et al. - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/D2NIH88W/1911.html:text/html},
}

@article{shanmugam_elements_2018,
	title = {Elements of causal inference: foundations and learning algorithms},
	volume = {88},
	issn = {0094-9655, 1563-5163},
	shorttitle = {Elements of causal inference},
	url = {https://www.tandfonline.com/doi/full/10.1080/00949655.2018.1505197},
	doi = {10.1080/00949655.2018.1505197},
	language = {en},
	number = {16},
	urldate = {2022-07-10},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Shanmugam, Ramalingam},
	month = nov,
	year = {2018},
	pages = {3248--3248},
	file = {Shanmugam - 2018 - Elements of causal inference foundations and lear.pdf:/Users/yisu/Zotero/storage/H98ZNQA4/Shanmugam - 2018 - Elements of causal inference foundations and lear.pdf:application/pdf},
}

@misc{caron_deep_2019,
	title = {Deep {Clustering} for {Unsupervised} {Learning} of {Visual} {Features}},
	url = {http://arxiv.org/abs/1807.05520},
	abstract = {Clustering is a class of unsupervised learning methods that has been extensively applied and studied in computer vision. Little work has been done to adapt it to the end-to-end training of visual features on large scale datasets. In this work, we present DeepCluster, a clustering method that jointly learns the parameters of a neural network and the cluster assignments of the resulting features. DeepCluster iteratively groups the features with a standard clustering algorithm, k-means, and uses the subsequent assignments as supervision to update the weights of the network. We apply DeepCluster to the unsupervised training of convolutional neural networks on large datasets like ImageNet and YFCC100M. The resulting model outperforms the current state of the art by a significant margin on all the standard benchmarks.},
	urldate = {2022-07-11},
	publisher = {arXiv},
	author = {Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
	month = mar,
	year = {2019},
	note = {arXiv:1807.05520 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/LGJGZNZR/Caron et al. - 2019 - Deep Clustering for Unsupervised Learning of Visua.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/UNI2HMNE/1807.html:text/html},
}

@inproceedings{rahdari_magic_2022,
	address = {Barcelona Spain},
	title = {The {Magic} of {Carousels}: {Single} vs. {Multi}-{List} {Recommender} {Systems}},
	isbn = {978-1-4503-9233-4},
	shorttitle = {The {Magic} of {Carousels}},
	url = {https://dl.acm.org/doi/10.1145/3511095.3531278},
	doi = {10.1145/3511095.3531278},
	language = {en},
	urldate = {2022-07-13},
	booktitle = {Proceedings of the 33rd {ACM} {Conference} on {Hypertext} and {Social} {Media}},
	publisher = {ACM},
	author = {Rahdari, Behnam and Kveton, Branislav and Brusilovsky, Peter},
	month = jun,
	year = {2022},
	pages = {166--174},
	file = {Rahdari et al. - 2022 - The Magic of Carousels Single vs. Multi-List Reco.pdf:/Users/yisu/Zotero/storage/7AMNYKPR/Rahdari et al. - 2022 - The Magic of Carousels Single vs. Multi-List Reco.pdf:application/pdf},
}

@article{fu_cma-clip_nodate,
	title = {{CMA}-{CLIP}: {CROSS}-{MODALITY} {ATTENTION} {CLIP} {FOR} {TEXT}-{IMAGE} {CLASSIFICATION}},
	abstract = {Multi-modal learning with both text and images beneﬁts multiple applications, such as attribute extraction for e-commerce products. In this paper, we propose Cross-Modality Attention Contrastive Language-Image Pre-training (CMA-CLIP), a new multi-modal architecture to jointly learn the ﬁne-grained inter-modality relationship. It fuses CLIP with a sequencewise attention module and a modality-wise attention module. The network uses CLIP to bridge the inter-modality gap at the global level, and uses the sequence-wise attention module to capture the ﬁne-grained alignment between text and images. Besides, it leverages a modality-wise attention module to learn the relevance of each modality to downstream tasks, making the network robust against irrelevant modalities. CMA-CLIP outperforms the state-of-the-art method on Fashion-Gen by 5.5\% in accuracy, achieves competitive performance on Food101 and performance on par with the state-of-the-art method on MM-IMDb. We also demonstrate CMA-CLIP’s robustness against irrelevant modalities on an Amazon dataset for the task of product attribute extraction.},
	language = {en},
	author = {Fu, Jinmiao and Xu, Shaoyuan and Liu, Huidong and Liu, Yang and Xie, Ning and Wang, Chien-Chih and Liu, Jia and Sun, Yi and Wang, Bryan},
	pages = {5},
	file = {Fu et al. - CMA-CLIP CROSS-MODALITY ATTENTION CLIP FOR TEXT-I.pdf:/Users/yisu/Zotero/storage/4N4286QN/Fu et al. - CMA-CLIP CROSS-MODALITY ATTENTION CLIP FOR TEXT-I.pdf:application/pdf},
}

@misc{li_offline_2018,
	title = {Offline {Evaluation} of {Ranking} {Policies} with {Click} {Models}},
	url = {http://arxiv.org/abs/1804.10488},
	abstract = {Many web systems rank and present a list of items to users, from recommender systems to search and advertising. An important problem in practice is to evaluate new ranking policies offline and optimize them before they are deployed. We address this problem by proposing evaluation algorithms for estimating the expected number of clicks on ranked lists from historical logged data. The existing algorithms are not guaranteed to be statistically efficient in our problem because the number of recommended lists can grow exponentially with their length. To overcome this challenge, we use models of user interaction with the list of items, the so-called click models, to construct estimators that learn statistically efficiently. We analyze our estimators and prove that they are more efficient than the estimators that do not use the structure of the click model, under the assumption that the click model holds. We evaluate our estimators in a series of experiments on a real-world dataset and show that they consistently outperform prior estimators.},
	urldate = {2022-08-24},
	publisher = {arXiv},
	author = {Li, Shuai and Abbasi-Yadkori, Yasin and Kveton, Branislav and Muthukrishnan, S. and Vinay, Vishwa and Wen, Zheng},
	month = jun,
	year = {2018},
	note = {arXiv:1804.10488 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/ZDQLXXX4/Li et al. - 2018 - Offline Evaluation of Ranking Policies with Click .pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/TJ87LMCA/1804.html:text/html},
}

@inproceedings{beygelzimer_offset_2009,
	address = {Paris, France},
	title = {The offset tree for learning with partial labels},
	isbn = {978-1-60558-495-9},
	url = {http://portal.acm.org/citation.cfm?doid=1557019.1557040},
	doi = {10.1145/1557019.1557040},
	abstract = {We present an algorithm, called the Oﬀset Tree, for learning to make decisions in situations where the payoﬀ of only one choice is observed, rather than all choices. The algorithm reduces this setting to binary classiﬁcation, allowing one to reuse of any existing, fully supervised binary classiﬁcation algorithm in this partial information setting. We show that the Oﬀset Tree is an optimal reduction to binary classiﬁcation. In particular, it has regret at most (k − 1) times the regret of the binary classiﬁer it uses (where k is the number of choices), and no reduction to binary classiﬁcation can do better. This reduction is also computationally optimal, both at training and test time, requiring just O(log2 k) work to train on an example or make a prediction.},
	language = {en},
	urldate = {2022-08-27},
	booktitle = {Proceedings of the 15th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining - {KDD} '09},
	publisher = {ACM Press},
	author = {Beygelzimer, Alina and Langford, John},
	year = {2009},
	pages = {129},
	file = {Beygelzimer and Langford - 2009 - The offset tree for learning with partial labels.pdf:/Users/yisu/Zotero/storage/W3EIIPEW/Beygelzimer and Langford - 2009 - The offset tree for learning with partial labels.pdf:application/pdf},
}

@article{rockafellar_optimization_2000,
	title = {Optimization of conditional value-at-risk},
	volume = {2},
	issn = {14651211},
	url = {http://www.risk.net/journal-of-risk/technical-paper/2161159/optimization-conditional-value-risk},
	doi = {10.21314/JOR.2000.038},
	language = {en},
	number = {3},
	urldate = {2022-09-04},
	journal = {The Journal of Risk},
	author = {Rockafellar, R. Tyrrell and Uryasev, Stanislav},
	year = {2000},
	pages = {21--41},
	file = {Rockafellar and Uryasev - 2000 - Optimization of conditional value-at-risk.pdf:/Users/yisu/Zotero/storage/88ASZARX/Rockafellar and Uryasev - 2000 - Optimization of conditional value-at-risk.pdf:application/pdf},
}

@article{abadie_synthetic_2010,
	title = {Synthetic {Control} {Methods} for {Comparative} {Case} {Studies}: {Estimating} the {Effect} of {California}’s {Tobacco} {Control} {Program}},
	volume = {105},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Synthetic {Control} {Methods} for {Comparative} {Case} {Studies}},
	url = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746},
	doi = {10.1198/jasa.2009.ap08746},
	language = {en},
	number = {490},
	urldate = {2022-09-05},
	journal = {Journal of the American Statistical Association},
	author = {Abadie, Alberto and Diamond, Alexis and Hainmueller, Jens},
	month = jun,
	year = {2010},
	pages = {493--505},
	file = {Abadie et al. - 2010 - Synthetic Control Methods for Comparative Case Stu.pdf:/Users/yisu/Zotero/storage/BFD6ISHS/Abadie et al. - 2010 - Synthetic Control Methods for Comparative Case Stu.pdf:application/pdf},
}

@article{luxenberg_portfolio_2022,
	title = {Portfolio {Construction} with {Gaussian} {Mixture} {Returns} and {Exponential} {Utility} via {Convex} {Optimization}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=4104852},
	doi = {10.2139/ssrn.4104852},
	abstract = {We consider the problem of choosing an optimal portfolio, assuming the asset returns have a Gaussian mixture (GM) distribution, with the objective of maximizing expected exponential utility. In this paper we show that this problem is convex, and readily solved exactly using domain-speciﬁc languages for convex optimization, without the need for sampling or scenarios. We then show how the closely related problem of minimizing entropic value at risk can also be formulated as a convex optimization problem.},
	language = {en},
	urldate = {2022-09-07},
	journal = {SSRN Electronic Journal},
	author = {Luxenberg, Eric and Boyd, Stephen},
	year = {2022},
	file = {Luxenberg and Boyd - 2022 - Portfolio Construction with Gaussian Mixture Retur.pdf:/Users/yisu/Zotero/storage/FCEWY633/Luxenberg and Boyd - 2022 - Portfolio Construction with Gaussian Mixture Retur.pdf:application/pdf},
}

@inproceedings{baudry_optimal_2021,
	title = {Optimal {Thompson} {Sampling} strategies for support-aware {CVaR} bandits},
	url = {https://proceedings.mlr.press/v139/baudry21a.html},
	abstract = {In this paper we study a multi-arm bandit problem in which the quality of each arm is measured by the Conditional Value at Risk (CVaR) at some level alpha of the reward distribution. While existing works in this setting mainly focus on Upper Confidence Bound algorithms, we introduce a new Thompson Sampling approach for CVaR bandits on bounded rewards that is flexible enough to solve a variety of problems grounded on physical resources. Building on a recent work by Riou \& Honda (2020), we introduce B-CVTS for continuous bounded rewards and M-CVTS for multinomial distributions. On the theoretical side, we provide a non-trivial extension of their analysis that enables to theoretically bound their CVaR regret minimization performance. Strikingly, our results show that these strategies are the first to provably achieve asymptotic optimality in CVaR bandits, matching the corresponding asymptotic lower bounds for this setting. Further, we illustrate empirically the benefit of Thompson Sampling approaches both in a realistic environment simulating a use-case in agriculture and on various synthetic examples.},
	language = {en},
	urldate = {2022-09-07},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Baudry, Dorian and Gautron, Romain and Kaufmann, Emilie and Maillard, Odalric},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {716--726},
	file = {Full Text PDF:/Users/yisu/Zotero/storage/EW5RQK75/Baudry et al. - 2021 - Optimal Thompson Sampling strategies for support-a.pdf:application/pdf;Supplementary PDF:/Users/yisu/Zotero/storage/TBRL7XDK/Baudry et al. - 2021 - Optimal Thompson Sampling strategies for support-a.pdf:application/pdf},
}

@inproceedings{riou_bandit_2020,
	title = {Bandit {Algorithms} {Based} on {Thompson} {Sampling} for {Bounded} {Reward} {Distributions}},
	url = {https://proceedings.mlr.press/v117/riou20a.html},
	abstract = {We focus on a classic reinforcement learning problem, called a multi-armed bandit, and more specifically in the stochastic setting with reward distributions bounded in \$[0,1]\$. For this model, an optimal problem-dependent asymptotic regret lower bound has been derived. However, the existing algorithms achieving this regret lower bound all require to solve an optimization problem at each step, inducing a large complexity. In this paper, we propose two new algorithms, which we prove to achieve the problem-dependent asymptotic regret lower bound. The first one, which we call Multinomial TS, is an adaptation of Thompson Sampling for Bernoulli rewards to multinomial reward distributions whose support is included in \${\textbackslash}\{0, {\textbackslash}frac\{1\}\{M\}, …, 1{\textbackslash}\}\$. This algorithm achieves the regret lower bound in the case of multinomial distributions with the aforementioned support, and it can be easily generalized to bounded reward distributions in \$[0, 1]\$ by randomly rounding the observed rewards. The second algorithm we introduce, which we call Non-parametric TS, is a randomized algorithm but not based on the posterior sampling in the strict sense. At each step, it computes an average of the observed rewards with random weight. Not only is it asymptotically optimal, but also it performs very well even for small horizons.},
	language = {en},
	urldate = {2022-09-07},
	booktitle = {Proceedings of the 31st {International} {Conference}  on {Algorithmic} {Learning} {Theory}},
	publisher = {PMLR},
	author = {Riou, Charles and Honda, Junya},
	month = jan,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {777--826},
	file = {Full Text PDF:/Users/yisu/Zotero/storage/7D3KE7QY/Riou and Honda - 2020 - Bandit Algorithms Based on Thompson Sampling for B.pdf:application/pdf},
}

@inproceedings{vakili_mean-variance_2015,
	title = {Mean-variance and value at risk in multi-armed bandit problems},
	doi = {10.1109/ALLERTON.2015.7447162},
	abstract = {We study risk-averse multi-armed bandit problems under different risk measures. We consider three risk mitigation models. In the first model, the variations in the reward values obtained at different times are considered as risk and the objective is to minimize the mean-variance of the observed rewards. In the second and the third models, the quantity of interest is the total reward at the end of the time horizon, and the objective is to minimize the mean-variance and maximize the value at risk of the total reward, respectively. We develop risk-averse online learning policies and analyze their regret performance. We also provide tight lower bounds on regret under the model of mean-variance of observations.},
	booktitle = {2015 53rd {Annual} {Allerton} {Conference} on {Communication}, {Control}, and {Computing} ({Allerton})},
	author = {Vakili, Sattar and Zhao, Qing},
	month = sep,
	year = {2015},
	keywords = {Biological system modeling, Computational modeling, Decision making, Investment, Random variables, Reactive power, Risk management},
	pages = {1330--1335},
	file = {IEEE Xplore Abstract Record:/Users/yisu/Zotero/storage/762SCYZ9/7447162.html:text/html;IEEE Xplore Full Text PDF:/Users/yisu/Zotero/storage/WR9XAM6V/Vakili and Zhao - 2015 - Mean-variance and value at risk in multi-armed ban.pdf:application/pdf},
}

@misc{du_continuous_2021,
	title = {Continuous {Mean}-{Covariance} {Bandits}},
	url = {http://arxiv.org/abs/2102.12090},
	abstract = {Existing risk-aware multi-armed bandit models typically focus on risk measures of individual options such as variance. As a result, they cannot be directly applied to important real-world online decision making problems with correlated options. In this paper, we propose a novel Continuous Mean-Covariance Bandit (CMCB) model to explicitly take into account option correlation. Specifically, in CMCB, there is a learner who sequentially chooses weight vectors on given options and observes random feedback according to the decisions. The agent's objective is to achieve the best trade-off between reward and risk, measured with option covariance. To capture different reward observation scenarios in practice, we consider three feedback settings, i.e., full-information, semi-bandit and full-bandit feedback. We propose novel algorithms with optimal regrets (within logarithmic factors), and provide matching lower bounds to validate their optimalities. The experimental results also demonstrate the superiority of our algorithms. To the best of our knowledge, this is the first work that considers option correlation in risk-aware bandits and explicitly quantifies how arbitrary covariance structures impact the learning performance. The novel analytical techniques we developed for exploiting the estimated covariance to build concentration and bounding the risk of selected actions based on sampling strategy properties can likely find applications in other bandit analysis and be of independent interests.},
	urldate = {2022-09-06},
	publisher = {arXiv},
	author = {Du, Yihan and Wang, Siwei and Fang, Zhixuan and Huang, Longbo},
	month = oct,
	year = {2021},
	note = {arXiv:2102.12090 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/LDHPHYDP/Du et al. - 2021 - Continuous Mean-Covariance Bandits.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/J9Y8544D/2102.html:text/html},
}

@article{kikawa_estimation_2015,
	title = {On the {Estimation} of a {Univariate} {Gaussian} {Distribution}: {A} {Comparative} {Approach}},
	volume = {05},
	issn = {2161-718X, 2161-7198},
	shorttitle = {On the {Estimation} of a {Univariate} {Gaussian} {Distribution}},
	url = {http://www.scirp.org/journal/doi.aspx?DOI=10.4236/ojs.2015.55046},
	doi = {10.4236/ojs.2015.55046},
	abstract = {Estimation of the unknown mean, μ and variance, σ2 of a univariate Gaussian distribution ( ) N µ,σ 2 given a single study variable x is considered. We propose an approach that does not require initialization of the sufficient unknown distribution parameters. The approach is motivated by linearizing the Gaussian distribution through differential techniques, and estimating, μ and σ2 as regression coefficients using the ordinary least squares method. Two simulated datasets on hereditary traits and morphometric analysis of housefly strains are used to evaluate the proposed method (PM), the maximum likelihood estimation (MLE), and the method of moments (MM). The methods are evaluated by re-estimating the required Gaussian parameters on both large and small samples. The root mean squared error (RMSE), mean error (ME), and the standard deviation (SD) are used to assess the accuracy of the PM and MLE; confidence intervals (CIs) are also constructed for the ME estimate. The PM compares well with both the MLE and MM approaches as they all produce estimates whose errors have good asymptotic properties, also small CIs are observed for the ME using the PM and MLE. The PM can be used symbiotically with the MLE to provide initial approximations at the expectation maximization step.},
	language = {en},
	number = {05},
	urldate = {2022-09-11},
	journal = {Open Journal of Statistics},
	author = {Kikawa, Cliff R. and Shatalov, Michael Y. and Kloppers, Petrus H. and Mkolesia, Andrew C.},
	year = {2015},
	pages = {445--454},
	file = {Kikawa et al. - 2015 - On the Estimation of a Univariate Gaussian Distrib.pdf:/Users/yisu/Zotero/storage/TIT4I8KH/Kikawa et al. - 2015 - On the Estimation of a Univariate Gaussian Distrib.pdf:application/pdf},
}

@article{bubeck_bounded_nodate,
	title = {Bounded regret in stochastic multi-armed bandits},
	abstract = {We study the stochastic multi-armed bandit problem when one knows the value µ(⋆) of an optimal arm, as a well as a positive lower bound on the smallest positive gap ∆. We propose a new randomized policy that attains a regret uniformly bounded over time in this setting. We also prove several lower bounds, which show in particular that bounded regret is not possible if one only knows ∆, and bounded regret of order 1/∆ is not possible if one only knows µ(⋆).},
	language = {en},
	author = {Bubeck, Sebastien and Perchet, Vianney and Rigollet, Philippe},
	pages = {13},
	file = {Bubeck et al. - Bounded regret in stochastic multi-armed bandits.pdf:/Users/yisu/Zotero/storage/B2WJV96F/Bubeck et al. - Bounded regret in stochastic multi-armed bandits.pdf:application/pdf},
}

@article{auer_using_nodate,
	title = {Using {Conﬁdence} {Bounds} for {Exploitation}-{Exploration} {Trade}-oﬀs},
	abstract = {We show how a standard tool from statistics — namely conﬁdence bounds — can be used to elegantly deal with situations which exhibit an exploitation-exploration trade-oﬀ. Our technique for designing and analyzing algorithms for such situations is general and can be applied when an algorithm has to make exploitation-versus-exploration decisions based on uncertain information provided by a random process.},
	language = {en},
	author = {Auer, Peter},
	pages = {26},
	file = {Auer - Using Conﬁdence Bounds for Exploitation-Exploratio.pdf:/Users/yisu/Zotero/storage/6FHG4QSD/Auer - Using Conﬁdence Bounds for Exploitation-Exploratio.pdf:application/pdf},
}

@misc{agrawal_analysis_2012,
	title = {Analysis of {Thompson} {Sampling} for the multi-armed bandit problem},
	url = {http://arxiv.org/abs/1111.1797},
	abstract = {The multi-armed bandit problem is a popular model for studying exploration/exploitation trade-off in sequential decision problems. Many algorithms are now available for this well-studied problem. One of the earliest algorithms, given by W. R. Thompson, dates back to 1933. This algorithm, referred to as Thompson Sampling, is a natural Bayesian algorithm. The basic idea is to choose an arm to play according to its probability of being the best arm. Thompson Sampling algorithm has experimentally been shown to be close to optimal. In addition, it is efficient to implement and exhibits several desirable properties such as small regret for delayed feedback. However, theoretical understanding of this algorithm was quite limited. In this paper, for the first time, we show that Thompson Sampling algorithm achieves logarithmic expected regret for the multi-armed bandit problem. More precisely, for the two-armed bandit problem, the expected regret in time \$T\$ is \$O({\textbackslash}frac\{{\textbackslash}ln T\}\{{\textbackslash}Delta\} + {\textbackslash}frac\{1\}\{{\textbackslash}Delta{\textasciicircum}3\})\$. And, for the \$N\$-armed bandit problem, the expected regret in time \$T\$ is \$O([({\textbackslash}sum\_\{i=2\}{\textasciicircum}N {\textbackslash}frac\{1\}\{{\textbackslash}Delta\_i{\textasciicircum}2\}){\textasciicircum}2] {\textbackslash}ln T)\$. Our bounds are optimal but for the dependence on \${\textbackslash}Delta\_i\$ and the constant factors in big-Oh.},
	urldate = {2022-09-12},
	publisher = {arXiv},
	author = {Agrawal, Shipra and Goyal, Navin},
	month = apr,
	year = {2012},
	note = {arXiv:1111.1797 [cs]},
	keywords = {Computer Science - Machine Learning, 68W40, 68Q25, Computer Science - Data Structures and Algorithms, F.2.0},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/NB93275F/Agrawal and Goyal - 2012 - Analysis of Thompson Sampling for the multi-armed .pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/FRC6PBYB/1111.html:text/html},
}

@misc{bubeck_regret_2012,
	title = {Regret {Analysis} of {Stochastic} and {Nonstochastic} {Multi}-armed {Bandit} {Problems}},
	url = {http://arxiv.org/abs/1204.5721},
	abstract = {Multi-armed bandit problems are the most basic examples of sequential decision problems with an exploration-exploitation trade-off. This is the balance between staying with the option that gave highest payoffs in the past and exploring new options that might give higher payoffs in the future. Although the study of bandit problems dates back to the Thirties, exploration-exploitation trade-offs arise in several modern applications, such as ad placement, website optimization, and packet routing. Mathematically, a multi-armed bandit is defined by the payoff process associated with each option. In this survey, we focus on two extreme cases in which the analysis of regret is particularly simple and elegant: i.i.d. payoffs and adversarial payoffs. Besides the basic setting of finitely many actions, we also analyze some of the most important variants and extensions, such as the contextual bandit model.},
	urldate = {2022-09-12},
	publisher = {arXiv},
	author = {Bubeck, Sébastien and Cesa-Bianchi, Nicolò},
	month = nov,
	year = {2012},
	note = {arXiv:1204.5721 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/2B3TISAG/Bubeck and Cesa-Bianchi - 2012 - Regret Analysis of Stochastic and Nonstochastic Mu.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/IFF4QJYZ/1204.html:text/html},
}

@article{angrist_identification_1996,
	title = {Identification of {Causal} {Effects} {Using} {Instrumental} {Variables}},
	language = {en},
	journal = {Journal of the American Statistical Association},
	author = {Angrist, Joshua D and Imbens, Guido W and Rubin, Donald B},
	year = {1996},
	pages = {29},
	file = {Angrist et al. - 1996 - Identification of Causal Effects Using Instrumenta.pdf:/Users/yisu/Zotero/storage/TF6AIZTL/Angrist et al. - 1996 - Identification of Causal Effects Using Instrumenta.pdf:application/pdf},
}

@inproceedings{graves_practical_2011,
	title = {Practical {Variational} {Inference} for {Neural} {Networks}},
	volume = {24},
	url = {https://papers.nips.cc/paper/2011/hash/7eb3c8be3d411e8ebfab08eba5f49632-Abstract.html},
	abstract = {Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural networks. Along the way it revisits several common regularisers from a variational perspective. It also provides a simple pruning heuristic that can both drastically reduce the number of network weights and lead to improved generalisation. Experimental results are provided for a hierarchical multidimensional recurrent neural network applied to the TIMIT speech corpus.},
	urldate = {2022-09-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Graves, Alex},
	year = {2011},
	file = {Full Text PDF:/Users/yisu/Zotero/storage/67NJCWKM/Graves - 2011 - Practical Variational Inference for Neural Network.pdf:application/pdf},
}

@misc{li_provably_2017,
	title = {Provably {Optimal} {Algorithms} for {Generalized} {Linear} {Contextual} {Bandits}},
	url = {http://arxiv.org/abs/1703.00048},
	abstract = {Contextual bandits are widely used in Internet services from news recommendation to advertising, and to Web search. Generalized linear models (logistical regression in particular) have demonstrated stronger performance than linear models in many applications where rewards are binary. However, most theoretical analyses on contextual bandits so far are on linear bandits. In this work, we propose an upper confidence bound based algorithm for generalized linear contextual bandits, which achieves an \${\textbackslash}tilde\{O\}({\textbackslash}sqrt\{dT\})\$ regret over \$T\$ rounds with \$d\$ dimensional feature vectors. This regret matches the minimax lower bound, up to logarithmic terms, and improves on the best previous result by a \${\textbackslash}sqrt\{d\}\$ factor, assuming the number of arms is fixed. A key component in our analysis is to establish a new, sharp finite-sample confidence bound for maximum-likelihood estimates in generalized linear models, which may be of independent interest. We also analyze a simpler upper confidence bound algorithm, which is useful in practice, and prove it to have optimal regret for certain cases.},
	urldate = {2022-09-21},
	publisher = {arXiv},
	author = {Li, Lihong and Lu, Yu and Zhou, Dengyong},
	month = jun,
	year = {2017},
	note = {arXiv:1703.00048 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/9XM2B3PD/Li et al. - 2017 - Provably Optimal Algorithms for Generalized Linear.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/NQ7FR8CY/1703.html:text/html},
}

@misc{russo_learning_2014,
	title = {Learning to {Optimize} {Via} {Posterior} {Sampling}},
	url = {http://arxiv.org/abs/1301.2609},
	abstract = {This paper considers the use of a simple posterior sampling algorithm to balance between exploration and exploitation when learning to optimize actions such as in multi-armed bandit problems. The algorithm, also known as Thompson Sampling, offers significant advantages over the popular upper confidence bound (UCB) approach, and can be applied to problems with finite or infinite action spaces and complicated relationships among action rewards. We make two theoretical contributions. The first establishes a connection between posterior sampling and UCB algorithms. This result lets us convert regret bounds developed for UCB algorithms into Bayesian regret bounds for posterior sampling. Our second theoretical contribution is a Bayesian regret bound for posterior sampling that applies broadly and can be specialized to many model classes. This bound depends on a new notion we refer to as the eluder dimension, which measures the degree of dependence among action rewards. Compared to UCB algorithm Bayesian regret bounds for specific model classes, our general bound matches the best available for linear models and is stronger than the best available for generalized linear models. Further, our analysis provides insight into performance advantages of posterior sampling, which are highlighted through simulation results that demonstrate performance surpassing recently proposed UCB algorithms.},
	urldate = {2022-09-23},
	publisher = {arXiv},
	author = {Russo, Daniel and Van Roy, Benjamin},
	month = feb,
	year = {2014},
	note = {arXiv:1301.2609 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/9XSHUXUK/Russo and Van Roy - 2014 - Learning to Optimize Via Posterior Sampling.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/QS4Z3977/1301.html:text/html},
}

@inproceedings{momma_multi-objective_2020,
	address = {Taipei Taiwan},
	title = {Multi-objective {Ranking} via {Constrained} {Optimization}},
	isbn = {978-1-4503-7024-0},
	url = {https://dl.acm.org/doi/10.1145/3366424.3382723},
	doi = {10.1145/3366424.3382723},
	abstract = {In this paper, we introduce an Augmented Lagrangian based method to incorporate the multiple objectives (MO) in a search ranking algorithm. Optimizing MOs is an essential and realistic requirement for building ranking models in production. The proposed method formulates MO in constrained optimization and solves the problem in the popular Boosting framework – a novel contribution of our work. Furthermore, we propose a procedure to set up all optimization parameters in the problem. The experimental results show that the method successfully achieves MO criteria much more efficiently than existing methods.},
	language = {en},
	urldate = {2022-09-26},
	booktitle = {Companion {Proceedings} of the {Web} {Conference} 2020},
	publisher = {ACM},
	author = {Momma, Michinari and Bagheri Garakani, Alireza and Ma, Nanxun and Sun, Yi},
	month = apr,
	year = {2020},
	pages = {111--112},
	file = {Momma et al. - 2020 - Multi-objective Ranking via Constrained Optimizati.pdf:/Users/yisu/Zotero/storage/VCDSWDGB/Momma et al. - 2020 - Multi-objective Ranking via Constrained Optimizati.pdf:application/pdf},
}

@inproceedings{agarwal_constrained_2015,
	address = {Florence Italy},
	title = {Constrained {Optimization} for {Homepage} {Relevance}},
	isbn = {978-1-4503-3473-0},
	url = {https://dl.acm.org/doi/10.1145/2740908.2745398},
	doi = {10.1145/2740908.2745398},
	abstract = {This paper considers an application of showing promotional widgets to web users on the homepage of a major professional social network site. The types of widgets include address book invitation, group join, friends’ skill endorsement and so forth. The objective is to optimize user engagement under certain business constraints. User actions on each widget may have very different downstream utilities, and quantiﬁcation of such utilities can sometimes be quite difﬁcult. Since there are multiple widgets to rank when a user visits, launching a personalized model to simply optimize user engagement such as clicks is often inappropriate. In this paper we propose a scalable constrained optimization framework to solve this problem. We consider several different types of constraints according to the business needs for this application. We show through both ofﬂine experiments and online A/B tests that our optimization framework can lead to signiﬁcant improvement in user engagement while satisfying the desired set of business objectives.},
	language = {en},
	urldate = {2022-09-26},
	booktitle = {Proceedings of the 24th {International} {Conference} on {World} {Wide} {Web}},
	publisher = {ACM},
	author = {Agarwal, Deepak and Chatterjee, Shaunak and Yang, Yang and Zhang, Liang},
	month = may,
	year = {2015},
	pages = {375--384},
	file = {Agarwal et al. - 2015 - Constrained Optimization for Homepage Relevance.pdf:/Users/yisu/Zotero/storage/TL78IPPU/Agarwal et al. - 2015 - Constrained Optimization for Homepage Relevance.pdf:application/pdf},
}

@inproceedings{pacchiano_stochastic_2021,
	title = {Stochastic {Bandits} with {Linear} {Constraints}},
	url = {https://proceedings.mlr.press/v130/pacchiano21a.html},
	abstract = {We study a constrained contextual linear bandit setting, where the goal of the agent is to produce a sequence of policies, whose expected cumulative reward over the course of multiple rounds is maximum, and each one of them has an expected cost below a certain threshold. We propose an upper-confidence bound algorithm for this problem, called optimistic pessimistic linear bandit (OPLB), and prove a sublinear bound on its regret that is inversely proportional to the difference between the constraint threshold and the cost of a known feasible action. Our algorithm balances exploration and constraint satisfaction using a novel idea that scales the radii of the reward and cost confidence sets with different scaling factors. We further specialize our results to multi-armed bandits and propose a computationally efficient algorithm for this setting and prove a a regret bound that is better than simply casting multi-armed bandits as an instance of linear bandits and using the regret bound of OPLB. We also prove a lower-bound for the problem studied in the paper and provide simulations to validate our theoretical results. Finally, we show how our algorithm and analysis can be extended to multiple constraints and to the case when the cost of the feasible action is unknown.},
	language = {en},
	urldate = {2022-09-26},
	booktitle = {Proceedings of {The} 24th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Pacchiano, Aldo and Ghavamzadeh, Mohammad and Bartlett, Peter and Jiang, Heinrich},
	month = mar,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {2827--2835},
	file = {Full Text PDF:/Users/yisu/Zotero/storage/VSFRZENA/Pacchiano et al. - 2021 - Stochastic Bandits with Linear Constraints.pdf:application/pdf;Supplementary PDF:/Users/yisu/Zotero/storage/MR7XSFA8/Pacchiano et al. - 2021 - Stochastic Bandits with Linear Constraints.pdf:application/pdf},
}

@inproceedings{kveton_randomized_2020,
	title = {Randomized {Exploration} in {Generalized} {Linear} {Bandits}},
	url = {https://proceedings.mlr.press/v108/kveton20a.html},
	abstract = {We study two randomized algorithms for generalized linear bandits. The first, GLM-TSL, samples a generalized linear model (GLM) from the Laplace approximation to the posterior distribution. The second, GLM-FPL, fits a GLM to a randomly perturbed history of past rewards. We analyze both algorithms and derive \${\textbackslash}tilde\{O\}(d {\textbackslash}sqrt\{n {\textbackslash}log K\})\$ upper bounds on their \$n\$-round regret, where \$d\$ is the number of features and \$K\$ is the number of arms. The former improves on prior work while the latter is the first for Gaussian noise perturbations in non-linear models. We empirically evaluate both GLM-TSL and GLM-FPL in logistic bandits, and apply GLM-FPL to neural network bandits. Our work showcases the role of randomization, beyond posterior sampling, in exploration.},
	language = {en},
	urldate = {2022-09-30},
	booktitle = {Proceedings of the {Twenty} {Third} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Kveton, Branislav and Zaheer, Manzil and Szepesvari, Csaba and Li, Lihong and Ghavamzadeh, Mohammad and Boutilier, Craig},
	month = jun,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {2066--2076},
	file = {Full Text PDF:/Users/yisu/Zotero/storage/6LVPSG2S/Kveton et al. - 2020 - Randomized Exploration in Generalized Linear Bandi.pdf:application/pdf;Supplementary PDF:/Users/yisu/Zotero/storage/YUTZ8TVE/Kveton et al. - 2020 - Randomized Exploration in Generalized Linear Bandi.pdf:application/pdf},
}

@misc{rendle_bpr_2012,
	title = {{BPR}: {Bayesian} {Personalized} {Ranking} from {Implicit} {Feedback}},
	shorttitle = {{BPR}},
	url = {http://arxiv.org/abs/1205.2618},
	abstract = {Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products). In this paper, we investigate the most common scenario with implicit feedback (e.g. clicks, purchases). There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive knearest-neighbor (kNN). Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking. In this paper we present a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem. We also provide a generic learning algorithm for optimizing models with respect to BPR-Opt. The learning method is based on stochastic gradient descent with bootstrap sampling. We show how to apply our method to two state-of-the-art recommender models: matrix factorization and adaptive kNN. Our experiments indicate that for the task of personalized ranking our optimization method outperforms the standard learning techniques for MF and kNN. The results show the importance of optimizing models for the right criterion.},
	urldate = {2022-10-01},
	publisher = {arXiv},
	author = {Rendle, Steffen and Freudenthaler, Christoph and Gantner, Zeno and Schmidt-Thieme, Lars},
	month = may,
	year = {2012},
	note = {arXiv:1205.2618 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/8FXY54HB/Rendle et al. - 2012 - BPR Bayesian Personalized Ranking from Implicit F.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/PLW9PFRP/1205.html:text/html},
}

@misc{osband_deep_2019,
	title = {Deep {Exploration} via {Randomized} {Value} {Functions}},
	url = {http://arxiv.org/abs/1703.07608},
	abstract = {We study the use of randomized value functions to guide deep exploration in reinforcement learning. This offers an elegant means for synthesizing statistically and computationally efficient exploration with common practical approaches to value function learning. We present several reinforcement learning algorithms that leverage randomized value functions and demonstrate their efficacy through computational studies. We also prove a regret bound that establishes statistical efficiency with a tabular representation.},
	urldate = {2022-10-01},
	publisher = {arXiv},
	author = {Osband, Ian and Van Roy, Benjamin and Russo, Daniel and Wen, Zheng},
	month = sep,
	year = {2019},
	note = {arXiv:1703.07608 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/RZ7UGKDK/Osband et al. - 2019 - Deep Exploration via Randomized Value Functions.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/VJWQLWKU/1703.html:text/html},
}

@article{sun_traffic_2021,
	title = {Traffic {Shaping} in {E}-{Commercial} {Search} {Engine}: {Multi}-{Objective} {Online} {Welfare} {Maximization}},
	volume = {35},
	issn = {2374-3468, 2159-5399},
	shorttitle = {Traffic {Shaping} in {E}-{Commercial} {Search} {Engine}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/16136},
	doi = {10.1609/aaai.v35i1.16136},
	abstract = {The e-commercial search engine is the primary gateway for customers to ﬁnd desired products and engage in online shopping. Besides displaying items to optimize for a single objective (i.e., relevance), ranking items needs to satisfy some other business requirements in practice. Recently, trafﬁc shaping was introduced to incorporate multiple objectives in a constrained optimization framework. However, many practical business requirements can not explicitly represented by linear constraints as in the existing work, and this may limit the scalability of their framework. This paper presents a uniﬁed framework from the aspect of multi-objective welfare maximization where we regard all business requirements as objectives to optimize. Our framework can naturally incorporate a wide range of application-driven requirements. In addition to formulating the problem, we design an online trafﬁc splitting algorithm that allows us to ﬂexibly adjust the priorities of different objectives, and it has rigorous theoretical guarantees over the adversarial scenario. We also run experiments on both synthetic and real-world datasets to validate our algorithms.},
	language = {en},
	number = {1},
	urldate = {2022-10-11},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Sun, Liucheng and Weng, Chenwei and Huo, Chengfu and Ren, Weijun and Zhang, Guochuan and Li, Xin},
	month = may,
	year = {2021},
	pages = {574--581},
	file = {Sun et al. - 2021 - Traffic Shaping in E-Commercial Search Engine Mul.pdf:/Users/yisu/Zotero/storage/852AMD4F/Sun et al. - 2021 - Traffic Shaping in E-Commercial Search Engine Mul.pdf:application/pdf},
}

@article{shah_online_nodate,
	title = {Online {Ranking} with {Constraints}: {A} {Primal}-{Dual} {Algorithm} and {Applications} to {Web} {Traﬃc}-{Shaping}},
	abstract = {We study the online constrained ranking problem motivated by an application to web-traﬃc shaping: an online stream of sessions arrive in which, within each session, we are asked to rank items. The challenge involves optimizing the ranking in each session so that local vs. global objectives are controlled: within each session one wishes to maximize a reward (local) while satisfying certain constraints over the entire set of sessions (global). A typical application of this setup is that of page optimization in a web portal. We wish to rank items so that not only is user engagement maximized in each session, but also other business constraints (such as the number of views/clicks delivered to various publishing partners) are satisﬁed.},
	language = {en},
	author = {Shah, Parikshit and Soni, Akshay and Chevalier, Troy},
	pages = {23},
	file = {Shah et al. - Online Ranking with Constraints A Primal-Dual Alg.pdf:/Users/yisu/Zotero/storage/FANHZL6M/Shah et al. - Online Ranking with Constraints A Primal-Dual Alg.pdf:application/pdf},
}

@inproceedings{chen_real-time_2011,
	address = {San Diego, California, USA},
	title = {Real-time bidding algorithms for performance-based display ad allocation},
	isbn = {978-1-4503-0813-7},
	url = {http://dl.acm.org/citation.cfm?doid=2020408.2020604},
	doi = {10.1145/2020408.2020604},
	abstract = {We describe a real-time bidding algorithm for performancebased display ad allocation. A central issue in performance display advertising is matching campaigns to ad impressions, which can be formulated as a constrained optimization problem that maximizes revenue subject to constraints such as budget limits and inventory availability. The current practice is to solve the optimization problem oﬄine at a tractable level of impression granularity (e.g., the placement level), and to serve ads online based on the precomputed static delivery scheme. Although this oﬄine approach takes a global view to achieve optimality, it fails to scale to ad delivery decision making at an individual impression level. Therefore, we propose a real-time bidding algorithm that enables ﬁne-grained impression valuation (e.g., targeting users with real-time conversion data), and adjusts value-based bid according to real-time constraint snapshot (e.g., budget consumption level). Theoretically, we show that under a linear programming (LP) primal-dual formulation, the simple real-time bidding algorithm is indeed an online solver to the original primal problem by taking the optimal solution to the dual problem as input. In other words, the online algorithm guarantees the oﬄine optimality given the same level of knowledge an oﬄine optimization would have. Empirically, we develop and experiment with two real-time bid adjustment approaches to adapting to the non-stationary nature of the marketplace: one adjusts bids against real-time constraint satisfaction level using control-theoretic methods, and the other adjusts bids also based on the historical bidding landscape statistically modeled. Finally, we show experimental results with real-world ad serving data.},
	language = {en},
	urldate = {2022-10-10},
	booktitle = {Proceedings of the 17th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining - {KDD} '11},
	publisher = {ACM Press},
	author = {Chen, Ye and Berkhin, Pavel and Anderson, Bo and Devanur, Nikhil R.},
	year = {2011},
	pages = {1307},
	file = {Chen et al. - 2011 - Real-time bidding algorithms for performance-based.pdf:/Users/yisu/Zotero/storage/WK3JX5AR/Chen et al. - 2011 - Real-time bidding algorithms for performance-based.pdf:application/pdf},
}

@misc{hazan_introduction_2021,
	title = {Introduction to {Online} {Convex} {Optimization}},
	url = {http://arxiv.org/abs/1909.05207},
	doi = {10.48550/arXiv.1909.05207},
	abstract = {This manuscript portrays optimization as a process. In many practical applications the environment is so complex that it is infeasible to lay out a comprehensive theoretical model and use classical algorithmic theory and mathematical optimization. It is necessary as well as beneficial to take a robust approach, by applying an optimization method that learns as one goes along, learning from experience as more aspects of the problem are observed. This view of optimization as a process has become prominent in varied fields and has led to some spectacular success in modeling and systems that are now part of our daily lives.},
	urldate = {2022-10-10},
	publisher = {arXiv},
	author = {Hazan, Elad},
	month = dec,
	year = {2021},
	note = {arXiv:1909.05207 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/Y9T9MLT2/Hazan - 2021 - Introduction to Online Convex Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/XAWYJPA4/1909.html:text/html},
}

@inproceedings{yi_distributed_2020,
	title = {A {Distributed} {Primal}-{Dual} {Algorithm} for {Bandit} {Online} {Convex} {Optimization} with {Time}-{Varying} {Coupled} {Inequality} {Constraints}},
	doi = {10.23919/ACC45564.2020.9147354},
	abstract = {This paper considers distributed bandit online optimization with time-varying coupled inequality constraints. The global cost and the coupled constraint functions are the summations of local convex cost and constraint functions, respectively. The local cost and constraint functions are held privately and only at the end of each period the constraint functions are fully revealed, while only the values of cost functions at queried points are revealed, i.e., in a so called bandit manner. A distributed bandit online primal-dual algorithm with two queries for the cost functions per period is proposed. The performance of the algorithm is evaluated using its expected regret, i.e., the expected difference between the outcome of the algorithm and the optimal choice in hindsight, as well as its constraint violation. We show that O(Tc) expected regret and O(T1-c/2) constraint violation are achieved by the proposed algorithm, where T is the total number of iterations and c ∈ [0.5, 1) is a user-defined trade-off parameter. Assuming √ Slater's condition, we show that O(√T) expected regret and O(√T) constraint violation are achieved. The theoretical results are illustrated by numerical simulations.},
	booktitle = {2020 {American} {Control} {Conference} ({ACC})},
	author = {Yi, Xinlei and Li, Xiuxian and Yang, Tao and Xie, Lihua and Chai, Tianyou and Johansson, Karl H.},
	month = jul,
	year = {2020},
	note = {ISSN: 2378-5861},
	keywords = {Convex functions, Cost function, Graph theory, Numerical models, Numerical simulation, Routing},
	pages = {327--332},
	file = {IEEE Xplore Abstract Record:/Users/yisu/Zotero/storage/EXKEIKAV/9147354.html:text/html;IEEE Xplore Full Text PDF:/Users/yisu/Zotero/storage/CVLN5BJX/Yi et al. - 2020 - A Distributed Primal-Dual Algorithm for Bandit Onl.pdf:application/pdf},
}

@article{may_optimistic_nodate,
	title = {Optimistic {Bayesian} {Sampling} in {Contextual}-{Bandit} {Problems}},
	abstract = {In sequential decision problems in an unknown environment, the decision maker often faces a dilemma over whether to explore to discover more about the environment, or to exploit current knowledge. We address the exploration-exploitation dilemma in a general setting encompassing both standard and contextualised bandit problems. The contextual bandit problem has recently resurfaced in attempts to maximise click-through rates in web based applications, a task with signiﬁcant commercial interest.},
	language = {en},
	author = {May, Benedict C and May, Ben and Uk, Bris Ac},
	pages = {38},
	file = {May et al. - Optimistic Bayesian Sampling in Contextual-Bandit .pdf:/Users/yisu/Zotero/storage/9HZKWH9G/May et al. - Optimistic Bayesian Sampling in Contextual-Bandit .pdf:application/pdf},
}

@book{lattimore_bandit_2020,
	edition = {1},
	title = {Bandit {Algorithms}},
	isbn = {978-1-108-57140-1 978-1-108-48682-8},
	url = {https://www.cambridge.org/core/product/identifier/9781108571401/type/book},
	language = {en},
	urldate = {2022-10-18},
	publisher = {Cambridge University Press},
	author = {Lattimore, Tor and Szepesvári, Csaba},
	month = jul,
	year = {2020},
	doi = {10.1017/9781108571401},
	file = {Lattimore and Szepesvári - 2020 - Bandit Algorithms.pdf:/Users/yisu/Zotero/storage/XL56CEH6/Lattimore and Szepesvári - 2020 - Bandit Algorithms.pdf:application/pdf},
}

@misc{faury_jointly_2022,
	title = {Jointly {Efficient} and {Optimal} {Algorithms} for {Logistic} {Bandits}},
	url = {http://arxiv.org/abs/2201.01985},
	abstract = {Logistic Bandits have recently undergone careful scrutiny by virtue of their combined theoretical and practical relevance. This research effort delivered statistically efficient algorithms, improving the regret of previous strategies by exponentially large factors. Such algorithms are however strikingly costly as they require \${\textbackslash}Omega(t)\$ operations at each round. On the other hand, a different line of research focused on computational efficiency (\${\textbackslash}mathcal\{O\}(1)\$ per-round cost), but at the cost of letting go of the aforementioned exponential improvements. Obtaining the best of both world is unfortunately not a matter of marrying both approaches. Instead we introduce a new learning procedure for Logistic Bandits. It yields confidence sets which sufficient statistics can be easily maintained online without sacrificing statistical tightness. Combined with efficient planning mechanisms we design fast algorithms which regret performance still match the problem-dependent lower-bound of Abeille et al. (2021). To the best of our knowledge, those are the first Logistic Bandit algorithms that simultaneously enjoy statistical and computational efficiency.},
	urldate = {2022-10-18},
	publisher = {arXiv},
	author = {Faury, Louis and Abeille, Marc and Jun, Kwang-Sung and Calauzènes, Clément},
	month = jan,
	year = {2022},
	note = {arXiv:2201.01985 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/SCKN2JAV/Faury et al. - 2022 - Jointly Efficient and Optimal Algorithms for Logis.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/YAUZBZZC/2201.html:text/html},
}

@misc{ding_efficient_2021,
	title = {An {Efficient} {Algorithm} {For} {Generalized} {Linear} {Bandit}: {Online} {Stochastic} {Gradient} {Descent} and {Thompson} {Sampling}},
	shorttitle = {An {Efficient} {Algorithm} {For} {Generalized} {Linear} {Bandit}},
	url = {http://arxiv.org/abs/2006.04012},
	abstract = {We consider the contextual bandit problem, where a player sequentially makes decisions based on past observations to maximize the cumulative reward. Although many algorithms have been proposed for contextual bandit, most of them rely on finding the maximum likelihood estimator at each iteration, which requires \$O(t)\$ time at the \$t\$-th iteration and are memory inefficient. A natural way to resolve this problem is to apply online stochastic gradient descent (SGD) so that the per-step time and memory complexity can be reduced to constant with respect to \$t\$, but a contextual bandit policy based on online SGD updates that balances exploration and exploitation has remained elusive. In this work, we show that online SGD can be applied to the generalized linear bandit problem. The proposed SGD-TS algorithm, which uses a single-step SGD update to exploit past information and uses Thompson Sampling for exploration, achieves \${\textbackslash}tilde\{O\}({\textbackslash}sqrt\{T\})\$ regret with the total time complexity that scales linearly in \$T\$ and \$d\$, where \$T\$ is the total number of rounds and \$d\$ is the number of features. Experimental results show that SGD-TS consistently outperforms existing algorithms on both synthetic and real datasets.},
	urldate = {2022-10-18},
	publisher = {arXiv},
	author = {Ding, Qin and Hsieh, Cho-Jui and Sharpnack, James},
	month = jun,
	year = {2021},
	note = {arXiv:2006.04012 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/QCDZEX7H/Ding et al. - 2021 - An Efficient Algorithm For Generalized Linear Band.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/4YJQ6PTA/2006.html:text/html},
}

@inproceedings{abbasi-yadkori_improved_2011,
	title = {Improved {Algorithms} for {Linear} {Stochastic} {Bandits}},
	volume = {24},
	url = {https://papers.nips.cc/paper/2011/hash/e1d5be1c7f2f456670de3d53c7b54f4a-Abstract.html},
	abstract = {We improve the theoretical analysis and empirical performance of algorithms for the stochastic multi-armed bandit problem and the linear stochastic multi-armed bandit problem. In particular, we show that a simple modification of Auer’s UCB algorithm (Auer, 2002) achieves with high probability constant regret. More importantly, we modify and, consequently, improve the analysis of the algorithm for the for linear stochastic bandit problem studied by Auer (2002), Dani et al. (2008), Rusmevichientong and Tsitsiklis (2010), Li et al. (2010). Our modification improves the regret bound by a logarithmic factor, though experiments show a vast improvement. In both cases, the improvement stems from the construction of smaller confidence sets. For their construction we use a novel tail inequality for vector-valued martingales.},
	urldate = {2022-10-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Abbasi-yadkori, Yasin and Pál, Dávid and Szepesvári, Csaba},
	year = {2011},
	file = {Full Text PDF:/Users/yisu/Zotero/storage/V4BZ4GWS/Abbasi-yadkori et al. - 2011 - Improved Algorithms for Linear Stochastic Bandits.pdf:application/pdf},
}

@article{auer_finite-time_nodate-1,
	title = {Finite-time {Analysis} of the {Multiarmed} {Bandit} {Problem}},
	abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to ﬁnd proﬁtable actions while taking the empirically best action as often as possible. A popular measure of a policy’s success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the ﬁrst ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efﬁcient policies, and for all reward distributions with bounded support.},
	language = {en},
	author = {Auer, Peter and Cesa-Bianchi, Nicolo},
	pages = {22},
	file = {Auer and Cesa-Bianchi - Finite-time Analysis of the Multiarmed Bandit Prob.pdf:/Users/yisu/Zotero/storage/Q7D5Y8L2/Auer and Cesa-Bianchi - Finite-time Analysis of the Multiarmed Bandit Prob.pdf:application/pdf},
}

@article{abbasi-yadkori_forced-exploration_nodate,
	title = {Forced-{Exploration} {Based} {Algorithms} for {Playing} in {Stochastic} {Linear} {Bandits}},
	abstract = {We study stochastic linear payoﬀ bandit problems and give a simple, computationally efﬁcient algorithm whose regret, under certain regu√larity assumptions on the action set, is O(d T ), where d is the dimensionality of the action space and T is the time-horizon. However, this result is problem dependent and not a minimax bound. We show that our algorithm is able to achieve lower regret bounds when we have sparsity in the problem. Our experimental results support our upper bound and show that the algorithm proposed might be competitive with alternative algorithms when the payoﬀs of the actions are correlated and when the parameters vector is sparse.},
	language = {en},
	author = {Abbasi-Yadkori, Yasin and Antos, Andras and Szepesvari, Csaba},
	pages = {6},
	file = {Abbasi-Yadkori et al. - Forced-Exploration Based Algorithms for Playing in.pdf:/Users/yisu/Zotero/storage/ZQXBVTHK/Abbasi-Yadkori et al. - Forced-Exploration Based Algorithms for Playing in.pdf:application/pdf},
}

@misc{li_fairness_2022,
	title = {Fairness in {Recommendation}: {A} {Survey}},
	shorttitle = {Fairness in {Recommendation}},
	url = {http://arxiv.org/abs/2205.13619},
	abstract = {As one of the most pervasive applications of machine learning, recommender systems are playing an important role on assisting human decision making. The satisfaction of users and the interests of platforms are closely related to the quality of the generated recommendation results. However, as a highly data-driven system, recommender system could be affected by data or algorithmic bias and thus generate unfair results, which could weaken the reliance of the systems. As a result, it is crucial to address the potential unfairness problems in recommendation settings. Recently, there has been growing attention on fairness considerations in recommender systems with more and more literature on approaches to promote fairness in recommendation. However, the studies are rather fragmented and lack a systematic organization, thus making it difficult to penetrate for new researchers to the domain. This motivates us to provide a systematic survey of existing works on fairness in recommendation. This survey focuses on the foundations for fairness in recommendation literature. It first presents a brief introduction about fairness in basic machine learning tasks such as classification and ranking in order to provide a general overview of fairness research, as well as introduce the more complex situations and challenges that need to be considered when studying fairness in recommender systems. After that, the survey will introduce fairness in recommendation with a focus on the taxonomies of current fairness definitions, the typical techniques for improving fairness, as well as the datasets for fairness studies in recommendation. The survey also talks about the challenges and opportunities in fairness research with the hope of promoting the fair recommendation research area and beyond.},
	urldate = {2022-10-21},
	publisher = {arXiv},
	author = {Li, Yunqi and Chen, Hanxiong and Xu, Shuyuan and Ge, Yingqiang and Tan, Juntao and Liu, Shuchang and Zhang, Yongfeng},
	month = jun,
	year = {2022},
	note = {arXiv:2205.13619 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/RF35NXW6/Li et al. - 2022 - Fairness in Recommendation A Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/XAL54GUV/2205.html:text/html},
}

@misc{mou_sustainable_2022,
	title = {Sustainable {Online} {Reinforcement} {Learning} for {Auto}-bidding},
	url = {http://arxiv.org/abs/2210.07006},
	abstract = {Recently, auto-bidding technique has become an essential tool to increase the revenue of advertisers. Facing the complex and ever-changing bidding environments in the real-world advertising system (RAS), state-of-the-art auto-bidding policies usually leverage reinforcement learning (RL) algorithms to generate real-time bids on behalf of the advertisers. Due to safety concerns, it was believed that the RL training process can only be carried out in an offline virtual advertising system (VAS) that is built based on the historical data generated in the RAS. In this paper, we argue that there exists significant gaps between the VAS and RAS, making the RL training process suffer from the problem of inconsistency between online and offline (IBOO). Firstly, we formally define the IBOO and systematically analyze its causes and influences. Then, to avoid the IBOO, we propose a sustainable online RL (SORL) framework that trains the auto-bidding policy by directly interacting with the RAS, instead of learning in the VAS. Specifically, based on our proof of the Lipschitz smooth property of the Q function, we design a safe and efficient online exploration (SER) policy for continuously collecting data from the RAS. Meanwhile, we derive the theoretical lower bound on the safety of the SER policy. We also develop a variance-suppressed conservative Q-learning (V-CQL) method to effectively and stably learn the auto-bidding policy with the collected data. Finally, extensive simulated and real-world experiments validate the superiority of our approach over the state-of-the-art auto-bidding algorithm.},
	urldate = {2022-10-24},
	publisher = {arXiv},
	author = {Mou, Zhiyu and Huo, Yusen and Bai, Rongquan and Xie, Mingzhou and Yu, Chuan and Xu, Jian and Zheng, Bo},
	month = oct,
	year = {2022},
	note = {arXiv:2210.07006 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/43DCTI44/Mou et al. - 2022 - Sustainable Online Reinforcement Learning for Auto.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/WAYTI5AJ/2210.html:text/html},
}

@book{casella_statistical_2002,
	address = {Australia ; Pacific Grove, CA},
	edition = {2nd ed},
	title = {Statistical inference},
	isbn = {978-0-534-24312-8},
	language = {en},
	publisher = {Thomson Learning},
	author = {Casella, George and Berger, Roger L.},
	year = {2002},
	keywords = {Probabilities, Mathematical statistics},
	file = {Casella and Berger - 2002 - Statistical inference.pdf:/Users/yisu/Zotero/storage/FQ97ZTYS/Casella and Berger - 2002 - Statistical inference.pdf:application/pdf},
}

@book{boyd_convex_2004,
	address = {Cambridge, UK ; New York},
	title = {Convex optimization},
	isbn = {978-0-521-83378-3},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Boyd, Stephen P. and Vandenberghe, Lieven},
	year = {2004},
	keywords = {Convex functions, Mathematical optimization},
	file = {Boyd and Vandenberghe - 2004 - Convex optimization.pdf:/Users/yisu/Zotero/storage/9ZFD9GEW/Boyd and Vandenberghe - 2004 - Convex optimization.pdf:application/pdf},
}

@article{stone_information_nodate,
	title = {Information {Theory}: {A} {Tutorial} {Introduction}},
	abstract = {Shannon’s mathematical theory of communication deﬁnes fundamental limits on how much information can be transmitted between the diﬀerent components of any man-made or biological system. This paper is an informal but rigorous introduction to the main ideas implicit in Shannon’s theory. An annotated reading list is provided for further reading.},
	language = {en},
	author = {Stone, J V},
	pages = {24},
	file = {Stone - Information Theory A Tutorial Introduction.pdf:/Users/yisu/Zotero/storage/PG86UHCI/Stone - Information Theory A Tutorial Introduction.pdf:application/pdf},
}

@article{mackay_information_nodate,
	title = {Information {Theory}, {Inference}, and {Learning} {Algorithms}},
	language = {en},
	author = {MacKay, David J C},
	pages = {640},
	file = {MacKay - Information Theory, Inference, and Learning Algori.pdf:/Users/yisu/Zotero/storage/SMEDP9GE/MacKay - Information Theory, Inference, and Learning Algori.pdf:application/pdf},
}

@article{fawzi_discovering_2022,
	title = {Discovering faster matrix multiplication algorithms with reinforcement learning},
	volume = {610},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-022-05172-4},
	doi = {10.1038/s41586-022-05172-4},
	abstract = {Abstract
            
              Improving the efficiency of algorithms for fundamental computations can have a widespread impact, as it can affect the overall speed of a large amount of computations. Matrix multiplication is one such primitive task, occurring in many systems—from neural networks to scientific computing routines. The automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human-designed algorithms. However, automating the algorithm discovery procedure is intricate, as the space of possible algorithms is enormous. Here we report a deep reinforcement learning approach based on AlphaZero
              1
              for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a single-player game where the objective is finding tensor decompositions within a finite factor space. AlphaTensor discovered algorithms that outperform the state-of-the-art complexity for many matrix sizes. Particularly relevant is the case of 4 × 4 matrices in a finite field, where AlphaTensor’s algorithm improves on Strassen’s two-level algorithm for the first time, to our knowledge, since its discovery 50 years ago
              2
              . We further showcase the flexibility of AlphaTensor through different use-cases: algorithms with state-of-the-art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware. Our results highlight AlphaTensor’s ability to accelerate the process of algorithmic discovery on a range of problems, and to optimize for different criteria.},
	language = {en},
	number = {7930},
	urldate = {2022-10-28},
	journal = {Nature},
	author = {Fawzi, Alhussein and Balog, Matej and Huang, Aja and Hubert, Thomas and Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and R. Ruiz, Francisco J. and Schrittwieser, Julian and Swirszcz, Grzegorz and Silver, David and Hassabis, Demis and Kohli, Pushmeet},
	month = oct,
	year = {2022},
	pages = {47--53},
	file = {Fawzi et al. - 2022 - Discovering faster matrix multiplication algorithm.pdf:/Users/yisu/Zotero/storage/2ENSSEDI/Fawzi et al. - 2022 - Discovering faster matrix multiplication algorithm.pdf:application/pdf},
}

@misc{bottou_optimization_2018,
	title = {Optimization {Methods} for {Large}-{Scale} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1606.04838},
	abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
	urldate = {2022-10-28},
	publisher = {arXiv},
	author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
	month = feb,
	year = {2018},
	note = {arXiv:1606.04838 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/Z5ZPLBHH/Bottou et al. - 2018 - Optimization Methods for Large-Scale Machine Learn.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/4QEUE45Z/1606.html:text/html},
}

@book{wasserman_all_2004,
	address = {New York},
	series = {Springer texts in statistics},
	title = {All of statistics: a concise course in statistical inference},
	isbn = {978-0-387-40272-7},
	shorttitle = {All of statistics},
	publisher = {Springer},
	author = {Wasserman, Larry},
	year = {2004},
	keywords = {Mathematical statistics},
	file = {all-of-statistics.pdf:/Users/yisu/Zotero/storage/LKAJWRTV/all-of-statistics.pdf:application/pdf},
}

@book{goodfellow_deep_2016,
	address = {Cambridge, Massachusetts},
	series = {Adaptive computation and machine learning},
	title = {Deep learning},
	isbn = {978-0-262-03561-3},
	publisher = {The MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	keywords = {Machine learning},
	file = {Ian Goodfellow, Yoshua Bengio, Aaron Courville - Deep Learning (2017, MIT).pdf:/Users/yisu/Zotero/storage/KXNZKJHX/Ian Goodfellow, Yoshua Bengio, Aaron Courville - Deep Learning (2017, MIT).pdf:application/pdf},
}

@book{kochenderfer_algorithms_2022,
	address = {Cambridge, Massachusetts},
	title = {Algorithms for decision making},
	isbn = {978-0-262-04701-2},
	abstract = {"This text introduces decision making under uncertainty from a computational perspective and provides an overview of the methods for building autonomous and decision-support systems"--},
	language = {en},
	publisher = {The MIT Press},
	author = {Kochenderfer, Mykel J. and Wheeler, Tim A. and Wray, Kyle H.},
	year = {2022},
	keywords = {Algorithms, Decision support systems, Mathematics},
	file = {Kochenderfer et al. - 2022 - Algorithms for decision making.pdf:/Users/yisu/Zotero/storage/8XQTV2EX/Kochenderfer et al. - 2022 - Algorithms for decision making.pdf:application/pdf},
}

@inproceedings{davis_relationship_2006,
	address = {Pittsburgh, Pennsylvania},
	title = {The relationship between {Precision}-{Recall} and {ROC} curves},
	isbn = {978-1-59593-383-6},
	url = {http://portal.acm.org/citation.cfm?doid=1143844.1143874},
	doi = {10.1145/1143844.1143874},
	abstract = {Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm’s performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an eﬃcient algorithm for computing this curve. Finally, we also note diﬀerences in the two types of curves are signiﬁcant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.},
	language = {en},
	urldate = {2022-11-01},
	booktitle = {Proceedings of the 23rd international conference on {Machine} learning  - {ICML} '06},
	publisher = {ACM Press},
	author = {Davis, Jesse and Goadrich, Mark},
	year = {2006},
	pages = {233--240},
	file = {Davis and Goadrich - 2006 - The relationship between Precision-Recall and ROC .pdf:/Users/yisu/Zotero/storage/GVMW5Y2G/Davis and Goadrich - 2006 - The relationship between Precision-Recall and ROC .pdf:application/pdf},
}

@misc{ruder_overview_2017,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	urldate = {2022-11-03},
	publisher = {arXiv},
	author = {Ruder, Sebastian},
	month = jun,
	year = {2017},
	note = {arXiv:1609.04747 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/GLRPKWNG/Ruder - 2017 - An overview of gradient descent optimization algor.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/EFKMPQ92/1609.html:text/html},
}

@inproceedings{perez_maurera_towards_2022,
	address = {Seattle WA USA},
	title = {Towards the {Evaluation} of {Recommender} {Systems} with {Impressions}},
	isbn = {978-1-4503-9278-5},
	url = {https://dl.acm.org/doi/10.1145/3523227.3551483},
	doi = {10.1145/3523227.3551483},
	abstract = {In Recommender Systems, impressions are a relatively new type of information that records all products previously shown to the users. They are also a complex source of information, combining the effects of the recommender system that generated them, search results, or business rules that may select specific products for recommendations. The fact that the user interacted with a specific item given a list of recommended ones may benefit from a richer interaction signal, in which some items the user did not interact with may be considered negative interactions. This work presents a preliminary evaluation of recommendation models with impressions. First, impressions are characterized by describing their assumptions, signals, and challenges. Then, an evaluation study with impressions is described. The study’s goal is two-fold: to measure the effects of impressions data on properly-tuned recommendation models using current open-source datasets and disentangle the signals within impressions data. Preliminary results suggest that impressions data and signals are nuanced, complex, and effective at improving the recommendation quality of recommenders. This work publishes the source code, datasets, and scripts used in the evaluation to promote reproducibility in the domain.},
	language = {en},
	urldate = {2022-11-03},
	booktitle = {Sixteenth {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Perez Maurera, Fernando Benjamin and Ferrari Dacrema, Maurizio and Cremonesi, Paolo},
	month = sep,
	year = {2022},
	pages = {610--615},
	file = {Perez Maurera et al. - 2022 - Towards the Evaluation of Recommender Systems with.pdf:/Users/yisu/Zotero/storage/ALH5ZX79/Perez Maurera et al. - 2022 - Towards the Evaluation of Recommender Systems with.pdf:application/pdf},
}

@article{chen_revisiting_2022,
	title = {Revisiting {Negative} {Sampling} {VS}. {Non}-{Sampling} in {Implicit} {Recommendation}},
	issn = {1046-8188, 1558-2868},
	url = {https://dl.acm.org/doi/10.1145/3522672},
	doi = {10.1145/3522672},
	abstract = {Recommendation systems play a vital role in alleviating information overload. Generally, a recommendation model is trained to discern between positive (liked) and negative (disliked) instances for each user. However, under the open-world assumption, there are only positive instances but no negative instances from users’ implicit feedback, which poses the imbalanced learning challenge of lacking negative samples. To address this, two types of training strategies have been proposed in previous studies, which are 1) negative sampling strategy and 2) non-sampling strategy. The irst strategy samples negative instances from missing data (i.e., unlabeled data), while the second one regards all the missing data as negative. Although training strategies are known to be essential for algorithm performance, the in-depth comparison of negative sampling and non-sampling is still left insuiciently explored by far. To bridge this gap, in this paper we systematically analyze the role of negative sampling and non-sampling for implicit recommendation. Speciically, we irst theoretically revisit the objection of negative sampling and non-sampling. Then, with a careful setup of various representative recommendation methods, we explore the performance of negative sampling and non-sampling in diferent scenarios. Our results empirically show that although negative sampling has been widely applied to recent recommendation models, it is non-trivial for uniform sampling methods to show comparable performance to non-sampling learning methods. Finally, we discuss the scalability and complexity of negative sampling and non-sampling, and present some open problems and future research topics that worth to be further explored. CCS Concepts: • Information systems → Recommender systems.},
	language = {en},
	urldate = {2022-11-10},
	journal = {ACM Transactions on Information Systems},
	author = {Chen, Chong and Ma, Weizhi and Zhang, Min and Wang, Chenyang and Liu, Yiqun and Ma, Shaoping},
	month = mar,
	year = {2022},
	pages = {3522672},
	file = {Chen et al. - 2022 - Revisiting Negative Sampling VS. Non-Sampling in I.pdf:/Users/yisu/Zotero/storage/YE88SYUM/Chen et al. - 2022 - Revisiting Negative Sampling VS. Non-Sampling in I.pdf:application/pdf},
}

@misc{chen_sampling_2017,
	title = {On {Sampling} {Strategies} for {Neural} {Network}-based {Collaborative} {Filtering}},
	url = {http://arxiv.org/abs/1706.07881},
	abstract = {Recent advances in neural networks have inspired people to design hybrid recommendation algorithms that can incorporate both (1) user-item interaction information and (2) content information including image, audio, and text. Despite their promising results, neural network-based recommendation algorithms pose extensive computational costs, making it challenging to scale and improve upon. In this paper, we propose a general neural network-based recommendation framework, which subsumes several existing state-of-the-art recommendation algorithms, and address the efficiency issue by investigating sampling strategies in the stochastic gradient descent training for the framework. We tackle this issue by first establishing a connection between the loss functions and the user-item interaction bipartite graph, where the loss function terms are defined on links while major computation burdens are located at nodes. We call this type of loss functions "graph-based" loss functions, for which varied mini-batch sampling strategies can have different computational costs. Based on the insight, three novel sampling strategies are proposed, which can significantly improve the training efficiency of the proposed framework (up to \${\textbackslash}times 30\$ times speedup in our experiments), as well as improving the recommendation performance. Theoretical analysis is also provided for both the computational cost and the convergence. We believe the study of sampling strategies have further implications on general graph-based loss functions, and would also enable more research under the neural network-based recommendation framework.},
	urldate = {2022-11-09},
	publisher = {arXiv},
	author = {Chen, Ting and Sun, Yizhou and Shi, Yue and Hong, Liangjie},
	month = jun,
	year = {2017},
	note = {arXiv:1706.07881 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval, Computer Science - Social and Information Networks},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/LX8RLVF6/Chen et al. - 2017 - On Sampling Strategies for Neural Network-based Co.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/QK3G56BE/1706.html:text/html},
}

@misc{gu_real_2021,
	title = {Real {Negatives} {Matter}: {Continuous} {Training} with {Real} {Negatives} for {Delayed} {Feedback} {Modeling}},
	shorttitle = {Real {Negatives} {Matter}},
	url = {http://arxiv.org/abs/2104.14121},
	abstract = {One of the difficulties of conversion rate (CVR) prediction is that the conversions can delay and take place long after the clicks. The delayed feedback poses a challenge: fresh data are beneficial to continuous training but may not have complete label information at the time they are ingested into the training pipeline. To balance model freshness and label certainty, previous methods set a short waiting window or even do not wait for the conversion signal. If conversion happens outside the waiting window, this sample will be duplicated and ingested into the training pipeline with a positive label. However, these methods have some issues. First, they assume the observed feature distribution remains the same as the actual distribution. But this assumption does not hold due to the ingestion of duplicated samples. Second, the certainty of the conversion action only comes from the positives. But the positives are scarce as conversions are sparse in commercial systems. These issues induce bias during the modeling of delayed feedback. In this paper, we propose DElayed FEedback modeling with Real negatives (DEFER) method to address these issues. The proposed method ingests real negative samples into the training pipeline. The ingestion of real negatives ensures the observed feature distribution is equivalent to the actual distribution, thus reducing the bias. The ingestion of real negatives also brings more certainty information of the conversion. To correct the distribution shift, DEFER employs importance sampling to weigh the loss function. Experimental results on industrial datasets validate the superiority of DEFER. DEFER have been deployed in the display advertising system of Alibaba, obtaining over 6.0\% improvement on CVR in several scenarios. The code and data in this paper are now open-sourced \{https://github.com/gusuperstar/defer.git\}.},
	urldate = {2022-11-08},
	publisher = {arXiv},
	author = {Gu, Siyu and Sheng, Xiang-Rong and Fan, Ying and Zhou, Guorui and Zhu, Xiaoqiang},
	month = aug,
	year = {2021},
	note = {arXiv:2104.14121 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/KQJV9NKE/Gu et al. - 2021 - Real Negatives Matter Continuous Training with Re.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/MZTH8TX9/2104.html:text/html},
}

@inproceedings{chapelle_modeling_2014,
	address = {New York New York USA},
	title = {Modeling delayed feedback in display advertising},
	isbn = {978-1-4503-2956-9},
	url = {https://dl.acm.org/doi/10.1145/2623330.2623634},
	doi = {10.1145/2623330.2623634},
	abstract = {In performance display advertising a key metric of a campaign eﬀectiveness is its conversion rate – the proportion of users who take a predeﬁned action on the advertiser website, such as a purchase. Predicting this conversion rate is thus essential for estimating the value of an impression and can be achieved via machine learning. One diﬃculty however is that the conversions can take place long after the impression – up to a month – and this delayed feedback hinders the conversion modeling. We tackle this issue by introducing an additional model that captures the conversion delay. Intuitively, this probabilistic model helps determining whether a user that has not converted should be treated as a negative sample – when the elapsed time is larger than the predicted delay – or should be discarded from the training set – when it is too early to tell. We provide experimental results on real traﬃc logs that demonstrate the eﬀectiveness of the proposed model.},
	language = {en},
	urldate = {2022-11-08},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {ACM},
	author = {Chapelle, Olivier},
	month = aug,
	year = {2014},
	pages = {1097--1105},
	file = {Chapelle - 2014 - Modeling delayed feedback in display advertising.pdf:/Users/yisu/Zotero/storage/5L5TL6HM/Chapelle - 2014 - Modeling delayed feedback in display advertising.pdf:application/pdf},
}

@inproceedings{vernade_linear_2020,
	title = {Linear bandits with {Stochastic} {Delayed} {Feedback}},
	url = {https://proceedings.mlr.press/v119/vernade20a.html},
	abstract = {Stochastic linear bandits are a natural and well-studied model for structured exploration/exploitation problems and are widely used in applications such as on-line marketing and recommendation. One of the main challenges faced by practitioners hoping to apply existing algorithms is that usually the feedback is randomly delayed and delays are only partially observable. For example, while a purchase is usually observable some time after the display, the decision of not buying is never explicitly sent to the system. In other words, the learner only observes delayed positive events. We formalize this problem as a novel stochastic delayed linear bandit and propose OTFLinUCB and OTFLinTS, two computationally efficient algorithms able to integrate new information as it becomes available and to deal with the permanently censored feedback. We prove optimal O(d{\textbackslash}sqrt\{T\}) bounds on the regret of the first algorithm and study the dependency on delay-dependent parameters. Our model, assumptions and results are validated by experiments on simulated and real data.},
	language = {en},
	urldate = {2022-11-08},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Vernade, Claire and Carpentier, Alexandra and Lattimore, Tor and Zappella, Giovanni and Ermis, Beyza and Brückner, Michael},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {9712--9721},
	file = {Full Text PDF:/Users/yisu/Zotero/storage/5QPY4F7J/Vernade et al. - 2020 - Linear bandits with Stochastic Delayed Feedback.pdf:application/pdf;Supplementary PDF:/Users/yisu/Zotero/storage/PV8TQ67C/Vernade et al. - 2020 - Linear bandits with Stochastic Delayed Feedback.pdf:application/pdf},
}

@inproceedings{filippi_parametric_2010,
	title = {Parametric {Bandits}: {The} {Generalized} {Linear} {Case}},
	volume = {23},
	shorttitle = {Parametric {Bandits}},
	url = {https://papers.nips.cc/paper/2010/hash/c2626d850c80ea07e7511bbae4c76f4b-Abstract.html},
	abstract = {We consider structured multi-armed bandit tasks in which the agent is guided by prior structural knowledge that can be exploited to efficiently select the optimal arm(s) in situations where the number of arms is large, or even infinite. We pro- pose a new optimistic, UCB-like, algorithm for non-linearly parameterized bandit problems using the Generalized Linear Model (GLM) framework. We analyze the regret of the proposed algorithm, termed GLM-UCB, obtaining results similar to those recently proved in the literature for the linear regression case. The analysis also highlights a key difficulty of the non-linear case which is solved in GLM-UCB by focusing on the reward space rather than on the parameter space. Moreover, as the actual efficiency of current parameterized bandit algorithms is often deceiving in practice, we provide an asymptotic argument leading to significantly faster convergence. Simulation studies on real data sets illustrate the performance and the robustness of the proposed GLM-UCB approach.},
	urldate = {2022-11-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Filippi, Sarah and Cappe, Olivier and Garivier, Aurélien and Szepesvári, Csaba},
	year = {2010},
	file = {Full Text PDF:/Users/yisu/Zotero/storage/G2TKDC9W/Filippi et al. - 2010 - Parametric Bandits The Generalized Linear Case.pdf:application/pdf},
}

@article{yang_contextual_2021,
	title = {Contextual {Bandits} with {Delayed} {Feedback} and {Semi}-supervised {Learning} ({Student} {Abstract})},
	volume = {35},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/17968},
	doi = {10.1609/aaai.v35i18.17968},
	abstract = {Contextual multi-armed bandit (MAB) is a classic online learning problem, where a learner/agent selects actions (i.e., arms) given contextual information and discovers optimal actions based on reward feedback. Applications of contextual bandit have been increasingly expanding, including advertisement, personalization, resource allocation in wireless networks, among others. Nonetheless, the reward feedback is delayed in many applications (e.g., a user may only provide service ratings after a period of time), creating challenges for contextual bandits. In this paper, we address delayed feedback in contextual bandits by using semi-supervised learning — incorporate estimates of delayed rewards to improve the estimation of future rewards. Concretely, the reward feedback for an arm selected at the beginning of a round is only observed by the agent/learner with some observation noise and provided to the agent after some a priori unknown but bounded delays. Motivated by semi-supervised learning that produces pseudo labels for unlabeled data to further improve the model performance, we generate ﬁctitious estimates of rewards that are delayed and have yet to arrive based on already-learnt reward functions. Thus, by combining semisupervised learning with online contextual bandit learning, we propose a novel extension and design two algorithms, which estimate the values for currently unavailable reward feedbacks to minimize the maximum estimation error and average estimation error, respectively.},
	language = {en},
	number = {18},
	urldate = {2022-11-08},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Yang, Luting and Yang, Jianyi and Ren, Shaolei},
	month = may,
	year = {2021},
	pages = {15943--15944},
	file = {Yang et al. - 2021 - Contextual Bandits with Delayed Feedback and Semi-.pdf:/Users/yisu/Zotero/storage/9BRT5QU6/Yang et al. - 2021 - Contextual Bandits with Delayed Feedback and Semi-.pdf:application/pdf},
}

@misc{zhao_dear_2021,
	title = {{DEAR}: {Deep} {Reinforcement} {Learning} for {Online} {Advertising} {Impression} in {Recommender} {Systems}},
	shorttitle = {{DEAR}},
	url = {http://arxiv.org/abs/1909.03602},
	abstract = {With the recent prevalence of Reinforcement Learning (RL), there have been tremendous interests in utilizing RL for online advertising in recommendation platforms (e.g., e-commerce and news feed sites). However, most RL-based advertising algorithms focus on optimizing ads' revenue while ignoring the possible negative influence of ads on user experience of recommended items (products, articles and videos). Developing an optimal advertising algorithm in recommendations faces immense challenges because interpolating ads improperly or too frequently may decrease user experience, while interpolating fewer ads will reduce the advertising revenue. Thus, in this paper, we propose a novel advertising strategy for the rec/ads trade-off. To be specific, we develop an RL-based framework that can continuously update its advertising strategies and maximize reward in the long run. Given a recommendation list, we design a novel Deep Q-network architecture that can determine three internally related tasks jointly, i.e., (i) whether to interpolate an ad or not in the recommendation list, and if yes, (ii) the optimal ad and (iii) the optimal location to interpolate. The experimental results based on real-world data demonstrate the effectiveness of the proposed framework.},
	urldate = {2022-11-17},
	publisher = {arXiv},
	author = {Zhao, Xiangyu and Gu, Changsheng and Zhang, Haoshenglun and Yang, Xiwang and Liu, Xiaobing and Tang, Jiliang and Liu, Hui},
	month = may,
	year = {2021},
	note = {arXiv:1909.03602 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/ITQGUCP5/Zhao et al. - 2021 - DEAR Deep Reinforcement Learning for Online Adver.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/23NI8INE/1909.html:text/html},
}

@inproceedings{dewet_finding_2019,
	address = {Anchorage AK USA},
	title = {Finding {Users} {Who} {Act} {Alike}: {Transfer} {Learning} for {Expanding} {Advertiser} {Audiences}},
	isbn = {978-1-4503-6201-6},
	shorttitle = {Finding {Users} {Who} {Act} {Alike}},
	url = {https://dl.acm.org/doi/10.1145/3292500.3330714},
	doi = {10.1145/3292500.3330714},
	abstract = {Audience Look-alike Targeting is an online advertising technique in which an advertiser specifies a set of seed customers and tasks the advertising platform with finding an expanded audience of similar users. We will describe a two-stage embedding-based audience expansion model that is deployed in production at Pinterest. For the first stage we trained a global user embedding model on sitewide user activity logs. In the second stage, we use transfer learning and statistical techniques to create lightweight seed list representations in the embedding space for each advertiser. We create a (user, seed list) affinity scoring function that makes use of these lightweight advertiser representations. We describe the end-to-end system that computes and serves this model at scale. Finally, we propose an ensemble approach that combines single-advertiser classifiers with the embedding-based technique. We show offline evaluation and online experiments to prove that the expanded audience generated by the ensemble model has the best results for all seed list sizes.},
	language = {en},
	urldate = {2022-11-15},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {deWet, Stephanie and Ou, Jiafan},
	month = jul,
	year = {2019},
	pages = {2251--2259},
	file = {deWet and Ou - 2019 - Finding Users Who Act Alike Transfer Learning for.pdf:/Users/yisu/Zotero/storage/EZBKQY8X/deWet and Ou - 2019 - Finding Users Who Act Alike Transfer Learning for.pdf:application/pdf},
}

@article{popov_adaptive_2018,
	title = {Adaptive look-alike targeting in social networks advertising},
	volume = {136},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050918315692},
	doi = {10.1016/j.procs.2018.08.264},
	abstract = {LAobosktr-alcitke targeting models are based on an assumption that user similarity correlates with the probability of receiving the same pnDaaoiiLpnDaaoitmtndndoefoefssouutdotdosspouuuuweweiipprkssttssnnottoottiieeee-oovviioovrrarrnnrreekkssiiesslttggssi’’hhssmffkeeww..iieeiiffesstteUUeeeeeeppss,,naaddtrrrrssttateetttteebbhhuuaauurissaaeegaaddrrnrreeygygcclleeeeennllkkeecsstssyyhhtti..ttocc,,eeniiooiimrrnnWWggddllgnniioogghhbbpooeemiiaalleennassyykkddddhhrott--vvrriddttaaaadbbsaahheeeevvlloettyyeeiirrppeeeelkknttsggeeiitteesssseehhtnnyyeeaiiovvmmeeddnneeraattetggddoooooolloohuu....nnbddnnettaaTTllTTaeehhiittbttsllnnhheeooeehhsseaddeeeeeeffdsllmmyyeeaaeccttiieeccohhnnlooiiaainnddkkneeiimnmrrttiibbeeiieaammppaaammaannssnrrlllcciiuuooouuzzkkaauuaccoppnneebbssscckrrooiieeiiseetteettll-ssrriiuhhiiccssaeeeettssmeesseelyyssddeeiffii,,kuupvveeiittmmmmnnooeddeelltlliﬂﬂddooyyoiimeeaammuu..ddnmmnnaaoeeTTeeppddppbbtnndllhhhrrppeecceccooiiiiarrlleelnnaassvvtiisssnneeee.mmoourrooddeebbttffsffoohhaaeeettttddooeellwwrhhttiieeaahhsrreehhssllddiuueessiippmeeccvveeaasseexxhheerrirraaffccpplrrffaaattﬀﬀtthhoollhhiiccrooiieessrrieettnniimmiitccooyyttnnyggttssrraaeeggsswwcaannddTT,,onncceecchhbbwwraaddrreeooryyeemmeeeuummtt..laammppssaOOsspprriiaatuuggnnaasseuuiiggnneeggooggsrrttggyynnnniieemmweennssxxnnSSssggieeppeeiitttaanntthggeeoohhmmaarraaffooaatiittppheellddiiaaoovlvlllennuuaaiioohheeccnnrrddpkkaaeeggggffiirss--aaeeeeoggaaaaccnnaabaallssllttcciiccggiiaooookknneehhboorrcceesseeiissirriieeddlaaiiiimmiiivvttnnlltnnhhddeeyooccnnmmuullddddllaaeeorruurreeaaiittfaaggddnnwwvvllppreeiiggeeoonnettpp--rrhhcttrrssggllaahhkkaaiicceggeeppeettaaiveeddaaiioolliirreessissss11nttssaassooaa22uugccppiiddbb..mmaapptt55tvvhhllpphll\%\%eeiieeiieeaanneccrrbbbbggaattssAAsiiiillppttssaaaeeiittUUaaiioomshshnnccooeennaaCCggeeessfftt..},
	language = {en},
	urldate = {2022-11-15},
	journal = {Procedia Computer Science},
	author = {Popov, Artem and Iakovleva, Daria},
	year = {2018},
	pages = {255--264},
	file = {Popov and Iakovleva - 2018 - Adaptive look-alike targeting in social networks a.pdf:/Users/yisu/Zotero/storage/EV6MVY98/Popov and Iakovleva - 2018 - Adaptive look-alike targeting in social networks a.pdf:application/pdf},
}

@inproceedings{chen_values_2021,
	address = {Amsterdam Netherlands},
	title = {Values of {User} {Exploration} in {Recommender} {Systems}},
	isbn = {978-1-4503-8458-2},
	url = {https://dl.acm.org/doi/10.1145/3460231.3474236},
	doi = {10.1145/3460231.3474236},
	abstract = {Reinforcement Learning (RL) has been sought after to bring nextgeneration recommender systems to further improve user experience on recommendation platforms. While the exploration-exploitation tradeoff is the foundation of RL research, the value of exploration in (RL-based) recommender systems is less well understood. Exploration, commonly seen as a tool to reduce model uncertainty in regions of sparse user interaction/feedback, is believed to cost user experience in the short term, while the indirect benefit of better model quality arrives at a later time. We focus on another aspect of exploration, which we refer to as user exploration to help discover new user interests, and argue it can improve user experience even in the more imminent term.},
	language = {en},
	urldate = {2022-11-30},
	booktitle = {Fifteenth {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Chen, Minmin and Wang, Yuyan and Xu, Can and Le, Ya and Sharma, Mohit and Richardson, Lee and Wu, Su-Lin and Chi, Ed},
	month = sep,
	year = {2021},
	pages = {85--95},
	file = {Chen et al. - 2021 - Values of User Exploration in Recommender Systems.pdf:/Users/yisu/Zotero/storage/RYMKXG4H/Chen et al. - 2021 - Values of User Exploration in Recommender Systems.pdf:application/pdf},
}

@article{mohri_foundations_nodate,
	title = {Foundations of {Machine} {Learning} (second edition)},
	language = {en},
	author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
	pages = {505},
	file = {Mohri et al. - Foundations of Machine Learning (second edition).pdf:/Users/yisu/Zotero/storage/BV3SHXLX/Mohri et al. - Foundations of Machine Learning (second edition).pdf:application/pdf},
}

@article{wright_augmented_nodate,
	title = {Augmented {Lagrangian} {Methods}},
	language = {en},
	author = {Wright, Stephen J},
	file = {Wright - Augmented Lagrangian Methods.pdf:/Users/yisu/Zotero/storage/9EE2N7H7/Wright - Augmented Lagrangian Methods.pdf:application/pdf},
}

@article{boyd_distributed_2010,
	title = {Distributed {Optimization} and {Statistical} {Learning} via the {Alternating} {Direction} {Method} of {Multipliers}},
	volume = {3},
	issn = {1935-8237, 1935-8245},
	url = {http://www.nowpublishers.com/article/Details/MAL-016},
	doi = {10.1561/2200000016},
	language = {en},
	number = {1},
	urldate = {2022-12-17},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Boyd, Stephen},
	year = {2010},
	pages = {1--122},
	file = {Boyd - 2010 - Distributed Optimization and Statistical Learning .pdf:/Users/yisu/Zotero/storage/IJS87JK5/Boyd - 2010 - Distributed Optimization and Statistical Learning .pdf:application/pdf},
}

@article{boyd_neal_nodate,
	title = {Neal {Parikh} {Department} of {Computer} {Science} {Stanford} {University}},
	language = {en},
	author = {Boyd, Stephen},
	file = {Boyd - Neal Parikh Department of Computer Science Stanfor.pdf:/Users/yisu/Zotero/storage/74GULQZT/Boyd - Neal Parikh Department of Computer Science Stanfor.pdf:application/pdf},
}

@misc{kang_decoupling_2020,
	title = {Decoupling {Representation} and {Classifier} for {Long}-{Tailed} {Recognition}},
	url = {http://arxiv.org/abs/1910.09217},
	abstract = {The long-tail distribution of the visual world poses great challenges for deep learning based classification models on how to handle the class imbalance problem. Existing solutions usually involve class-balancing strategies, e.g., by loss re-weighting, data re-sampling, or transfer learning from head- to tail-classes, but most of them adhere to the scheme of jointly learning representations and classifiers. In this work, we decouple the learning procedure into representation learning and classification, and systematically explore how different balancing strategies affect them for long-tailed recognition. The findings are surprising: (1) data imbalance might not be an issue in learning high-quality representations; (2) with representations learned with the simplest instance-balanced (natural) sampling, it is also possible to achieve strong long-tailed recognition ability by adjusting only the classifier. We conduct extensive experiments and set new state-of-the-art performance on common long-tailed benchmarks like ImageNet-LT, Places-LT and iNaturalist, showing that it is possible to outperform carefully designed losses, sampling strategies, even complex modules with memory, by using a straightforward approach that decouples representation and classification. Our code is available at https://github.com/facebookresearch/classifier-balancing.},
	urldate = {2022-12-18},
	publisher = {arXiv},
	author = {Kang, Bingyi and Xie, Saining and Rohrbach, Marcus and Yan, Zhicheng and Gordo, Albert and Feng, Jiashi and Kalantidis, Yannis},
	month = feb,
	year = {2020},
	note = {arXiv:1910.09217 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/SMHTGIS9/Kang et al. - 2020 - Decoupling Representation and Classifier for Long-.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/MUNN43GP/1910.html:text/html},
}

@misc{guo_survey_2020,
	title = {A {Survey} on {Knowledge} {Graph}-{Based} {Recommender} {Systems}},
	url = {http://arxiv.org/abs/2003.00911},
	abstract = {To solve the information explosion problem and enhance user experience in various online applications, recommender systems have been developed to model users preferences. Although numerous efforts have been made toward more personalized recommendations, recommender systems still suffer from several challenges, such as data sparsity and cold start. In recent years, generating recommendations with the knowledge graph as side information has attracted considerable interest. Such an approach can not only alleviate the abovementioned issues for a more accurate recommendation, but also provide explanations for recommended items. In this paper, we conduct a systematical survey of knowledge graph-based recommender systems. We collect recently published papers in this field and summarize them from two perspectives. On the one hand, we investigate the proposed algorithms by focusing on how the papers utilize the knowledge graph for accurate and explainable recommendation. On the other hand, we introduce datasets used in these works. Finally, we propose several potential research directions in this field.},
	urldate = {2023-01-12},
	publisher = {arXiv},
	author = {Guo, Qingyu and Zhuang, Fuzhen and Qin, Chuan and Zhu, Hengshu and Xie, Xing and Xiong, Hui and He, Qing},
	month = feb,
	year = {2020},
	note = {arXiv:2003.00911 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/HPIMH68T/Guo et al. - 2020 - A Survey on Knowledge Graph-Based Recommender Syst.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/74BDI35U/2003.html:text/html},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2023-02-21},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2022},
	note = {arXiv:1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/6RQ35KII/Kingma and Welling - 2022 - Auto-Encoding Variational Bayes.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/MGIT5H32/1312.html:text/html},
}

@article{schafer_recommender_nodate,
	title = {Recommender {Systems} in {E}-{Commerce}},
	abstract = {Recommender systems are changing from novelties used by a few E-commerce sites, to serious business tools that are re-shaping the world of E-commerce. Many of the largest commerce Web sites are already using recommender systems to help their customers find products to purchase. A recommender system learns from a customer and recommends products that she will find most valuable from among the available products. In this paper we present an explanation of how recommender systems help Ecommerce sites increase sales, and analyze six sites that use recommender systems including several sites that use more than one recommender system. Based on the examples, we create a taxonomy of recommender systems, including the interfaces they present to customers, the technologies used to create the recommendations, and the inputs they need from customers. We conclude with ideas for new applications of recommender systems to E-commerce.},
	language = {en},
	author = {Schafer, J Ben and Konstan, Joseph and Riedl, John},
	file = {Schafer et al. - Recommender Systems in E-Commerce.pdf:/Users/yisu/Zotero/storage/H4FNGRXC/Schafer et al. - Recommender Systems in E-Commerce.pdf:application/pdf},
}

@misc{zhou_comprehensive_2023,
	title = {A {Comprehensive} {Survey} on {Pretrained} {Foundation} {Models}: {A} {History} from {BERT} to {ChatGPT}},
	shorttitle = {A {Comprehensive} {Survey} on {Pretrained} {Foundation} {Models}},
	url = {http://arxiv.org/abs/2302.09419},
	abstract = {The Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A pretrained foundation model, such as BERT, GPT-3, MAE, DALLE-E, and ChatGPT, is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. The idea of pretraining behind PFMs plays an important role in the application of large models. Different from previous methods that apply convolution and recurrent modules for feature extractions, the generative pre-training (GPT) method applies Transformer as the feature extractor and is trained on large datasets with an autoregressive paradigm. Similarly, the BERT apples transformers to train on large datasets as a contextual language model. Recently, the ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few show prompting. With the extraordinary success of PFMs, AI has made waves in a variety of fields over the past few years. Considerable methods, datasets, and evaluation metrics have been proposed in the literature, the need is raising for an updated survey. This study provides a comprehensive review of recent research advancements, current and future challenges, and opportunities for PFMs in text, image, graph, as well as other data modalities. We first review the basic components and existing pretraining in natural language processing, computer vision, and graph learning. We then discuss other advanced PFMs for other data modalities and unified PFMs considering the data quality and quantity. Besides, we discuss relevant research about the fundamentals of the PFM, including model efficiency and compression, security, and privacy. Finally, we lay out key implications, future research directions, challenges, and open problems.},
	urldate = {2023-02-24},
	publisher = {arXiv},
	author = {Zhou, Ce and Li, Qian and Li, Chen and Yu, Jun and Liu, Yixin and Wang, Guangjing and Zhang, Kai and Ji, Cheng and Yan, Qiben and He, Lifang and Peng, Hao and Li, Jianxin and Wu, Jia and Liu, Ziwei and Xie, Pengtao and Xiong, Caiming and Pei, Jian and Yu, Philip S. and Sun, Lichao},
	month = feb,
	year = {2023},
	note = {arXiv:2302.09419 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/UPXQVIQJ/Zhou et al. - 2023 - A Comprehensive Survey on Pretrained Foundation Mo.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/VLW8X9SK/2302.html:text/html},
}

@misc{pereyra_regularizing_2017,
	title = {Regularizing {Neural} {Networks} by {Penalizing} {Confident} {Output} {Distributions}},
	url = {http://arxiv.org/abs/1701.06548},
	abstract = {We systematically explore regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. Furthermore, we connect a maximum entropy based confidence penalty to label smoothing through the direction of the KL divergence. We exhaustively evaluate the proposed confidence penalty and label smoothing on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and the confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyperparameters, suggesting the wide applicability of these regularizers.},
	urldate = {2023-02-27},
	publisher = {arXiv},
	author = {Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, Łukasz and Hinton, Geoffrey},
	month = jan,
	year = {2017},
	note = {arXiv:1701.06548 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/2JX856T3/Pereyra et al. - 2017 - Regularizing Neural Networks by Penalizing Confide.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/PRPHSE3K/1701.html:text/html},
}

@misc{lin_focal_2018,
	title = {Focal {Loss} for {Dense} {Object} {Detection}},
	url = {http://arxiv.org/abs/1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	month = feb,
	year = {2018},
	note = {arXiv:1708.02002 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/I4PLHCQ8/Lin et al. - 2018 - Focal Loss for Dense Object Detection.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/N2S3JF8G/1708.html:text/html},
}

@inproceedings{szegedy_rethinking_2016,
	address = {Las Vegas, NV, USA},
	title = {Rethinking the {Inception} {Architecture} for {Computer} {Vision}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780677/},
	doi = {10.1109/CVPR.2016.308},
	abstract = {Convolutional networks are at the core of most stateof-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efﬁciency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efﬁciently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classiﬁcation challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error and 17.3\% top-1 error.},
	language = {en},
	urldate = {2023-02-28},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
	month = jun,
	year = {2016},
	pages = {2818--2826},
	file = {Szegedy et al. - 2016 - Rethinking the Inception Architecture for Computer.pdf:/Users/yisu/Zotero/storage/66IRXS83/Szegedy et al. - 2016 - Rethinking the Inception Architecture for Computer.pdf:application/pdf},
}

@misc{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2023-03-03},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv:1301.3781 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/WXPGCX7X/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/K5SBUKR5/1301.html:text/html},
}

@article{bengio_neural_nodate,
	title = {A {Neural} {Probabilistic} {Language} {Model}},
	abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difﬁcult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to ﬁght the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a signiﬁcant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach signiﬁcantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
	language = {en},
	author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Jauvin, Christian},
	file = {Bengio et al. - A Neural Probabilistic Language Model.pdf:/Users/yisu/Zotero/storage/4IG9ER5G/Bengio et al. - A Neural Probabilistic Language Model.pdf:application/pdf},
}

@inproceedings{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html},
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships.  In this paper we present several improvements that make the Skip-gram model more expressive and enable it to learn higher quality vectors more rapidly.  We show that by subsampling frequent words we obtain significant speedup,  and also learn higher quality representations as measured by our tasks. We also introduce Negative Sampling, a simplified variant of Noise Contrastive Estimation (NCE) that learns more accurate vectors for frequent words compared to the hierarchical softmax.   An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases.  For example, the meanings of Canada'' and "Air'' cannot be easily combined to obtain "Air Canada''.  Motivated by this example, we present a simple and efficient method for finding phrases, and show that their vector representations can be accurately learned by the Skip-gram model. "},
	urldate = {2023-03-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	year = {2013},
	file = {Full Text PDF:/Users/yisu/Zotero/storage/ZYJ2T5Y2/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf:application/pdf},
}

@inproceedings{pennington_glove_2014,
	address = {Doha, Qatar},
	title = {{GloVe}: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {{GloVe}},
	url = {https://aclanthology.org/D14-1162},
	doi = {10.3115/v1/D14-1162},
	urldate = {2023-03-05},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	month = oct,
	year = {2014},
	pages = {1532--1543},
	file = {Full Text PDF:/Users/yisu/Zotero/storage/5XMMGUW4/Pennington et al. - 2014 - GloVe Global Vectors for Word Representation.pdf:application/pdf},
}

@misc{bojanowski_enriching_2017,
	title = {Enriching {Word} {Vectors} with {Subword} {Information}},
	url = {http://arxiv.org/abs/1607.04606},
	abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character \$n\$-grams. A vector representation is associated to each character \$n\$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	month = jun,
	year = {2017},
	note = {arXiv:1607.04606 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/NI9V3FXQ/Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/5RGBLRNV/1607.html:text/html},
}

@misc{joulin_fasttextzip_2016,
	title = {{FastText}.zip: {Compressing} text classification models},
	shorttitle = {{FastText}.zip},
	url = {http://arxiv.org/abs/1612.03651},
	abstract = {We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory. After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store word embeddings. While the original technique leads to a loss in accuracy, we adapt this method to circumvent quantization artefacts. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and Jégou, Hérve and Mikolov, Tomas},
	month = dec,
	year = {2016},
	note = {arXiv:1612.03651 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/BUVCYXKC/Joulin et al. - 2016 - FastText.zip Compressing text classification mode.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/NRZAWDGW/1612.html:text/html},
}

@article{wang_learning_2016,
	title = {Learning to {Hash} for {Indexing} {Big} {Data}—{A} {Survey}},
	volume = {104},
	issn = {1558-2256},
	doi = {10.1109/JPROC.2015.2487976},
	abstract = {The explosive growth in Big Data has attracted much attention in designing efficient indexing and search methods recently. In many critical applications such as large-scale search and pattern matching, finding the nearest neighbors to a query is a fundamental research problem. However, the straightforward solution using exhaustive comparison is infeasible due to the prohibitive computational complexity and memory requirement. In response, approximate nearest neighbor (ANN) search based on hashing techniques has become popular due to its promising performance in both efficiency and accuracy. Prior randomized hashing methods, e.g., locality-sensitive hashing (LSH), explore data-independent hash functions with random projections or permutations. Although having elegant theoretic guarantees on the search quality in certain metric spaces, performance of randomized hashing has been shown insufficient in many real-world applications. As a remedy, new approaches incorporating data-driven learning methods in development of advanced hash functions have emerged. Such learning-to-hash methods exploit information such as data distributions or class labels when optimizing the hash codes or functions. Importantly, the learned hash codes are able to preserve the proximity of neighboring data in the original feature spaces in the hash code spaces. The goal of this paper is to provide readers with systematic understanding of insights, pros, and cons of the emerging techniques. We provide a comprehensive survey of the learning-to-hash framework and representative techniques of various types, including unsupervised, semisupervised, and supervised. In addition, we also summarize recent hashing approaches utilizing the deep learning models. Finally, we discuss the future direction and trends of research in this area.},
	number = {1},
	journal = {Proceedings of the IEEE},
	author = {Wang, Jun and Liu, Wei and Kumar, Sanjiv and Chang, Shih-Fu},
	month = jan,
	year = {2016},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Big data, Approximate nearest neighbor (ANN) search, Artificial neural networks, Binary codes, deep learning, Indexing, learning to hash, Semantics, semisupervised learning, supervised learning, Tagging, Twitter, unsupervised learning},
	pages = {34--57},
	file = {IEEE Xplore Abstract Record:/Users/yisu/Zotero/storage/EHNQWDZB/stamp.html:text/html;IEEE Xplore Full Text PDF:/Users/yisu/Zotero/storage/2CXC8GQE/Wang et al. - 2016 - Learning to Hash for Indexing Big Data—A Survey.pdf:application/pdf},
}

@article{gionis_similarity_nodate,
	title = {Similarity {Search} in {High} {Dimensions} via {Hashing}},
	language = {en},
	author = {Gionis, Aristides and Indyk, Piotr and Motwani, Rajeev},
	file = {Gionis et al. - Similarity Search in High Dimensions via Hashing.pdf:/Users/yisu/Zotero/storage/67X4RD7X/Gionis et al. - Similarity Search in High Dimensions via Hashing.pdf:application/pdf},
}

@misc{joulin_bag_2016,
	title = {Bag of {Tricks} for {Efficient} {Text} {Classification}},
	url = {http://arxiv.org/abs/1607.01759},
	abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore{\textasciitilde}CPU, and classify half a million sentences among{\textasciitilde}312K classes in less than a minute.},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
	month = aug,
	year = {2016},
	note = {arXiv:1607.01759 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/277PLAWC/Joulin et al. - 2016 - Bag of Tricks for Efficient Text Classification.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/3GITKE4H/1607.html:text/html},
}

@inproceedings{wang_dcn_2021,
	title = {{DCN} {V2}: {Improved} {Deep} \& {Cross} {Network} and {Practical} {Lessons} for {Web}-scale {Learning} to {Rank} {Systems}},
	shorttitle = {{DCN} {V2}},
	url = {http://arxiv.org/abs/2008.13535},
	doi = {10.1145/3442381.3450078},
	abstract = {Learning effective feature crosses is the key behind building recommender systems. However, the sparse and large feature space requires exhaustive search to identify effective crosses. Deep \& Cross Network (DCN) was proposed to automatically and efficiently learn bounded-degree predictive feature interactions. Unfortunately, in models that serve web-scale traffic with billions of training examples, DCN showed limited expressiveness in its cross network at learning more predictive feature interactions. Despite significant research progress made, many deep learning models in production still rely on traditional feed-forward neural networks to learn feature crosses inefficiently. In light of the pros/cons of DCN and existing feature interaction learning approaches, we propose an improved framework DCN-V2 to make DCN more practical in large-scale industrial settings. In a comprehensive experimental study with extensive hyper-parameter search and model tuning, we observed that DCN-V2 approaches outperform all the state-of-the-art algorithms on popular benchmark datasets. The improved DCN-V2 is more expressive yet remains cost efficient at feature interaction learning, especially when coupled with a mixture of low-rank architecture. DCN-V2 is simple, can be easily adopted as building blocks, and has delivered significant offline accuracy and online business metrics gains across many web-scale learning to rank systems at Google.},
	urldate = {2023-03-08},
	booktitle = {Proceedings of the {Web} {Conference} 2021},
	author = {Wang, Ruoxi and Shivanna, Rakesh and Cheng, Derek Z. and Jain, Sagar and Lin, Dong and Hong, Lichan and Chi, Ed H.},
	month = apr,
	year = {2021},
	note = {arXiv:2008.13535 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval},
	pages = {1785--1797},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/CNSGYU83/Wang et al. - 2021 - DCN V2 Improved Deep & Cross Network and Practica.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/CPWGD628/2008.html:text/html},
}

@misc{wang_deep_2017,
	title = {Deep \& {Cross} {Network} for {Ad} {Click} {Predictions}},
	url = {http://arxiv.org/abs/1708.05123},
	abstract = {Feature engineering has been the key to the success of many prediction models. However, the process is non-trivial and often requires manual feature engineering or exhaustive searching. DNNs are able to automatically learn feature interactions; however, they generate all the interactions implicitly, and are not necessarily efficient in learning all types of cross features. In this paper, we propose the Deep \& Cross Network (DCN) which keeps the benefits of a DNN model, and beyond that, it introduces a novel cross network that is more efficient in learning certain bounded-degree feature interactions. In particular, DCN explicitly applies feature crossing at each layer, requires no manual feature engineering, and adds negligible extra complexity to the DNN model. Our experimental results have demonstrated its superiority over the state-of-art algorithms on the CTR prediction dataset and dense classification dataset, in terms of both model accuracy and memory usage.},
	urldate = {2023-03-08},
	publisher = {arXiv},
	author = {Wang, Ruoxi and Fu, Bin and Fu, Gang and Wang, Mingliang},
	month = aug,
	year = {2017},
	note = {arXiv:1708.05123 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/679JCHNM/Wang et al. - 2017 - Deep & Cross Network for Ad Click Predictions.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/YSPAQKSI/1708.html:text/html},
}

@misc{li_align_2021-1,
	title = {Align before {Fuse}: {Vision} and {Language} {Representation} {Learning} with {Momentum} {Distillation}},
	shorttitle = {Align before {Fuse}},
	url = {http://arxiv.org/abs/2107.07651},
	abstract = {Large-scale vision and language representation learning has shown promising improvements on various vision-language tasks. Most existing methods employ a transformer-based multimodal encoder to jointly model visual tokens (region-based image features) and word tokens. Because the visual tokens and word tokens are unaligned, it is challenging for the multimodal encoder to learn image-text interactions. In this paper, we introduce a contrastive loss to ALign the image and text representations BEfore Fusing (ALBEF) them through cross-modal attention, which enables more grounded vision and language representation learning. Unlike most existing methods, our method does not require bounding box annotations nor high-resolution images. In order to improve learning from noisy web data, we propose momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model. We provide a theoretical analysis of ALBEF from a mutual information maximization perspective, showing that different training tasks can be interpreted as different ways to generate views for an image-text pair. ALBEF achieves state-of-the-art performance on multiple downstream vision-language tasks. On image-text retrieval, ALBEF outperforms methods that are pre-trained on orders of magnitude larger datasets. On VQA and NLVR\${\textasciicircum}2\$, ALBEF achieves absolute improvements of 2.37\% and 3.84\% compared to the state-of-the-art, while enjoying faster inference speed. Code and pre-trained models are available at https://github.com/salesforce/ALBEF/.},
	urldate = {2023-03-10},
	publisher = {arXiv},
	author = {Li, Junnan and Selvaraju, Ramprasaath R. and Gotmare, Akhilesh Deepak and Joty, Shafiq and Xiong, Caiming and Hoi, Steven},
	month = oct,
	year = {2021},
	note = {arXiv:2107.07651 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/FKWPXP9L/Li et al. - 2021 - Align before Fuse Vision and Language Representat.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/76YUVPAR/2107.html:text/html},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	urldate = {2023-03-10},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/IB5EFAYC/Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/25JWXCRJ/2203.html:text/html},
}

@misc{ruder_overview_2017-1,
	title = {An {Overview} of {Multi}-{Task} {Learning} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1706.05098},
	abstract = {Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.},
	urldate = {2023-03-10},
	publisher = {arXiv},
	author = {Ruder, Sebastian},
	month = jun,
	year = {2017},
	note = {arXiv:1706.05098 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/SNHMJ2SP/Ruder - 2017 - An Overview of Multi-Task Learning in Deep Neural .pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/AE9QWG6L/1706.html:text/html},
}

@misc{dutting_optimal_2022,
	title = {Optimal {Auctions} through {Deep} {Learning}: {Advances} in {Differentiable} {Economics}},
	shorttitle = {Optimal {Auctions} through {Deep} {Learning}},
	url = {http://arxiv.org/abs/1706.03459},
	abstract = {Designing an incentive compatible auction that maximizes expected revenue is an intricate task. The single-item case was resolved in a seminal piece of work by Myerson in 1981, but more than 40 years later a full analytical understanding of the optimal design still remains elusive for settings with two or more items. In this work, we initiate the exploration of the use of tools from deep learning for the automated design of optimal auctions. We model an auction as a multi-layer neural network, frame optimal auction design as a constrained learning problem, and show how it can be solved using standard machine learning pipelines. In addition to providing generalization bounds, we present extensive experimental results, recovering essentially all known solutions that come from the theoretical analysis of optimal auction design problems and obtaining novel mechanisms for settings in which the optimal mechanism is unknown.},
	urldate = {2023-03-13},
	publisher = {arXiv},
	author = {Dütting, Paul and Feng, Zhe and Narasimhan, Harikrishna and Parkes, David C. and Ravindranath, Sai Srivatsa},
	month = oct,
	year = {2022},
	note = {arXiv:1706.03459 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/KVEDE9DX/Dütting et al. - 2022 - Optimal Auctions through Deep Learning Advances i.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/SHFI43NK/1706.html:text/html},
}

@inproceedings{cipolla_multi-task_2018,
	address = {Salt Lake City, UT, USA},
	title = {Multi-task {Learning} {Using} {Uncertainty} to {Weigh} {Losses} for {Scene} {Geometry} and {Semantics}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578879/},
	doi = {10.1109/CVPR.2018.00781},
	abstract = {Numerous deep learning applications beneﬁt from multitask learning with multiple regression and classiﬁcation objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task’s loss. Tuning these weights by hand is a difﬁcult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classiﬁcation and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.},
	language = {en},
	urldate = {2023-03-17},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Cipolla, Roberto and Gal, Yarin and Kendall, Alex},
	month = jun,
	year = {2018},
	pages = {7482--7491},
	file = {Cipolla et al. - 2018 - Multi-task Learning Using Uncertainty to Weigh Los.pdf:/Users/yisu/Zotero/storage/7W5CZEBV/Cipolla et al. - 2018 - Multi-task Learning Using Uncertainty to Weigh Los.pdf:application/pdf},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2023-03-19},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/6DI5CYP6/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/4Y9BTUCP/1706.html:text/html},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2023-03-19},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/7BQD7BHA/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/XYRY43XQ/1810.html:text/html},
}

@misc{cui_class-balanced_2019,
	title = {Class-{Balanced} {Loss} {Based} on {Effective} {Number} of {Samples}},
	url = {http://arxiv.org/abs/1901.05555},
	abstract = {With the rapid increase of large-scale, real-world datasets, it becomes critical to address the problem of long-tailed data distribution (i.e., a few classes account for most of the data, while most classes are under-represented). Existing solutions typically adopt class re-balancing strategies such as re-sampling and re-weighting based on the number of observations for each class. In this work, we argue that as the number of samples increases, the additional benefit of a newly added data point will diminish. We introduce a novel theoretical framework to measure data overlap by associating with each sample a small neighboring region rather than a single point. The effective number of samples is defined as the volume of samples and can be calculated by a simple formula \$(1-{\textbackslash}beta{\textasciicircum}\{n\})/(1-{\textbackslash}beta)\$, where \$n\$ is the number of samples and \${\textbackslash}beta {\textbackslash}in [0,1)\$ is a hyperparameter. We design a re-weighting scheme that uses the effective number of samples for each class to re-balance the loss, thereby yielding a class-balanced loss. Comprehensive experiments are conducted on artificially induced long-tailed CIFAR datasets and large-scale datasets including ImageNet and iNaturalist. Our results show that when trained with the proposed class-balanced loss, the network is able to achieve significant performance gains on long-tailed datasets.},
	urldate = {2023-03-20},
	publisher = {arXiv},
	author = {Cui, Yin and Jia, Menglin and Lin, Tsung-Yi and Song, Yang and Belongie, Serge},
	month = jan,
	year = {2019},
	note = {arXiv:1901.05555 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/62ZR8GQV/Cui et al. - 2019 - Class-Balanced Loss Based on Effective Number of S.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/G9KYEHBD/1901.html:text/html},
}

@misc{crawshaw_multi-task_2020,
	title = {Multi-{Task} {Learning} with {Deep} {Neural} {Networks}: {A} {Survey}},
	shorttitle = {Multi-{Task} {Learning} with {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2009.09796},
	abstract = {Multi-task learning (MTL) is a subfield of machine learning in which multiple tasks are simultaneously learned by a shared model. Such approaches offer advantages like improved data efficiency, reduced overfitting through shared representations, and fast learning by leveraging auxiliary information. However, the simultaneous learning of multiple tasks presents new design and optimization challenges, and choosing which tasks should be learned jointly is in itself a non-trivial problem. In this survey, we give an overview of multi-task learning methods for deep neural networks, with the aim of summarizing both the well-established and most recent directions within the field. Our discussion is structured according to a partition of the existing deep MTL techniques into three groups: architectures, optimization methods, and task relationship learning. We also provide a summary of common multi-task benchmarks.},
	urldate = {2023-03-20},
	publisher = {arXiv},
	author = {Crawshaw, Michael},
	month = sep,
	year = {2020},
	note = {arXiv:2009.09796 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/XKKY296Y/Crawshaw - 2020 - Multi-Task Learning with Deep Neural Networks A S.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/6ACTTEX2/2009.html:text/html},
}

@misc{sener_multi-task_2019,
	title = {Multi-{Task} {Learning} as {Multi}-{Objective} {Optimization}},
	url = {http://arxiv.org/abs/1810.04650},
	abstract = {In multi-task learning, multiple tasks are solved jointly, sharing inductive bias between them. Multi-task learning is inherently a multi-objective problem because different tasks may conflict, necessitating a trade-off. A common compromise is to optimize a proxy objective that minimizes a weighted linear combination of per-task losses. However, this workaround is only valid when the tasks do not compete, which is rarely the case. In this paper, we explicitly cast multi-task learning as multi-objective optimization, with the overall objective of finding a Pareto optimal solution. To this end, we use algorithms developed in the gradient-based multi-objective optimization literature. These algorithms are not directly applicable to large-scale learning problems since they scale poorly with the dimensionality of the gradients and the number of tasks. We therefore propose an upper bound for the multi-objective loss and show that it can be optimized efficiently. We further prove that optimizing this upper bound yields a Pareto optimal solution under realistic assumptions. We apply our method to a variety of multi-task deep learning problems including digit classification, scene understanding (joint semantic segmentation, instance segmentation, and depth estimation), and multi-label classification. Our method produces higher-performing models than recent multi-task learning formulations or per-task training.},
	urldate = {2023-03-24},
	publisher = {arXiv},
	author = {Sener, Ozan and Koltun, Vladlen},
	month = jan,
	year = {2019},
	note = {arXiv:1810.04650 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/UQ7WV7Z8/Sener and Koltun - 2019 - Multi-Task Learning as Multi-Objective Optimizatio.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/HTKZ8N79/1810.html:text/html},
}

@inproceedings{chopra_learning_2005,
	address = {San Diego, CA, USA},
	title = {Learning a {Similarity} {Metric} {Discriminatively}, with {Application} to {Face} {Verification}},
	volume = {1},
	isbn = {978-0-7695-2372-9},
	url = {http://ieeexplore.ieee.org/document/1467314/},
	doi = {10.1109/CVPR.2005.202},
	abstract = {We present a method for training a similarity metric from data. The method can be used for recognition or veriﬁcation applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the ✂☎✄ norm in the target space approximates the “semantic” distance in the input space. The method is applied to a face veriﬁcation task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of variability in the pose, lighting, expression, position, and artiﬁcial occlusions such as dark glasses and obscuring scarves.},
	language = {en},
	urldate = {2023-03-24},
	booktitle = {2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'05)},
	publisher = {IEEE},
	author = {Chopra, S. and Hadsell, R. and LeCun, Y.},
	year = {2005},
	pages = {539--546},
	file = {Chopra et al. - 2005 - Learning a Similarity Metric Discriminatively, wit.pdf:/Users/yisu/Zotero/storage/YY996359/Chopra et al. - 2005 - Learning a Similarity Metric Discriminatively, wit.pdf:application/pdf},
}

@inproceedings{yi_sampling-bias-corrected_2019,
	address = {Copenhagen Denmark},
	title = {Sampling-bias-corrected neural modeling for large corpus item recommendations},
	isbn = {978-1-4503-6243-6},
	url = {https://dl.acm.org/doi/10.1145/3298689.3346996},
	doi = {10.1145/3298689.3346996},
	abstract = {Many recommendation systems retrieve and score items from a very large corpus. A common recipe to handle data sparsity and power-law item distribution is to learn item representations from its content features. Apart from many content-aware systems based on matrix factorization, we consider a modeling framework using two-tower neural net, with one of the towers (item tower) encoding a wide variety of item content features. A general recipe of training such two-tower models is to optimize loss functions calculated from in-batch negatives, which are items sampled from a random minibatch. However, in-batch loss is subject to sampling biases, potentially hurting model performance, particularly in the case of highly skewed distribution. In this paper, we present a novel algorithm for estimating item frequency from streaming data. Through theoretical analysis and simulation, we show that the proposed algorithm can work without requiring fixed item vocabulary, and is capable of producing unbiased estimation and being adaptive to item distribution change. We then apply the sampling-bias-corrected modeling approach to build a large scale neural retrieval system for YouTube recommendations. The system is deployed to retrieve personalized suggestions from a corpus with tens of millions of videos. We demonstrate the effectiveness of sampling-bias correction through offline experiments on two real-world datasets. We also conduct live A/B testings to show that the neural retrieval system leads to improved recommendation quality for YouTube.},
	language = {en},
	urldate = {2023-03-25},
	booktitle = {Proceedings of the 13th {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Yi, Xinyang and Yang, Ji and Hong, Lichan and Cheng, Derek Zhiyuan and Heldt, Lukasz and Kumthekar, Aditee and Zhao, Zhe and Wei, Li and Chi, Ed},
	month = sep,
	year = {2019},
	pages = {269--277},
	file = {Yi et al. - 2019 - Sampling-bias-corrected neural modeling for large .pdf:/Users/yisu/Zotero/storage/GLVQ8CCK/Yi et al. - 2019 - Sampling-bias-corrected neural modeling for large .pdf:application/pdf},
}

@inproceedings{schroff_facenet_2015,
	title = {{FaceNet}: {A} {Unified} {Embedding} for {Face} {Recognition} and {Clustering}},
	shorttitle = {{FaceNet}},
	url = {http://arxiv.org/abs/1503.03832},
	doi = {10.1109/CVPR.2015.7298682},
	abstract = {Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63\%. On YouTube Faces DB it achieves 95.12\%. Our system cuts the error rate in comparison to the best published result by 30\% on both datasets. We also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other.},
	urldate = {2023-03-25},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
	month = jun,
	year = {2015},
	note = {arXiv:1503.03832 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {815--823},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/LSRRRAFP/Schroff et al. - 2015 - FaceNet A Unified Embedding for Face Recognition .pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/M6SDTLLC/1503.html:text/html},
}

@inproceedings{sohn_improved_2016,
	title = {Improved {Deep} {Metric} {Learning} with {Multi}-class {N}-pair {Loss} {Objective}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/6b180037abbebea991d8b1232f8a8ca9-Abstract.html},
	abstract = {Deep metric learning has gained much popularity in recent years, following the success of deep learning. However, existing frameworks of deep metric learning based on contrastive loss and triplet loss often suffer from slow convergence, partially because they employ only one negative example while not interacting with the other negative classes in each update. In this paper, we propose to address this problem with a new metric learning objective called multi-class N-pair loss. The proposed objective function firstly generalizes triplet loss by allowing joint comparison among more than one negative examples – more specifically, N-1 negative examples – and secondly reduces the computational burden of evaluating deep embedding vectors via an efficient batch construction strategy using only N pairs of examples, instead of (N+1)×N. We demonstrate the superiority of our proposed loss to the triplet loss as well as other competing loss functions for a variety of tasks on several visual recognition benchmark, including fine-grained object recognition and verification, image clustering and retrieval, and face verification and identification.},
	urldate = {2023-03-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sohn, Kihyuk},
	year = {2016},
	file = {Full Text PDF:/Users/yisu/Zotero/storage/Q4QKQUPG/Sohn - 2016 - Improved Deep Metric Learning with Multi-class N-p.pdf:application/pdf},
}

@misc{oord_representation_2019,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	urldate = {2023-03-25},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	note = {arXiv:1807.03748 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/MMM5I3N3/Oord et al. - 2019 - Representation Learning with Contrastive Predictiv.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/D77DCD3X/1807.html:text/html},
}

@misc{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	urldate = {2023-03-25},
	publisher = {arXiv},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {arXiv:2002.05709 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/UWP4NHLG/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/9R2HT2QM/2002.html:text/html},
}

@article{le-khac_contrastive_2020,
	title = {Contrastive {Representation} {Learning}: {A} {Framework} and {Review}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {Contrastive {Representation} {Learning}},
	url = {http://arxiv.org/abs/2010.05113},
	doi = {10.1109/ACCESS.2020.3031549},
	abstract = {Contrastive Learning has recently received interest due to its success in self-supervised representation learning in the computer vision domain. However, the origins of Contrastive Learning date as far back as the 1990s and its development has spanned across many fields and domains including Metric Learning and natural language processing. In this paper we provide a comprehensive literature review and we propose a general Contrastive Representation Learning framework that simplifies and unifies many different contrastive learning methods. We also provide a taxonomy for each of the components of contrastive learning in order to summarise it and distinguish it from other forms of machine learning. We then discuss the inductive biases which are present in any contrastive learning system and we analyse our framework under different views from various sub-fields of Machine Learning. Examples of how contrastive learning has been applied in computer vision, natural language processing, audio processing, and others, as well as in Reinforcement Learning are also presented. Finally, we discuss the challenges and some of the most promising future research directions ahead.},
	urldate = {2023-03-25},
	journal = {IEEE Access},
	author = {Le-Khac, Phuc H. and Healy, Graham and Smeaton, Alan F.},
	year = {2020},
	note = {arXiv:2010.05113 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {193907--193934},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/YX4BWHTG/Le-Khac et al. - 2020 - Contrastive Representation Learning A Framework a.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/367SIV58/2010.html:text/html},
}

@inproceedings{huang_embedding-based_2020,
	title = {Embedding-based {Retrieval} in {Facebook} {Search}},
	url = {http://arxiv.org/abs/2006.11632},
	doi = {10.1145/3394486.3403305},
	abstract = {Search in social networks such as Facebook poses different challenges than in classical web search: besides the query text, it is important to take into account the searcher's context to provide relevant results. Their social graph is an integral part of this context and is a unique aspect of Facebook search. While embedding-based retrieval (EBR) has been applied in eb search engines for years, Facebook search was still mainly based on a Boolean matching model. In this paper, we discuss the techniques for applying EBR to a Facebook Search system. We introduce the unified embedding framework developed to model semantic embeddings for personalized search, and the system to serve embedding-based retrieval in a typical search system based on an inverted index. We discuss various tricks and experiences on end-to-end optimization of the whole system, including ANN parameter tuning and full-stack optimization. Finally, we present our progress on two selected advanced topics about modeling. We evaluated EBR on verticals for Facebook Search with significant metrics gains observed in online A/B experiments. We believe this paper will provide useful insights and experiences to help people on developing embedding-based retrieval systems in search engines.},
	urldate = {2023-03-26},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	author = {Huang, Jui-Ting and Sharma, Ashish and Sun, Shuying and Xia, Li and Zhang, David and Pronin, Philip and Padmanabhan, Janani and Ottaviano, Giuseppe and Yang, Linjun},
	month = aug,
	year = {2020},
	note = {arXiv:2006.11632 [cs]},
	keywords = {Computer Science - Information Retrieval},
	pages = {2553--2561},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/GJGXNEXX/Huang et al. - 2020 - Embedding-based Retrieval in Facebook Search.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/GT7TS2L9/2006.html:text/html},
}

@misc{yao_self-supervised_2021,
	title = {Self-supervised {Learning} for {Large}-scale {Item} {Recommendations}},
	url = {http://arxiv.org/abs/2007.12865},
	abstract = {Large scale recommender models find most relevant items from huge catalogs, and they play a critical role in modern search and recommendation systems. To model the input space with large-vocab categorical features, a typical recommender model learns a joint embedding space through neural networks for both queries and items from user feedback data. However, with millions to billions of items in the corpus, users tend to provide feedback for a very small set of them, causing a power-law distribution. This makes the feedback data for long-tail items extremely sparse. Inspired by the recent success in self-supervised representation learning research in both computer vision and natural language understanding, we propose a multi-task self-supervised learning (SSL) framework for large-scale item recommendations. The framework is designed to tackle the label sparsity problem by learning better latent relationship of item features. Specifically, SSL improves item representation learning as well as serving as additional regularization to improve generalization. Furthermore, we propose a novel data augmentation method that utilizes feature correlations within the proposed framework. We evaluate our framework using two real-world datasets with 500M and 1B training examples respectively. Our results demonstrate the effectiveness of SSL regularization and show its superior performance over the state-of-the-art regularization techniques. We also have already launched the proposed techniques to a web-scale commercial app-to-app recommendation system, with significant improvements top-tier business metrics demonstrated in A/B experiments on live traffic. Our online results also verify our hypothesis that our framework indeed improves model performance even more on slices that lack supervision.},
	urldate = {2023-03-26},
	publisher = {arXiv},
	author = {Yao, Tiansheng and Yi, Xinyang and Cheng, Derek Zhiyuan and Yu, Felix and Chen, Ting and Menon, Aditya and Hong, Lichan and Chi, Ed H. and Tjoa, Steve and Kang, Jieqi and Ettinger, Evan},
	month = feb,
	year = {2021},
	note = {arXiv:2007.12865 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/9RJUGVJ5/Yao et al. - 2021 - Self-supervised Learning for Large-scale Item Reco.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/IK2GZ7RD/2007.html:text/html},
}

@misc{he_neural_2017,
	title = {Neural {Collaborative} {Filtering}},
	url = {http://arxiv.org/abs/1708.05031},
	abstract = {In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation — collaborative ﬁltering — on the basis of implicit feedback.},
	language = {en},
	urldate = {2023-03-27},
	publisher = {arXiv},
	author = {He, Xiangnan and Liao, Lizi and Zhang, Hanwang and Nie, Liqiang and Hu, Xia and Chua, Tat-Seng},
	month = aug,
	year = {2017},
	note = {arXiv:1708.05031 [cs]},
	keywords = {Computer Science - Information Retrieval},
	file = {He et al. - 2017 - Neural Collaborative Filtering.pdf:/Users/yisu/Zotero/storage/HVSP92WS/He et al. - 2017 - Neural Collaborative Filtering.pdf:application/pdf},
}

@misc{naumov_deep_2019,
	title = {Deep {Learning} {Recommendation} {Model} for {Personalization} and {Recommendation} {Systems}},
	url = {http://arxiv.org/abs/1906.00091},
	abstract = {With the advent of deep learning, neural network-based recommendation models have emerged as an important tool for tackling personalization and recommendation tasks. These networks differ significantly from other deep learning networks due to their need to handle categorical features and are not well studied or understood. In this paper, we develop a state-of-the-art deep learning recommendation model (DLRM) and provide its implementation in both PyTorch and Caffe2 frameworks. In addition, we design a specialized parallelization scheme utilizing model parallelism on the embedding tables to mitigate memory constraints while exploiting data parallelism to scale-out compute from the fully-connected layers. We compare DLRM against existing recommendation models and characterize its performance on the Big Basin AI platform, demonstrating its usefulness as a benchmark for future algorithmic experimentation and system co-design.},
	urldate = {2023-03-27},
	publisher = {arXiv},
	author = {Naumov, Maxim and Mudigere, Dheevatsa and Shi, Hao-Jun Michael and Huang, Jianyu and Sundaraman, Narayanan and Park, Jongsoo and Wang, Xiaodong and Gupta, Udit and Wu, Carole-Jean and Azzolini, Alisson G. and Dzhulgakov, Dmytro and Mallevich, Andrey and Cherniavskii, Ilia and Lu, Yinghai and Krishnamoorthi, Raghuraman and Yu, Ansha and Kondratenko, Volodymyr and Pereira, Stephanie and Chen, Xianjie and Chen, Wenlin and Rao, Vijay and Jia, Bill and Xiong, Liang and Smelyanskiy, Misha},
	month = may,
	year = {2019},
	note = {arXiv:1906.00091 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval, I.2.6, 68T05, H.3.3, H.3.4, I.5.0},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/9R2GCU6Y/Naumov et al. - 2019 - Deep Learning Recommendation Model for Personaliza.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/BD3Q8L49/1906.html:text/html},
}

@article{volkovs_dropoutnet_nodate,
	title = {{DropoutNet}: {Addressing} {Cold} {Start} in {Recommender} {Systems}},
	abstract = {Latent models have become the default choice for recommender systems due to their performance and scalability. However, research in this area has primarily focused on modeling user-item interactions, and few latent models have been developed for cold start. Deep learning has recently achieved remarkable success showing excellent results for diverse input types. Inspired by these results we propose a neural network based latent model called DropoutNet to address the cold start problem in recommender systems. Unlike existing approaches that incorporate additional content-based objective terms, we instead focus on the optimization and show that neural network models can be explicitly trained for cold start through dropout. Our model can be applied on top of any existing latent model effectively providing cold start capabilities, and full power of deep architectures. Empirically we demonstrate state-of-the-art accuracy on publicly available benchmarks. Code is available at https://github.com/layer6ai-labs/DropoutNet.},
	language = {en},
	author = {Volkovs, Maksims and Yu, Guangwei and Poutanen, Tomi},
	file = {Volkovs et al. - DropoutNet Addressing Cold Start in Recommender S.pdf:/Users/yisu/Zotero/storage/BPZ3SFF8/Volkovs et al. - DropoutNet Addressing Cold Start in Recommender S.pdf:application/pdf},
}

@inproceedings{zhu_learning_2021,
	title = {Learning to {Expand} {Audience} via {Meta} {Hybrid} {Experts} and {Critics} for {Recommendation} and {Advertising}},
	url = {http://arxiv.org/abs/2105.14688},
	doi = {10.1145/3447548.3467093},
	abstract = {In recommender systems and advertising platforms, marketers always want to deliver products, contents, or advertisements to potential audiences over media channels such as display, video, or social. Given a set of audiences or customers (seed users), the audience expansion technique (look-alike modeling) is a promising solution to identify more potential audiences, who are similar to the seed users and likely to finish the business goal of the target campaign. However, look-alike modeling faces two challenges: (1) In practice, a company could run hundreds of marketing campaigns to promote various contents within completely different categories every day, e.g., sports, politics, society. Thus, it is difficult to utilize a common method to expand audiences for all campaigns. (2) The seed set of a certain campaign could only cover limited users. Therefore, a customized approach based on such a seed set is likely to be overfitting. In this paper, to address these challenges, we propose a novel two-stage framework named Meta Hybrid Experts and Critics (MetaHeac) which has been deployed in WeChat Look-alike System. In the offline stage, a general model which can capture the relationships among various tasks is trained from a meta-learning perspective on all existing campaign tasks. In the online stage, for a new campaign, a customized model is learned with the given seed set based on the general model. According to both offline and online experiments, the proposed MetaHeac shows superior effectiveness for both content marketing campaigns in recommender systems and advertising campaigns in advertising platforms. Besides, MetaHeac has been successfully deployed in WeChat for the promotion of both contents and advertisements, leading to great improvement in the quality of marketing. The code has been available at {\textbackslash}url\{https://github.com/easezyc/MetaHeac\}.},
	urldate = {2023-03-28},
	booktitle = {Proceedings of the 27th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	author = {Zhu, Yongchun and Liu, Yudan and Xie, Ruobing and Zhuang, Fuzhen and Hao, Xiaobo and Ge, Kaikai and Zhang, Xu and Lin, Leyu and Cao, Juan},
	month = aug,
	year = {2021},
	note = {arXiv:2105.14688 [cs]},
	keywords = {Computer Science - Information Retrieval},
	pages = {4005--4013},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/L7M8542G/Zhu et al. - 2021 - Learning to Expand Audience via Meta Hybrid Expert.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/YKCZI2DB/2105.html:text/html},
}

@inproceedings{aghajanyan_muppet_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Muppet: {Massive} {Multi}-task {Representations} with {Pre}-{Finetuning}},
	shorttitle = {Muppet},
	url = {https://aclanthology.org/2021.emnlp-main.468},
	doi = {10.18653/v1/2021.emnlp-main.468},
	abstract = {We propose pre-finetuning, an additional large-scale learning stage between language model pre-training and fine-tuning. Pre-finetuning is massively multi-task learning (around 50 datasets, over 4.8 million total labeled examples), and is designed to encourage learning of representations that generalize better to many different tasks. We show that pre-finetuning consistently improves performance for pretrained discriminators (e.g. RoBERTa) and generation models (e.g. BART) on a wide range of tasks (sentence prediction, commonsense reasoning, MRC, etc.), while also significantly improving sample efficiency during fine-tuning. We also show that large-scale multi-tasking is crucial; pre-finetuning can hurt performance when few tasks are used up until a critical point (usually above 15) after which performance improves linearly in the number of tasks.},
	urldate = {2023-03-28},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Aghajanyan, Armen and Gupta, Anchit and Shrivastava, Akshat and Chen, Xilun and Zettlemoyer, Luke and Gupta, Sonal},
	month = nov,
	year = {2021},
	pages = {5799--5811},
	file = {Full Text PDF:/Users/yisu/Zotero/storage/7P222WAB/Aghajanyan et al. - 2021 - Muppet Massive Multi-task Representations with Pr.pdf:application/pdf},
}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2023-04-02},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/LCAKN92L/OpenAI - 2023 - GPT-4 Technical Report.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/VGRVZFQT/2303.html:text/html},
}

@inproceedings{ma_modeling_2018,
	address = {London United Kingdom},
	title = {Modeling {Task} {Relationships} in {Multi}-task {Learning} with {Multi}-gate {Mixture}-of-{Experts}},
	isbn = {978-1-4503-5552-0},
	url = {https://dl.acm.org/doi/10.1145/3219819.3220007},
	doi = {10.1145/3219819.3220007},
	language = {en},
	urldate = {2023-04-03},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Ma, Jiaqi and Zhao, Zhe and Yi, Xinyang and Chen, Jilin and Hong, Lichan and Chi, Ed H.},
	month = jul,
	year = {2018},
	pages = {1930--1939},
	file = {Full Text:/Users/yisu/Zotero/storage/4RGRVCIK/Ma et al. - 2018 - Modeling Task Relationships in Multi-task Learning.pdf:application/pdf},
}

@article{shazeer_outrageously_2017,
	title = {{OUTRAGEOUSLY} {LARGE} {NEURAL} {NETWORKS}: {THE} {SPARSELY}-{GATED} {MIXTURE}-{OF}-{EXPERTS} {LAYER}},
	abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are signiﬁcant algorithmic and performance challenges. In this work, we address these challenges and ﬁnally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efﬁciency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve signiﬁcantly better results than state-of-the-art at lower computational cost.},
	language = {en},
	author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Dean, Jeff},
	year = {2017},
	file = {Shazeer et al. - 2017 - OUTRAGEOUSLY LARGE NEURAL NETWORKS THE SPARSELY-G.pdf:/Users/yisu/Zotero/storage/DPVXG7BB/Shazeer et al. - 2017 - OUTRAGEOUSLY LARGE NEURAL NETWORKS THE SPARSELY-G.pdf:application/pdf},
}

@misc{sennrich_neural_2016,
	title = {Neural {Machine} {Translation} of {Rare} {Words} with {Subword} {Units}},
	url = {http://arxiv.org/abs/1508.07909},
	abstract = {Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	month = jun,
	year = {2016},
	note = {arXiv:1508.07909 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/DYMH422C/Sennrich et al. - 2016 - Neural Machine Translation of Rare Words with Subw.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/6BZWNDVI/1508.html:text/html},
}

@article{hu_squeeze-and-excitation_nodate,
	title = {Squeeze-and-{Excitation} {Networks}},
	abstract = {Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive ﬁelds. In order to boost the representational power of a network, several recent approaches have shown the beneﬁt of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the “Squeezeand-Excitation” (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we ﬁnd that SE blocks produce signiﬁcant performance improvements for existing state-ofthe-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classiﬁcation submission which won ﬁrst place and signiﬁcantly reduced the top-5 error to 2.251\%, achieving a ∼25\% relative improvement over the winning entry of 2016. Code and models are available at https: //github.com/hujie-frank/SENet.},
	language = {en},
	author = {Hu, Jie and Shen, Li and Sun, Gang},
	file = {Hu et al. - Squeeze-and-Excitation Networks.pdf:/Users/yisu/Zotero/storage/F548AJ24/Hu et al. - Squeeze-and-Excitation Networks.pdf:application/pdf},
}

@inproceedings{huang_fibinet_2019,
	title = {{FiBiNET}: {Combining} {Feature} {Importance} and {Bilinear} feature {Interaction} for {Click}-{Through} {Rate} {Prediction}},
	shorttitle = {{FiBiNET}},
	url = {http://arxiv.org/abs/1905.09433},
	doi = {10.1145/3298689.3347043},
	abstract = {Advertising and feed ranking are essential to many Internet companies such as Facebook and Sina Weibo. Among many real-world advertising and feed ranking systems, click through rate (CTR) prediction plays a central role. There are many proposed models in this field such as logistic regression, tree based models, factorization machine based models and deep learning based CTR models. However, many current works calculate the feature interactions in a simple way such as Hadamard product and inner product and they care less about the importance of features. In this paper, a new model named FiBiNET as an abbreviation for Feature Importance and Bilinear feature Interaction NETwork is proposed to dynamically learn the feature importance and fine-grained feature interactions. On the one hand, the FiBiNET can dynamically learn the importance of features via the Squeeze-Excitation network (SENET) mechanism; on the other hand, it is able to effectively learn the feature interactions via bilinear function. We conduct extensive experiments on two real-world datasets and show that our shallow model outperforms other shallow models such as factorization machine(FM) and field-aware factorization machine(FFM). In order to improve performance further, we combine a classical deep neural network(DNN) component with the shallow model to be a deep model. The deep FiBiNET consistently outperforms the other state-of-the-art deep models such as DeepFM and extreme deep factorization machine(XdeepFM).},
	urldate = {2023-04-03},
	booktitle = {Proceedings of the 13th {ACM} {Conference} on {Recommender} {Systems}},
	author = {Huang, Tongwen and Zhang, Zhiqi and Zhang, Junlin},
	month = sep,
	year = {2019},
	note = {arXiv:1905.09433 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {169--177},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/MS6DUVRE/Huang et al. - 2019 - FiBiNET Combining Feature Importance and Bilinear.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/LP882CLG/1905.html:text/html},
}

@misc{bahdanau_neural_2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2023-04-03},
	publisher = {arXiv},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv:1409.0473 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/U9JI5859/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/2M6V5W6K/1409.html:text/html},
}

@misc{hendrycks_gaussian_2020,
	title = {Gaussian {Error} {Linear} {Units} ({GELUs})},
	url = {http://arxiv.org/abs/1606.08415},
	doi = {10.48550/arXiv.1606.08415},
	abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is \$x{\textbackslash}Phi(x)\$, where \${\textbackslash}Phi(x)\$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (\$x{\textbackslash}mathbf\{1\}\_\{x{\textgreater}0\}\$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
	urldate = {2023-04-04},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Gimpel, Kevin},
	month = jul,
	year = {2020},
	note = {arXiv:1606.08415 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/QHPWWM9X/Hendrycks and Gimpel - 2020 - Gaussian Error Linear Units (GELUs).pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/WMXHZ5F9/1606.html:text/html},
}

@misc{noauthor_feed_nodate,
	title = {Feed {\textbar} {LinkedIn}},
	url = {https://www.linkedin.com/feed/},
	urldate = {2023-04-06},
	file = {Feed | LinkedIn:/Users/yisu/Zotero/storage/TM8JTTQS/feed.html:text/html},
}

@article{scarselli_graph_2009,
	title = {The {Graph} {Neural} {Network} {Model}},
	volume = {20},
	issn = {1045-9227, 1941-0093},
	url = {http://ieeexplore.ieee.org/document/4700287/},
	doi = {10.1109/TNN.2008.2005605},
	abstract = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.},
	language = {en},
	number = {1},
	urldate = {2023-04-07},
	journal = {IEEE Transactions on Neural Networks},
	author = {Scarselli, F. and Gori, M. and {Ah Chung Tsoi} and Hagenbuchner, M. and Monfardini, G.},
	month = jan,
	year = {2009},
	pages = {61--80},
	file = {Scarselli et al. - 2009 - The Graph Neural Network Model.pdf:/Users/yisu/Zotero/storage/ZL6LQZUX/Scarselli et al. - 2009 - The Graph Neural Network Model.pdf:application/pdf},
}

@misc{kipf_semi-supervised_2017,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = feb,
	year = {2017},
	note = {arXiv:1609.02907 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/PTXD4NMZ/Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/RLXFG6RP/1609.html:text/html},
}

@misc{gilmer_neural_2017,
	title = {Neural {Message} {Passing} for {Quantum} {Chemistry}},
	url = {http://arxiv.org/abs/1704.01212},
	abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
	month = jun,
	year = {2017},
	note = {arXiv:1704.01212 [cs]},
	keywords = {Computer Science - Machine Learning, I.2.6},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/8NMRGDJ4/Gilmer et al. - 2017 - Neural Message Passing for Quantum Chemistry.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/UT6AI5B3/1704.html:text/html},
}

@misc{hamilton_inductive_2018,
	title = {Inductive {Representation} {Learning} on {Large} {Graphs}},
	url = {http://arxiv.org/abs/1706.02216},
	abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	month = sep,
	year = {2018},
	note = {arXiv:1706.02216 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Social and Information Networks},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/GWK6WIRU/Hamilton et al. - 2018 - Inductive Representation Learning on Large Graphs.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/6V5CPN9P/1706.html:text/html},
}

@misc{velickovic_graph_2018,
	title = {Graph {Attention} {Networks}},
	url = {http://arxiv.org/abs/1710.10903},
	abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
	month = feb,
	year = {2018},
	note = {arXiv:1710.10903 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Social and Information Networks},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/VXZYKPBE/Veličković et al. - 2018 - Graph Attention Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/XTYJEK6G/1710.html:text/html},
}

@misc{pancha_pinnerformer_2022,
	title = {{PinnerFormer}: {Sequence} {Modeling} for {User} {Representation} at {Pinterest}},
	shorttitle = {{PinnerFormer}},
	url = {http://arxiv.org/abs/2205.04507},
	abstract = {Sequential models have become increasingly popular in powering personalized recommendation systems over the past several years. These approaches traditionally model a user's actions on a website as a sequence to predict the user's next action. While theoretically simplistic, these models are quite challenging to deploy in production, commonly requiring streaming infrastructure to reflect the latest user activity and potentially managing mutable data for encoding a user's hidden state. Here we introduce PinnerFormer, a user representation trained to predict a user's future long-term engagement using a sequential model of a user's recent actions. Unlike prior approaches, we adapt our modeling to a batch infrastructure via our new dense all-action loss, modeling long-term future actions instead of next action prediction. We show that by doing so, we significantly close the gap between batch user embeddings that are generated once a day and realtime user embeddings generated whenever a user takes an action. We describe our design decisions via extensive offline experimentation and ablations and validate the efficacy of our approach in A/B experiments showing substantial improvements in Pinterest's user retention and engagement when comparing PinnerFormer against our previous user representation. PinnerFormer is deployed in production as of Fall 2021.},
	urldate = {2023-04-07},
	publisher = {arXiv},
	author = {Pancha, Nikil and Zhai, Andrew and Leskovec, Jure and Rosenberg, Charles},
	month = may,
	year = {2022},
	note = {arXiv:2205.04507 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/JVLCJVEJ/Pancha et al. - 2022 - PinnerFormer Sequence Modeling for User Represent.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/HQR3TMI5/2205.html:text/html},
}

@misc{hidasi_session-based_2016,
	title = {Session-based {Recommendations} with {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.06939},
	abstract = {We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.},
	urldate = {2023-04-08},
	publisher = {arXiv},
	author = {Hidasi, Balázs and Karatzoglou, Alexandros and Baltrunas, Linas and Tikk, Domonkos},
	month = mar,
	year = {2016},
	note = {arXiv:1511.06939 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/RR7JMAY5/Hidasi et al. - 2016 - Session-based Recommendations with Recurrent Neura.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/FLGPJYE9/1511.html:text/html},
}

@inproceedings{yang_mixed_2020,
	address = {Taipei Taiwan},
	title = {Mixed {Negative} {Sampling} for {Learning} {Two}-tower {Neural} {Networks} in {Recommendations}},
	isbn = {978-1-4503-7024-0},
	url = {https://dl.acm.org/doi/10.1145/3366424.3386195},
	doi = {10.1145/3366424.3386195},
	abstract = {Learning query and item representations is important for building large scale recommendation systems. In many real applications where there is a huge catalog of items to recommend, the problem of efficiently retrieving top k items given user’s query from deep corpus leads to a family of factorized modeling approaches where queries and items are jointly embedded into a low-dimensional space. In this paper, we first showcase how to apply a two-tower neural network framework, which is also known as dual encoder in the natural language community, to improve a large-scale, production app recommendation system. Furthermore, we offer a novel negative sampling approach called Mixed Negative Sampling (MNS). In particular, different from commonly used batch or unigram sampling methods, MNS uses a mixture of batch and uniformly sampled negatives to tackle the selection bias of implicit user feedback. We conduct extensive offline experiments using large-scale production dataset and show that MNS outperforms other baseline sampling methods. We also conduct online A/B testing and demonstrate that the two-tower retrieval model based on MNS significantly improves retrieval quality by encouraging more high-quality app installs.},
	language = {en},
	urldate = {2023-04-08},
	booktitle = {Companion {Proceedings} of the {Web} {Conference} 2020},
	publisher = {ACM},
	author = {Yang, Ji and Yi, Xinyang and Zhiyuan Cheng, Derek and Hong, Lichan and Li, Yang and Xiaoming Wang, Simon and Xu, Taibai and Chi, Ed H.},
	month = apr,
	year = {2020},
	pages = {441--447},
	file = {Yang et al. - 2020 - Mixed Negative Sampling for Learning Two-tower Neu.pdf:/Users/yisu/Zotero/storage/TFBCUTR6/Yang et al. - 2020 - Mixed Negative Sampling for Learning Two-tower Neu.pdf:application/pdf},
}

@inproceedings{pal_pinnersage_2020,
	address = {Virtual Event CA USA},
	title = {{PinnerSage}: {Multi}-{Modal} {User} {Embedding} {Framework} for {Recommendations} at {Pinterest}},
	isbn = {978-1-4503-7998-4},
	shorttitle = {{PinnerSage}},
	url = {https://dl.acm.org/doi/10.1145/3394486.3403280},
	doi = {10.1145/3394486.3403280},
	language = {en},
	urldate = {2023-04-09},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Pal, Aditya and Eksombatchai, Chantat and Zhou, Yitong and Zhao, Bo and Rosenberg, Charles and Leskovec, Jure},
	month = aug,
	year = {2020},
	pages = {2311--2320},
	file = {Submitted Version:/Users/yisu/Zotero/storage/EQJDV9GX/Pal et al. - 2020 - PinnerSage Multi-Modal User Embedding Framework f.pdf:application/pdf},
}

@misc{wu_starspace_2017,
	title = {{StarSpace}: {Embed} {All} {The} {Things}!},
	shorttitle = {{StarSpace}},
	url = {http://arxiv.org/abs/1709.03856},
	abstract = {We present StarSpace, a general-purpose neural embedding model that can solve a wide variety of problems: labeling tasks such as text classification, ranking tasks such as information retrieval/web search, collaborative filtering-based or content-based recommendation, embedding of multi-relational graphs, and learning word, sentence or document level embeddings. In each case the model works by embedding those entities comprised of discrete features and comparing them against each other -- learning similarities dependent on the task. Empirical results on a number of tasks show that StarSpace is highly competitive with existing methods, whilst also being generally applicable to new cases where those methods are not.},
	urldate = {2023-04-09},
	publisher = {arXiv},
	author = {Wu, Ledell and Fisch, Adam and Chopra, Sumit and Adams, Keith and Bordes, Antoine and Weston, Jason},
	month = nov,
	year = {2017},
	note = {arXiv:1709.03856 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/8GJZ2RAR/Wu et al. - 2017 - StarSpace Embed All The Things!.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/IHR9MIUQ/1709.html:text/html},
}

@misc{zhao_learning_2018,
	title = {Learning {Item}-{Interaction} {Embeddings} for {User} {Recommendations}},
	url = {http://arxiv.org/abs/1812.04407},
	abstract = {Industry-scale recommendation systems have become a cornerstone of the e-commerce shopping experience. For Etsy, an online marketplace with over 50 million handmade and vintage items, users come to rely on personalized recommendations to surface relevant items from its massive inventory. One hallmark of Etsy's shopping experience is the multitude of ways in which a user can interact with an item they are interested in: they can view it, favorite it, add it to a collection, add it to cart, purchase it, etc. We hypothesize that the different ways in which a user interacts with an item indicates different kinds of intent. Consequently, a user's recommendations should be based not only on the item from their past activity, but also the way in which they interacted with that item. In this paper, we propose a novel method for learning interaction-based item embeddings that encode the co-occurrence patterns of not only the item itself, but also the interaction type. The learned embeddings give us a convenient way of approximating the likelihood that one item-interaction pair would co-occur with another by way of a simple inner product. Because of its computational efficiency, our model lends itself naturally as a candidate set selection method, and we evaluate it as such in an industry-scale recommendation system that serves live traffic on Etsy.com. Our experiments reveal that taking interaction type into account shows promising results in improving the accuracy of modeling user shopping behavior.},
	urldate = {2023-04-09},
	publisher = {arXiv},
	author = {Zhao, Xiaoting and Louca, Raphael and Hu, Diane and Hong, Liangjie},
	month = dec,
	year = {2018},
	note = {arXiv:1812.04407 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/VW6QJTLU/Zhao et al. - 2018 - Learning Item-Interaction Embeddings for User Reco.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/WZ48DQC9/1812.html:text/html},
}

@inproceedings{ying_graph_2018,
	title = {Graph {Convolutional} {Neural} {Networks} for {Web}-{Scale} {Recommender} {Systems}},
	url = {http://arxiv.org/abs/1806.01973},
	doi = {10.1145/3219819.3219890},
	abstract = {Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. We deploy PinSage at Pinterest and train it on 7.5 billion examples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.},
	urldate = {2023-04-11},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	author = {Ying, Rex and He, Ruining and Chen, Kaifeng and Eksombatchai, Pong and Hamilton, William L. and Leskovec, Jure},
	month = jul,
	year = {2018},
	note = {arXiv:1806.01973 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval},
	pages = {974--983},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/U8U8CU7M/Ying et al. - 2018 - Graph Convolutional Neural Networks for Web-Scale .pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/G7PJ85WN/1806.html:text/html},
}

@inproceedings{eksombatchai_pixie_2018,
	address = {Lyon, France},
	title = {Pixie: {A} {System} for {Recommending} 3+ {Billion} {Items} to 200+ {Million} {Users} in {Real}-{Time}},
	isbn = {978-1-4503-5639-8},
	shorttitle = {Pixie},
	url = {http://dl.acm.org/citation.cfm?doid=3178876.3186183},
	doi = {10.1145/3178876.3186183},
	abstract = {User experience in modern content discovery applications critically depends on high-quality personalized recommendations. However, building systems that provide such recommendations presents a major challenge due to a massive pool of items, a large number of users, and requirements for recommendations to be responsive to user actions and generated on demand in real-time. Here we present Pixie, a scalable graph-based real-time recommender system that we developed and deployed at Pinterest. Given a set of user-specific pins as a query, Pixie selects in real-time from billions of possible pins those that are most related to the query. To generate recommendations, we develop Pixie Random Walk algorithm that utilizes the Pinterest object graph of 3 billion nodes and 17 billion edges. Experiments show that recommendations provided by Pixie lead up to 50\% higher user engagement when compared to the previous Hadoop-based production system. Furthermore, we develop a graph pruning strategy at that leads to an additional 58\% improvement in recommendations. Last, we discuss system aspects of Pixie, where a single server executes 1,200 recommendation requests per second with 60 millisecond latency. Today, systems backed by Pixie contribute to more than 80\% of all user engagement on Pinterest.},
	language = {en},
	urldate = {2023-04-15},
	booktitle = {Proceedings of the 2018 {World} {Wide} {Web} {Conference} on {World} {Wide} {Web} - {WWW} '18},
	publisher = {ACM Press},
	author = {Eksombatchai, Chantat and Jindal, Pranav and Liu, Jerry Zitao and Liu, Yuchen and Sharma, Rahul and Sugnet, Charles and Ulrich, Mark and Leskovec, Jure},
	year = {2018},
	pages = {1775--1784},
	file = {Eksombatchai et al. - 2018 - Pixie A System for Recommending 3+ Billion Items .pdf:/Users/yisu/Zotero/storage/96VFS8C2/Eksombatchai et al. - 2018 - Pixie A System for Recommending 3+ Billion Items .pdf:application/pdf},
}

@misc{barkan_item2vec_2017,
	title = {{Item2Vec}: {Neural} {Item} {Embedding} for {Collaborative} {Filtering}},
	shorttitle = {{Item2Vec}},
	url = {http://arxiv.org/abs/1603.04259},
	abstract = {Many Collaborative Filtering (CF) algorithms are item-based in the sense that they analyze item-item relations in order to produce item similarities. Recently, several works in the field of Natural Language Processing (NLP) suggested to learn a latent representation of words using neural embedding algorithms. Among them, the Skip-gram with Negative Sampling (SGNS), also known as word2vec, was shown to provide state-of-the-art results on various linguistics tasks. In this paper, we show that item-based CF can be cast in the same framework of neural word embedding. Inspired by SGNS, we describe a method we name item2vec for item-based CF that produces embedding for items in a latent space. The method is capable of inferring item-item relations even when user information is not available. We present experimental results that demonstrate the effectiveness of the item2vec method and show it is competitive with SVD.},
	urldate = {2023-04-15},
	publisher = {arXiv},
	author = {Barkan, Oren and Koenigstein, Noam},
	month = feb,
	year = {2017},
	note = {arXiv:1603.04259 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/PCGJ9U27/Barkan and Koenigstein - 2017 - Item2Vec Neural Item Embedding for Collaborative .pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/KWLCF3KL/1603.html:text/html},
}

@article{zhang_deep_2020,
	title = {Deep {Learning} based {Recommender} {System}: {A} {Survey} and {New} {Perspectives}},
	volume = {52},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Deep {Learning} based {Recommender} {System}},
	url = {http://arxiv.org/abs/1707.07435},
	doi = {10.1145/3285029},
	abstract = {With the ever-growing volume of online information, recommender systems have been an effective strategy to overcome such information overload. The utility of recommender systems cannot be overstated, given its widespread adoption in many web applications, along with its potential impact to ameliorate many problems related to over-choice. In recent years, deep learning has garnered considerable interest in many research fields such as computer vision and natural language processing, owing not only to stellar performance but also the attractive property of learning feature representations from scratch. The influence of deep learning is also pervasive, recently demonstrating its effectiveness when applied to information retrieval and recommender systems research. Evidently, the field of deep learning in recommender system is flourishing. This article aims to provide a comprehensive review of recent research efforts on deep learning based recommender systems. More concretely, we provide and devise a taxonomy of deep learning based recommendation models, along with providing a comprehensive summary of the state-of-the-art. Finally, we expand on current trends and provide new perspectives pertaining to this new exciting development of the field.},
	number = {1},
	urldate = {2023-04-15},
	journal = {ACM Computing Surveys},
	author = {Zhang, Shuai and Yao, Lina and Sun, Aixin and Tay, Yi},
	month = jan,
	year = {2020},
	note = {arXiv:1707.07435 [cs]},
	keywords = {Computer Science - Information Retrieval},
	pages = {1--38},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/E9QCKBPT/Zhang et al. - 2020 - Deep Learning based Recommender System A Survey a.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/BE93DZXC/1707.html:text/html},
}

@article{li_feature_2018,
	title = {Feature {Selection}: {A} {Data} {Perspective}},
	volume = {50},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Feature {Selection}},
	url = {https://dl.acm.org/doi/10.1145/3136625},
	doi = {10.1145/3136625},
	abstract = {Feature selection, as a data preprocessing strategy, has been proven to be effective and efficient in preparing data (especially high-dimensional data) for various data-mining and machine-learning problems. The objectives of feature selection include building simpler and more comprehensible models, improving data-mining performance, and preparing clean, understandable data. The recent proliferation of big data has presented some substantial challenges and opportunities to feature selection. In this survey, we provide a comprehensive and structured overview of recent advances in feature selection research. Motivated by current challenges and opportunities in the era of big data, we revisit feature selection research from a data perspective and review representative feature selection algorithms for conventional data, structured data, heterogeneous data and streaming data. Methodologically, to emphasize the differences and similarities of most existing feature selection algorithms for conventional data, we categorize them into four main groups: similarity-based, information-theoretical-based, sparse-learning-based, and statistical-based methods. To facilitate and promote the research in this community, we also present an open source feature selection repository that consists of most of the popular feature selection algorithms (http://featureselection.asu.edu/). Also, we use it as an example to show how to evaluate feature selection algorithms. At the end of the survey, we present a discussion about some open problems and challenges that require more attention in future research.},
	language = {en},
	number = {6},
	urldate = {2023-04-17},
	journal = {ACM Computing Surveys},
	author = {Li, Jundong and Cheng, Kewei and Wang, Suhang and Morstatter, Fred and Trevino, Robert P. and Tang, Jiliang and Liu, Huan},
	month = nov,
	year = {2018},
	pages = {1--45},
	file = {Submitted Version:/Users/yisu/Zotero/storage/CTRWINSM/Li et al. - 2018 - Feature Selection A Data Perspective.pdf:application/pdf},
}

@article{perlich_machine_2014,
	title = {Machine learning for targeted display advertising: transfer learning in action},
	volume = {95},
	issn = {0885-6125, 1573-0565},
	shorttitle = {Machine learning for targeted display advertising},
	url = {http://link.springer.com/10.1007/s10994-013-5375-2},
	doi = {10.1007/s10994-013-5375-2},
	abstract = {This paper presents a detailed discussion of problem formulation and data representation issues in the design, deployment, and operation of a massive-scale machine learning system for targeted display advertising. Notably, the machine learning system itself is deployed and has been in continual use for years, for thousands of advertising campaigns (in contrast to simply having the models from the system be deployed). In this application, acquiring sufﬁcient data for training from the ideal sampling distribution is prohibitively expensive. Instead, data are drawn from surrogate domains and learning tasks, and then transferred to the target task. We present the design of this multistage transfer learning system, highlighting the problem formulation aspects. We then present a detailed experimental evaluation, showing that the different transfer stages indeed each add value. We next present production results across a variety of advertising clients from a variety of industries, illustrating the performance of the system in use. We close the paper with a collection of lessons learned from the work over half a decade on this complex, deployed, and broadly used machine learning system.},
	language = {en},
	number = {1},
	urldate = {2023-04-19},
	journal = {Machine Learning},
	author = {Perlich, C. and Dalessandro, B. and Raeder, T. and Stitelman, O. and Provost, F.},
	month = apr,
	year = {2014},
	pages = {103--127},
	file = {Perlich et al. - 2014 - Machine learning for targeted display advertising.pdf:/Users/yisu/Zotero/storage/KP7AMEJU/Perlich et al. - 2014 - Machine learning for targeted display advertising.pdf:application/pdf},
}

@misc{mahajan_pie_2023,
	title = {{PIE}: {Personalized} {Interest} {Exploration} for {Large}-{Scale} {Recommender} {Systems}},
	shorttitle = {{PIE}},
	url = {http://arxiv.org/abs/2304.06844},
	doi = {10.1145/3543873.3584656},
	abstract = {Recommender systems are increasingly successful in recommending personalized content to users. However, these systems often capitalize on popular content. There is also a continuous evolution of user interests that need to be captured, but there is no direct way to systematically explore users' interests. This also tends to affect the overall quality of the recommendation pipeline as training data is generated from the candidates presented to the user. In this paper, we present a framework for exploration in large-scale recommender systems to address these challenges. It consists of three parts, first the user-creator exploration which focuses on identifying the best creators that users are interested in, second the online exploration framework and third a feed composition mechanism that balances explore and exploit to ensure optimal prevalence of exploratory videos. Our methodology can be easily integrated into an existing large-scale recommender system with minimal modifications. We also analyze the value of exploration by defining relevant metrics around user-creator connections and understanding how this helps the overall recommendation pipeline with strong online gains in creator and ecosystem value. In contrast to the regression on user engagement metrics generally seen while exploring, our method is able to achieve significant improvements of 3.50\% in strong creator connections and 0.85\% increase in novel creator connections. Moreover, our work has been deployed in production on Facebook Watch, a popular video discovery and sharing platform serving billions of users.},
	urldate = {2023-04-18},
	author = {Mahajan, Khushhall Chandra and Dharwadker, Amey Porobo and Shah, Romil and Qu, Simeng and Bang, Gaurav and Schumitsch, Brad},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06844 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/DP3KRUKI/Mahajan et al. - 2023 - PIE Personalized Interest Exploration for Large-S.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/U867RJ4D/2304.html:text/html},
}

@article{ross_mutual_2014,
	title = {Mutual {Information} between {Discrete} and {Continuous} {Data} {Sets}},
	volume = {9},
	issn = {1932-6203},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3929353/},
	doi = {10.1371/journal.pone.0087357},
	abstract = {Mutual information (MI) is a powerful method for detecting relationships between data sets. There are accurate methods for estimating MI that avoid problems with “binning” when both data sets are discrete or when both data sets are continuous. We present an accurate, non-binning MI estimator for the case of one discrete data set and one continuous data set. This case applies when measuring, for example, the relationship between base sequence and gene expression level, or the effect of a cancer drug on patient survival time. We also show how our method can be adapted to calculate the Jensen–Shannon divergence of two or more data sets.},
	number = {2},
	urldate = {2023-04-20},
	journal = {PLoS ONE},
	author = {Ross, Brian C.},
	month = feb,
	year = {2014},
	pmid = {24586270},
	pmcid = {PMC3929353},
	pages = {e87357},
	file = {Full Text PDF:/Users/yisu/Zotero/storage/N3ABINBI/Ross - 2014 - Mutual Information between Discrete and Continuous.pdf:application/pdf},
}

@misc{covert_understanding_2020,
	title = {Understanding {Global} {Feature} {Contributions} {With} {Additive} {Importance} {Measures}},
	url = {http://arxiv.org/abs/2004.00668},
	abstract = {Understanding the inner workings of complex machine learning models is a long-standing problem and most recent research has focused on local interpretability. To assess the role of individual input features in a global sense, we explore the perspective of defining feature importance through the predictive power associated with each feature. We introduce two notions of predictive power (model-based and universal) and formalize this approach with a framework of additive importance measures, which unifies numerous methods in the literature. We then propose SAGE, a model-agnostic method that quantifies predictive power while accounting for feature interactions. Our experiments show that SAGE can be calculated efficiently and that it assigns more accurate importance values than other methods.},
	urldate = {2023-04-20},
	publisher = {arXiv},
	author = {Covert, Ian and Lundberg, Scott and Lee, Su-In},
	month = oct,
	year = {2020},
	note = {arXiv:2004.00668 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/EUW5MNV4/2004.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/FKE59656/Covert et al. - 2020 - Understanding Global Feature Contributions With Ad.pdf:application/pdf},
}

@misc{reimers_sentence-bert_2019,
	title = {Sentence-{BERT}: {Sentence} {Embeddings} using {Siamese} {BERT}-{Networks}},
	shorttitle = {Sentence-{BERT}},
	url = {http://arxiv.org/abs/1908.10084},
	abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
	urldate = {2023-04-21},
	publisher = {arXiv},
	author = {Reimers, Nils and Gurevych, Iryna},
	month = aug,
	year = {2019},
	note = {arXiv:1908.10084 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/CBEDQVAG/1908.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/Q94PHNEJ/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf:application/pdf},
}

@misc{karpukhin_dense_2020,
	title = {Dense {Passage} {Retrieval} for {Open}-{Domain} {Question} {Answering}},
	url = {http://arxiv.org/abs/2004.04906},
	abstract = {Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9\%-19\% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.},
	urldate = {2023-04-23},
	publisher = {arXiv},
	author = {Karpukhin, Vladimir and Oğuz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
	month = sep,
	year = {2020},
	note = {arXiv:2004.04906 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/PZCDBJ2K/2004.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/IJDX62QH/Karpukhin et al. - 2020 - Dense Passage Retrieval for Open-Domain Question A.pdf:application/pdf},
}

@misc{shoeybi_megatron-lm_2020,
	title = {Megatron-{LM}: {Training} {Multi}-{Billion} {Parameter} {Language} {Models} {Using} {Model} {Parallelism}},
	shorttitle = {Megatron-{LM}},
	url = {http://arxiv.org/abs/1909.08053},
	abstract = {Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76\% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30\% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5\% compared to SOTA accuracy of 63.2\%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9\% compared to SOTA accuracy of 89.4\%).},
	urldate = {2023-04-23},
	publisher = {arXiv},
	author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
	month = mar,
	year = {2020},
	note = {arXiv:1909.08053 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/B4UD6JDQ/1909.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/YKHXZN7Z/Shoeybi et al. - 2020 - Megatron-LM Training Multi-Billion Parameter Lang.pdf:application/pdf},
}

@misc{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv:1907.11692 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/TTCBEF2I/1907.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/NAM74JMK/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf:application/pdf},
}

@inproceedings{zhao_jointly_2020,
	title = {Jointly {Learning} to {Recommend} and {Advertise}},
	url = {http://arxiv.org/abs/2003.00097},
	doi = {10.1145/3394486.3403384},
	abstract = {Online recommendation and advertising are two major income channels for online recommendation platforms (e.g. e-commerce and news feed site). However, most platforms optimize recommending and advertising strategies by different teams separately via different techniques, which may lead to suboptimal overall performances. To this end, in this paper, we propose a novel two-level reinforcement learning framework to jointly optimize the recommending and advertising strategies, where the first level generates a list of recommendations to optimize user experience in the long run; then the second level inserts ads into the recommendation list that can balance the immediate advertising revenue from advertisers and the negative influence of ads on long-term user experience. To be specific, the first level tackles high combinatorial action space problem that selects a subset items from the large item space; while the second level determines three internally related tasks, i.e., (i) whether to insert an ad, and if yes, (ii) the optimal ad and (iii) the optimal location to insert. The experimental results based on real-world data demonstrate the effectiveness of the proposed framework. We have released the implementation code to ease reproductivity.},
	urldate = {2023-04-25},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	author = {Zhao, Xiangyu and Zheng, Xudong and Yang, Xiwang and Liu, Xiaobing and Tang, Jiliang},
	month = aug,
	year = {2020},
	note = {arXiv:2003.00097 [cs]},
	keywords = {Computer Science - Information Retrieval},
	pages = {3319--3327},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/GIAAUH5E/2003.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/YHRVLWZG/Zhao et al. - 2020 - Jointly Learning to Recommend and Advertise.pdf:application/pdf},
}

@misc{lewis_incrementality_2022,
	title = {Incrementality {Bidding} and {Attribution}},
	url = {http://arxiv.org/abs/2208.12809},
	abstract = {The causal effect of showing an ad to a potential customer versus not, commonly referred to as "incrementality", is the fundamental question of advertising effectiveness. In digital advertising three major puzzle pieces are central to rigorously quantifying advertising incrementality: ad buying/bidding/pricing, attribution, and experimentation. Building on the foundations of machine learning and causal econometrics, we propose a methodology that unifies these three concepts into a computationally viable model of both bidding and attribution which spans the randomization, training, cross validation, scoring, and conversion attribution of advertising's causal effects. Implementation of this approach is likely to secure a significant improvement in the return on investment of advertising.},
	urldate = {2023-04-25},
	publisher = {arXiv},
	author = {Lewis, Randall and Wong, Jeffrey},
	month = aug,
	year = {2022},
	note = {arXiv:2208.12809 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/2K8GXKC8/2208.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/E76VPWJT/Lewis and Wong - 2022 - Incrementality Bidding and Attribution.pdf:application/pdf},
}

@inproceedings{bhamidipati_large_2017,
	address = {Singapore Singapore},
	title = {A {Large} {Scale} {Prediction} {Engine} for {App} {Install} {Clicks} and {Conversions}},
	isbn = {978-1-4503-4918-5},
	url = {https://dl.acm.org/doi/10.1145/3132847.3132868},
	doi = {10.1145/3132847.3132868},
	language = {en},
	urldate = {2023-04-25},
	booktitle = {Proceedings of the 2017 {ACM} on {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {ACM},
	author = {Bhamidipati, Narayan and Kant, Ravi and Mishra, Shaunak},
	month = nov,
	year = {2017},
	pages = {167--175},
	file = {Full Text PDF:/Users/yisu/Zotero/storage/8J3TS4JE/Bhamidipati et al. - 2017 - A Large Scale Prediction Engine for App Install Cl.pdf:application/pdf},
}

@article{chapelle_simple_2015,
	title = {Simple and {Scalable} {Response} {Prediction} for {Display} {Advertising}},
	volume = {5},
	issn = {2157-6904, 2157-6912},
	url = {https://dl.acm.org/doi/10.1145/2532128},
	doi = {10.1145/2532128},
	abstract = {Clickthrough and conversation rates estimation are two core predictions tasks in display advertising. We present in this article a machine learning framework based on logistic regression that is specifically designed to tackle the specifics of display advertising. The resulting system has the following characteristics: It is easy to implement and deploy, it is highly scalable (we have trained it on terabytes of data), and it provides models with state-of-the-art accuracy.},
	language = {en},
	number = {4},
	urldate = {2023-04-25},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Chapelle, Olivier and Manavoglu, Eren and Rosales, Romer},
	month = jan,
	year = {2015},
	pages = {1--34},
	file = {Chapelle et al. - 2015 - Simple and Scalable Response Prediction for Displa.pdf:/Users/yisu/Zotero/storage/EV2JWD2B/Chapelle et al. - 2015 - Simple and Scalable Response Prediction for Displa.pdf:application/pdf;Full Text PDF:/Users/yisu/Zotero/storage/P6YKPWNW/Chapelle et al. - 2015 - Simple and Scalable Response Prediction for Displa.pdf:application/pdf},
}

@article{choi_online_2020,
	title = {Online {Display} {Advertising} {Markets}: {A} {Literature} {Review} and {Future} {Directions}},
	volume = {31},
	issn = {1047-7047, 1526-5536},
	shorttitle = {Online {Display} {Advertising} {Markets}},
	url = {https://pubsonline.informs.org/doi/10.1287/isre.2019.0902},
	doi = {10.1287/isre.2019.0902},
	abstract = {This paper summarizes the display advertising literature, organizing the content by the agents in the display advertising ecosystem, and proposes new research directions. In doing so, we take an interdisciplinary view, drawing connections among diverse streams of theoretical and empirical research in information systems, marketing, economics, operations, and computer science. By providing an integrated view of the display advertising ecosystem, we hope to bring attention to the outstanding research opportunities in this economically consequential and rapidly growing market.},
	language = {en},
	number = {2},
	urldate = {2023-04-25},
	journal = {Information Systems Research},
	author = {Choi, Hana and Mela, Carl F. and Balseiro, Santiago R. and Leary, Adam},
	month = jun,
	year = {2020},
	pages = {556--575},
	file = {Choi et al. - 2020 - Online Display Advertising Markets A Literature R.pdf:/Users/yisu/Zotero/storage/FTHLCSFM/Choi et al. - 2020 - Online Display Advertising Markets A Literature R.pdf:application/pdf},
}

@misc{chen_top-k_2021,
	title = {Top-{K} {Off}-{Policy} {Correction} for a {REINFORCE} {Recommender} {System}},
	url = {http://arxiv.org/abs/1812.02353},
	abstract = {Industrial recommender systems deal with extremely large action spaces -- many millions of items to recommend. Moreover, they need to serve billions of users, who are unique at any point in time, making a complex user state space. Luckily, huge quantities of logged implicit feedback (e.g., user clicks, dwell time) are available for learning. Learning from the logged feedback is however subject to biases caused by only observing feedback on recommendations selected by the previous versions of the recommender. In this work, we present a general recipe of addressing such biases in a production top-K recommender system at Youtube, built with a policy-gradient-based algorithm, i.e. REINFORCE. The contributions of the paper are: (1) scaling REINFORCE to a production recommender system with an action space on the orders of millions; (2) applying off-policy correction to address data biases in learning from logged feedback collected from multiple behavior policies; (3) proposing a novel top-K off-policy correction to account for our policy recommending multiple items at a time; (4) showcasing the value of exploration. We demonstrate the efficacy of our approaches through a series of simulations and multiple live experiments on Youtube.},
	urldate = {2023-04-27},
	publisher = {arXiv},
	author = {Chen, Minmin and Beutel, Alex and Covington, Paul and Jain, Sagar and Belletti, Francois and Chi, Ed},
	month = dec,
	year = {2021},
	note = {arXiv:1812.02353 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/P8DC9ER7/1812.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/84NH6RJ2/Chen et al. - 2021 - Top-K Off-Policy Correction for a REINFORCE Recomm.pdf:application/pdf},
}

@inproceedings{shen_effective_2015,
	address = {Sydney NSW Australia},
	title = {Effective {Audience} {Extension} in {Online} {Advertising}},
	isbn = {978-1-4503-3664-2},
	url = {https://dl.acm.org/doi/10.1145/2783258.2788603},
	doi = {10.1145/2783258.2788603},
	abstract = {In digital advertising, advertisers want to reach the right audience over media channels such as display, mobile, video, or social at the appropriate cost. The right audience for an advertiser consists of existing customers as well as valuable prospects, those that can potentially be turned into future customers. Identifying valuable prospects is called the audience extension problem because advertisers ﬁnd new customers by extending the desirable criteria for their starting point, which is their existing audience or customers. The complexity of the audience extension problem stems from the diﬃculty of deﬁning desirable criteria objectively, the number of desirable criteria (such as similarity, diversity, performance) to simultaneously satisfy, and the expected runtime (a few minutes) to ﬁnd a solution over billions of cookie-based users. In this paper, we formally deﬁne the audience extension problem, propose an algorithm that extends a given audience set eﬃciently under multiple desirable criteria, and experimentally validate its performance. Instead of iterating over individual users, the algorithm takes in Boolean rules that deﬁne the seed audience and returns a new set of Boolean rules that corresponds to the extended audience that satisfy the multiple criteria.},
	language = {en},
	urldate = {2023-04-29},
	booktitle = {Proceedings of the 21th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Shen, Jianqiang and Geyik, Sahin Cem and Dasdan, Ali},
	month = aug,
	year = {2015},
	pages = {2099--2108},
	file = {Shen et al. - 2015 - Effective Audience Extension in Online Advertising.pdf:/Users/yisu/Zotero/storage/9HT9B36V/Shen et al. - 2015 - Effective Audience Extension in Online Advertising.pdf:application/pdf},
}

@misc{kang_self-attentive_2018,
	title = {Self-{Attentive} {Sequential} {Recommendation}},
	url = {http://arxiv.org/abs/1808.09781},
	abstract = {Sequential dynamics are a key feature of many modern recommender systems, which seek to capture the `context' of users' activities on the basis of actions they have performed recently. To capture such patterns, two approaches have proliferated: Markov Chains (MCs) and Recurrent Neural Networks (RNNs). Markov Chains assume that a user's next action can be predicted on the basis of just their last (or last few) actions, while RNNs in principle allow for longer-term semantics to be uncovered. Generally speaking, MC-based methods perform best in extremely sparse datasets, where model parsimony is critical, while RNNs perform better in denser datasets where higher model complexity is affordable. The goal of our work is to balance these two goals, by proposing a self-attention based sequential model (SASRec) that allows us to capture long-term semantics (like an RNN), but, using an attention mechanism, makes its predictions based on relatively few actions (like an MC). At each time step, SASRec seeks to identify which items are `relevant' from a user's action history, and use them to predict the next item. Extensive empirical studies show that our method outperforms various state-of-the-art sequential models (including MC/CNN/RNN-based approaches) on both sparse and dense datasets. Moreover, the model is an order of magnitude more efficient than comparable CNN/RNN-based models. Visualizations on attention weights also show how our model adaptively handles datasets with various density, and uncovers meaningful patterns in activity sequences.},
	urldate = {2023-04-29},
	publisher = {arXiv},
	author = {Kang, Wang-Cheng and McAuley, Julian},
	month = aug,
	year = {2018},
	note = {arXiv:1808.09781 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Retrieval},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/JULPB5RE/1808.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/QDH56NXB/Kang and McAuley - 2018 - Self-Attentive Sequential Recommendation.pdf:application/pdf},
}

@misc{ouyang_representation_2019,
	title = {Representation {Learning}-{Assisted} {Click}-{Through} {Rate} {Prediction}},
	url = {http://arxiv.org/abs/1906.04365},
	abstract = {Click-through rate (CTR) prediction is a critical task in online advertising systems. Most existing methods mainly model the feature-CTR relationship and suffer from the data sparsity issue. In this paper, we propose DeepMCP, which models other types of relationships in order to learn more informative and statistically reliable feature representations, and in consequence to improve the performance of CTR prediction. In particular, DeepMCP contains three parts: a matching subnet, a correlation subnet and a prediction subnet. These subnets model the user-ad, ad-ad and feature-CTR relationship respectively. When these subnets are jointly optimized under the supervision of the target labels, the learned feature representations have both good prediction powers and good representation abilities. Experiments on two large-scale datasets demonstrate that DeepMCP outperforms several state-of-the-art models for CTR prediction.},
	urldate = {2023-04-29},
	publisher = {arXiv},
	author = {Ouyang, Wentao and Zhang, Xiuwu and Ren, Shukui and Qi, Chao and Liu, Zhaojie and Du, Yanlong},
	month = jul,
	year = {2019},
	note = {arXiv:1906.04365 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/JMHILQNG/1906.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/KF8GM2F5/Ouyang et al. - 2019 - Representation Learning-Assisted Click-Through Rat.pdf:application/pdf},
}

@article{iqbal_production_2019,
	title = {Production {Ranking} {Systems}: {A} {Review}},
	abstract = {The problem of ranking is a multi-billion dollar problem. In this paper we present an overview of several production quality ranking systems. We show that due to conflicting goals of employing the most effective machine learning models and responding to users in real time, ranking systems have evolved into a system of systems, where each subsystem can be viewed as a component layer. We view these layers as being data processing, representation learning, candidate selection and online inference. Each layer employs different algorithms and tools, with every end-to-end ranking system spanning multiple architectures. Our goal is to familiarize the general audience with a working knowledge of ranking at scale, the tools and algorithms employed and the challenges introduced by adopting a layered approach.},
	language = {en},
	author = {Iqbal, Murium and Subedi, Nishan and Aryafar, Kamelia},
	year = {2019},
	file = {Iqbal et al. - 2019 - Production Ranking Systems A Review.pdf:/Users/yisu/Zotero/storage/2VSYQZNU/Iqbal et al. - 2019 - Production Ranking Systems A Review.pdf:application/pdf},
}

@misc{geva_transformer_2021,
	title = {Transformer {Feed}-{Forward} {Layers} {Are} {Key}-{Value} {Memories}},
	url = {http://arxiv.org/abs/2012.14913},
	abstract = {Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
	month = sep,
	year = {2021},
	note = {arXiv:2012.14913 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/YIT8H366/2012.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/PQCUNH9P/Geva et al. - 2021 - Transformer Feed-Forward Layers Are Key-Value Memo.pdf:application/pdf},
}

@misc{yang_harnessing_2023,
	title = {Harnessing the {Power} of {LLMs} in {Practice}: {A} {Survey} on {ChatGPT} and {Beyond}},
	shorttitle = {Harnessing the {Power} of {LLMs} in {Practice}},
	url = {http://arxiv.org/abs/2304.13712},
	abstract = {This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at {\textbackslash}url\{https://github.com/Mooler0410/LLMsPracticalGuide\}.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Xiaotian and Feng, Qizhang and Jiang, Haoming and Yin, Bing and Hu, Xia},
	month = apr,
	year = {2023},
	note = {arXiv:2304.13712 [cs]
version: 2},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/MHEGJHHF/2304.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/TDYL86RX/Yang et al. - 2023 - Harnessing the Power of LLMs in Practice A Survey.pdf:application/pdf},
}

@incollection{goos_simple_2001,
	address = {Berlin, Heidelberg},
	title = {A {Simple} {Approach} to {Ordinal} {Classification}},
	volume = {2167},
	isbn = {978-3-540-42536-6 978-3-540-44795-5},
	url = {http://link.springer.com/10.1007/3-540-44795-4_13},
	abstract = {Machine learning methods for classiﬁcation problems commonly assume that the class values are unordered. However, in many practical applications the class values do exhibit a natural order—for example, when learning how to grade. The standard approach to ordinal classiﬁcation converts the class value into a numeric quantity and applies a regression learner to the transformed data, translating the output back into a discrete class value in a post-processing step. A disadvantage of this method is that it can only be applied in conjunction with a regression scheme.},
	language = {en},
	urldate = {2023-05-03},
	booktitle = {Machine {Learning}: {ECML} 2001},
	publisher = {Springer Berlin Heidelberg},
	author = {Frank, Eibe and Hall, Mark},
	editor = {Goos, G. and Hartmanis, J. and Van Leeuwen, J. and De Raedt, Luc and Flach, Peter},
	year = {2001},
	doi = {10.1007/3-540-44795-4_13},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {145--156},
	file = {Frank and Hall - 2001 - A Simple Approach to Ordinal Classification.pdf:/Users/yisu/Zotero/storage/LMKJTSH8/Frank and Hall - 2001 - A Simple Approach to Ordinal Classification.pdf:application/pdf},
}

@inproceedings{ng_spectral_2001,
	title = {On {Spectral} {Clustering}: {Analysis} and an algorithm},
	volume = {14},
	shorttitle = {On {Spectral} {Clustering}},
	url = {https://papers.nips.cc/paper_files/paper/2001/hash/801272ee79cfde7fa5960571fee36b9b-Abstract.html},
	abstract = {Despite many empirical successes of spectral  clustering  methods(cid:173) algorithms  that  cluster  points  using  eigenvectors  of  matrices  de(cid:173) rived  from  the  data- there  are  several  unresolved  issues.  First,  there  are  a  wide  variety  of  algorithms  that  use  the  eigenvectors  in  slightly  different  ways.  Second,  many of these  algorithms  have  no  proof that  they  will  actually  compute  a  reasonable  clustering.  In  this  paper,  we  present  a  simple  spectral  clustering  algorithm  that can be implemented using a  few  lines  of Matlab.  Using  tools  from  matrix  perturbation  theory,  we  analyze  the  algorithm,  and  give  conditions  under  which  it  can  be  expected  to  do  well.  We  also  show  surprisingly  good  experimental  results  on  a  number  of  challenging clustering problems.},
	urldate = {2023-05-03},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Ng, Andrew and Jordan, Michael and Weiss, Yair},
	year = {2001},
	file = {Full Text PDF:/Users/yisu/Zotero/storage/A96KNYEZ/Ng et al. - 2001 - On Spectral Clustering Analysis and an algorithm.pdf:application/pdf},
}

@misc{schlichtkrull_modeling_2017,
	title = {Modeling {Relational} {Data} with {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1703.06103},
	abstract = {Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to deal with the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved by enriching them with an encoder model to accumulate evidence over multiple inference steps in the relational graph, demonstrating a large improvement of 29.8\% on FB15k-237 over a decoder-only baseline.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Schlichtkrull, Michael and Kipf, Thomas N. and Bloem, Peter and Berg, Rianne van den and Titov, Ivan and Welling, Max},
	month = oct,
	year = {2017},
	note = {arXiv:1703.06103 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Databases},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/3HBQQPPA/1703.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/4P9DX8HK/Schlichtkrull et al. - 2017 - Modeling Relational Data with Graph Convolutional .pdf:application/pdf},
}

@misc{yu_folkscope_2022,
	title = {{FolkScope}: {Intention} {Knowledge} {Graph} {Construction} for {Discovering} {E}-commerce {Commonsense}},
	shorttitle = {{FolkScope}},
	url = {http://arxiv.org/abs/2211.08316},
	abstract = {As stated by Oren Etzioni, ``commonsense is the dark matter of artificial intelligence''. In e-commerce, understanding users' needs or intentions requires substantial commonsense knowledge, e.g., ``A user bought an iPhone and a compatible case because the user wanted the phone to be protected''. In this paper, we present FolkScope, an intention knowledge graph construction framework, to reveal the structure of humans' minds about purchasing items on e-commerce platforms such as Amazon. As commonsense knowledge is usually ineffable and not expressed explicitly, it is challenging to perform any kind of information extraction. Thus, we propose a new approach that leverages the generation power of large-scale language models and human-in-the-loop annotations to semi-automatically construct the knowledge graph. We annotate a large amount of assertions for both plausibility and typicality of an intention that can explain a purchasing or co-purchasing behavior, where the intention can be an open reason or a predicate falling into one of 18 categories aligning with ConceptNet, e.g., IsA, MadeOf, UsedFor, etc. Then we populate the annotated information to all automatically generated ones, and further structurize the assertions using pattern mining and conceptualization to form more condensed and abstractive knowledge. We evaluate our knowledge graph using both intrinsic quality measures and a downstream application, i.e., recommendation. The comprehensive study shows that our knowledge graph can well model e-commerce commonsense knowledge and can have many potential applications.},
	urldate = {2023-05-04},
	publisher = {arXiv},
	author = {Yu, Changlong and Wang, Weiqi and Liu, Xin and Bai, Jiaxin and Song, Yangqiu and Li, Zheng and Gao, Yifan and Cao, Tianyu and Yin, Bing},
	month = nov,
	year = {2022},
	note = {arXiv:2211.08316 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/S2YPZZAE/2211.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/YPPYWTS9/Yu et al. - 2022 - FolkScope Intention Knowledge Graph Construction .pdf:application/pdf},
}

@inproceedings{sutton_policy_1999,
	title = {Policy {Gradient} {Methods} for {Reinforcement} {Learning} with {Function} {Approximation}},
	volume = {12},
	url = {https://papers.nips.cc/paper_files/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html},
	abstract = {Function  approximation  is  essential  to  reinforcement  learning,  but  the standard approach of approximating a  value function and deter(cid:173) mining  a  policy  from  it  has so  far  proven theoretically  intractable.  In this paper we explore an alternative approach in which the policy  is explicitly represented by its own function approximator,  indepen(cid:173) dent of the value function,  and is  updated according to the gradient  of expected reward with respect to the policy parameters.  Williams's  REINFORCE method and actor-critic methods are examples of this  approach.  Our  main  new  result  is  to  show  that  the  gradient  can  be  written  in  a  form  suitable  for  estimation  from  experience  aided  by  an  approximate  action-value  or  advantage  function.  Using  this  result,  we  prove for  the first  time that a  version  of policy  iteration  with arbitrary differentiable function approximation is convergent to  a  locally optimal policy.},
	urldate = {2023-05-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
	year = {1999},
	file = {Full Text PDF:/Users/yisu/Zotero/storage/GQDBUTCC/Sutton et al. - 1999 - Policy Gradient Methods for Reinforcement Learning.pdf:application/pdf},
}

@article{silver_mastering_2016,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature16961},
	doi = {10.1038/nature16961},
	language = {en},
	number = {7587},
	urldate = {2023-05-09},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	month = jan,
	year = {2016},
	pages = {484--489},
	file = {Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf:/Users/yisu/Zotero/storage/HPFDGXLH/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf:application/pdf},
}

@article{silver_mastering_2017,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature24270},
	doi = {10.1038/nature24270},
	abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo. Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games. To beat world champions at the game of Go, the computer program AlphaGo has relied largely on supervised learning from millions of human expert moves. David Silver and colleagues have now produced a system called AlphaGo Zero, which is based purely on reinforcement learning and learns solely from self-play. Starting from random moves, it can reach superhuman level in just a couple of days of training and five million games of self-play, and can now beat all previous versions of AlphaGo. Because the machine independently discovers the same fundamental principles of the game that took humans millennia to conceptualize, the work suggests that such principles have some universal character, beyond human bias.},
	language = {en},
	number = {7676},
	urldate = {2023-05-09},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	note = {Number: 7676
Publisher: Nature Publishing Group},
	keywords = {Computational science, Computer science, Reward},
	pages = {354--359},
	file = {Submitted Version:/Users/yisu/Zotero/storage/NM7NAF59/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:application/pdf},
}

@misc{girdhar_imagebind_2023,
	title = {{ImageBind}: {One} {Embedding} {Space} {To} {Bind} {Them} {All}},
	shorttitle = {{ImageBind}},
	url = {http://arxiv.org/abs/2305.05665},
	abstract = {We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.},
	urldate = {2023-05-13},
	publisher = {arXiv},
	author = {Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
	month = may,
	year = {2023},
	note = {arXiv:2305.05665 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Multimedia},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/4R5653N3/2305.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/GCE44SPA/Girdhar et al. - 2023 - ImageBind One Embedding Space To Bind Them All.pdf:application/pdf},
}

@misc{touvron_llama_2023,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	shorttitle = {{LLaMA}},
	url = {http://arxiv.org/abs/2302.13971},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13971 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/Y4HDQ3U2/2302.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/QEIZTSHJ/Touvron et al. - 2023 - LLaMA Open and Efficient Foundation Language Mode.pdf:application/pdf},
}

@misc{rajbhandari_zero_2020,
	title = {{ZeRO}: {Memory} {Optimizations} {Toward} {Training} {Trillion} {Parameter} {Models}},
	shorttitle = {{ZeRO}},
	url = {http://arxiv.org/abs/1910.02054},
	abstract = {Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware. We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create the world's largest language model (Turing-NLG, 17B parameters) with record breaking accuracy.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
	month = may,
	year = {2020},
	note = {arXiv:1910.02054 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/Z6KBE7PU/1910.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/PH5WP56G/Rajbhandari et al. - 2020 - ZeRO Memory Optimizations Toward Training Trillio.pdf:application/pdf},
}

@misc{yu_megabyte_2023,
	title = {{MEGABYTE}: {Predicting} {Million}-byte {Sequences} with {Multiscale} {Transformers}},
	shorttitle = {{MEGABYTE}},
	url = {http://arxiv.org/abs/2305.07185},
	abstract = {Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.},
	urldate = {2023-05-18},
	publisher = {arXiv},
	author = {Yu, Lili and Simig, Dániel and Flaherty, Colin and Aghajanyan, Armen and Zettlemoyer, Luke and Lewis, Mike},
	month = may,
	year = {2023},
	note = {arXiv:2305.07185 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/RECHNXJ5/2305.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/GXTRXACK/Yu et al. - 2023 - MEGABYTE Predicting Million-byte Sequences with M.pdf:application/pdf},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	urldate = {2023-05-19},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/HZM98VZ4/2106.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/GNBPCGW2/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf},
}

@misc{liu_understanding_2020,
	title = {Understanding the {Difficulty} of {Training} {Transformers}},
	url = {http://arxiv.org/abs/2004.08249},
	abstract = {Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding designing cutting-edge optimizers and learning rate schedulers carefully (e.g., conventional SGD fails to train Transformers effectively). Our objective here is to understand \${\textbackslash}textit\{what complicates Transformer training\}\$ from both empirical and theoretical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that influences training substantially -- for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (e.g., parameter updates) and results in significant disturbances in the model output. Yet we observe that a light dependency limits the model potential and leads to inferior trained models. Inspired by our analysis, we propose Admin (\${\textbackslash}textbf\{Ad\}\$aptive \${\textbackslash}textbf\{m\}\$odel \${\textbackslash}textbf\{in\}\$itialization) to stabilize stabilize the early stage's training and unleash its full potential in the late stage. Extensive experiments show that Admin is more stable, converges faster, and leads to better performance. Implementations are released at: https://github.com/LiyuanLucasLiu/Transforemr-Clinic.},
	urldate = {2023-05-20},
	publisher = {arXiv},
	author = {Liu, Liyuan and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
	month = sep,
	year = {2020},
	note = {arXiv:2004.08249 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/TIV9TNYR/2004.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/FTNS6NE5/Liu et al. - 2020 - Understanding the Difficulty of Training Transform.pdf:application/pdf},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2023-05-25},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/SYIHHS8P/2201.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/UY23MPMA/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf},
}

@misc{nye_show_2021,
	title = {Show {Your} {Work}: {Scratchpads} for {Intermediate} {Computation} with {Language} {Models}},
	shorttitle = {Show {Your} {Work}},
	url = {http://arxiv.org/abs/2112.00114},
	abstract = {Large pre-trained language models perform remarkably well on tasks that can be done "in one pass", such as generating realistic text or synthesizing computer programs. However, they struggle with tasks that require unbounded multi-step computation, such as adding integers or executing programs. Surprisingly, we find that these same models are able to perform complex multi-step computations -- even in the few-shot regime -- when asked to perform the operation "step by step", showing the results of intermediate computations. In particular, we train transformers to perform multi-step computations by asking them to emit intermediate computation steps into a "scratchpad". On a series of increasingly complex tasks ranging from long addition to the execution of arbitrary programs, we show that scratchpads dramatically improve the ability of language models to perform multi-step computations.},
	urldate = {2023-05-25},
	publisher = {arXiv},
	author = {Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and Sutton, Charles and Odena, Augustus},
	month = nov,
	year = {2021},
	note = {arXiv:2112.00114 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/TC28KA6L/2112.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/AH6I69WY/Nye et al. - 2021 - Show Your Work Scratchpads for Intermediate Compu.pdf:application/pdf},
}

@misc{press_train_2022,
	title = {Train {Short}, {Test} {Long}: {Attention} with {Linear} {Biases} {Enables} {Input} {Length} {Extrapolation}},
	shorttitle = {Train {Short}, {Test} {Long}},
	url = {http://arxiv.org/abs/2108.12409},
	abstract = {Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11\% faster and using 11\% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.},
	urldate = {2023-05-29},
	publisher = {arXiv},
	author = {Press, Ofir and Smith, Noah A. and Lewis, Mike},
	month = apr,
	year = {2022},
	note = {arXiv:2108.12409 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/E2MK6L7F/2108.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/L5MGE23M/Press et al. - 2022 - Train Short, Test Long Attention with Linear Bias.pdf:application/pdf},
}

@misc{dao_flashattention_2022,
	title = {{FlashAttention}: {Fast} and {Memory}-{Efficient} {Exact} {Attention} with {IO}-{Awareness}},
	shorttitle = {{FlashAttention}},
	url = {http://arxiv.org/abs/2205.14135},
	abstract = {Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15\% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3\${\textbackslash}times\$ speedup on GPT-2 (seq. length 1K), and 2.4\${\textbackslash}times\$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4\% accuracy) and Path-256 (seq. length 64K, 63.1\% accuracy).},
	urldate = {2023-05-29},
	publisher = {arXiv},
	author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and Ré, Christopher},
	month = jun,
	year = {2022},
	note = {arXiv:2205.14135 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/H6CCH93F/2205.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/5UQAF8G6/Dao et al. - 2022 - FlashAttention Fast and Memory-Efficient Exact At.pdf:application/pdf},
}

@misc{raffel_exploring_2020,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {http://arxiv.org/abs/1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	urldate = {2023-05-29},
	publisher = {arXiv},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = jul,
	year = {2020},
	note = {arXiv:1910.10683 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/JQN3YNDF/1910.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/P2CU2SPQ/Raffel et al. - 2020 - Exploring the Limits of Transfer Learning with a U.pdf:application/pdf},
}

@misc{su_roformer_2022,
	title = {{RoFormer}: {Enhanced} {Transformer} with {Rotary} {Position} {Embedding}},
	shorttitle = {{RoFormer}},
	url = {http://arxiv.org/abs/2104.09864},
	abstract = {Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: {\textbackslash}url\{https://huggingface.co/docs/transformers/model\_doc/roformer\}.},
	urldate = {2023-05-29},
	publisher = {arXiv},
	author = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
	month = aug,
	year = {2022},
	note = {arXiv:2104.09864 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/TGUIAVCH/2104.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/QGNTEIER/Su et al. - 2022 - RoFormer Enhanced Transformer with Rotary Positio.pdf:application/pdf},
}

@article{lewis_retrieval-augmented_nodate,
	title = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when ﬁne-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-speciﬁc architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pretrained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and evaluate our models on a wide range of knowledgeintensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract architectures. For language generation tasks, we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	language = {en},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	file = {Lewis et al. - Retrieval-Augmented Generation for Knowledge-Inten.pdf:/Users/yisu/Zotero/storage/U5NXHDGE/Lewis et al. - Retrieval-Augmented Generation for Knowledge-Inten.pdf:application/pdf},
}

@misc{dettmers_qlora_2023,
	title = {{QLoRA}: {Efficient} {Finetuning} of {Quantized} {LLMs}},
	shorttitle = {{QLoRA}},
	url = {http://arxiv.org/abs/2305.14314},
	abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
	urldate = {2023-06-15},
	publisher = {arXiv},
	author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	month = may,
	year = {2023},
	note = {arXiv:2305.14314 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/QCP2VYNR/2305.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/SDL996FG/Dettmers et al. - 2023 - QLoRA Efficient Finetuning of Quantized LLMs.pdf:application/pdf},
}

@misc{wang_self-instruct_2023,
	title = {Self-{Instruct}: {Aligning} {Language} {Models} with {Self}-{Generated} {Instructions}},
	shorttitle = {Self-{Instruct}},
	url = {http://arxiv.org/abs/2212.10560},
	abstract = {Large "instruction-tuned" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning. Our code and data are available at https://github.com/yizhongw/self-instruct.},
	urldate = {2023-06-17},
	publisher = {arXiv},
	author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
	month = may,
	year = {2023},
	note = {arXiv:2212.10560 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/ZYDQC4SJ/2212.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/T6RGX5ZH/Wang et al. - 2023 - Self-Instruct Aligning Language Models with Self-.pdf:application/pdf},
}

@misc{touvron_llama_2023-1,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	shorttitle = {{LLaMA}},
	url = {http://arxiv.org/abs/2302.13971},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
	urldate = {2023-06-20},
	publisher = {arXiv},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13971 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/VHEKR8JA/2302.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/9UNYPGMR/Touvron et al. - 2023 - LLaMA Open and Efficient Foundation Language Mode.pdf:application/pdf},
}

@misc{bahdanau_neural_2016-1,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	doi = {10.48550/arXiv.1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2023-06-25},
	publisher = {arXiv},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv:1409.0473 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/5XGDQDAE/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/THA98RRL/1409.html:text/html},
}

@article{tang_overlapping_nodate,
	title = {Overlapping {Experiment} {Infrastructure}: {More}, {Better}, {Faster} {Experimentation}},
	abstract = {At Google, experimentation is practically a mantra; we evaluate almost every change that potentially affects what our users experience. Such changes include not only obvious user-visible changes such as modiﬁcations to a user interface, but also more subtle changes such as different machine learning algorithms that might affect ranking or content selection. Our insatiable appetite for experimentation has led us to tackle the problems of how to run more experiments, how to run experiments that produce better decisions, and how to run them faster. In this paper, we describe Google’s overlapping experiment infrastructure that is a key component to solving these problems. In addition, because an experiment infrastructure alone is insufﬁcient, we also discuss the associated tools and educational processes required to use it effectively. We conclude by describing trends that show the success of this overall experimental environment. While the paper speciﬁcally describes the experiment system and experimental processes we have in place at Google, we believe they can be generalized and applied by any entity interested in using experimentation to improve search engines and other web applications.},
	language = {en},
	author = {Tang, Diane and Agarwal, Ashish and O’Brien, Deirdre and Meyer, Mike},
	file = {Tang et al. - Overlapping Experiment Infrastructure More, Bette.pdf:/Users/yisu/Zotero/storage/H4LBI2AH/Tang et al. - Overlapping Experiment Infrastructure More, Bette.pdf:application/pdf},
}

@misc{gao_precise_2022,
	title = {Precise {Zero}-{Shot} {Dense} {Retrieval} without {Relevance} {Labels}},
	url = {http://arxiv.org/abs/2212.10496},
	abstract = {While dense retrieval has been shown effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance label is available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings{\textasciitilde}(HyDE). Given a query, HyDE first zero-shot instructs an instruction-following language model (e.g. InstructGPT) to generate a hypothetical document. The document captures relevance patterns but is unreal and may contain false details. Then, an unsupervised contrastively learned encoder{\textasciitilde}(e.g. Contriever) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, where similar real documents are retrieved based on vector similarity. This second step ground the generated document to the actual corpus, with the encoder's dense bottleneck filtering out the incorrect details. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers, across various tasks (e.g. web search, QA, fact verification) and languages{\textasciitilde}(e.g. sw, ko, ja).},
	urldate = {2023-07-02},
	publisher = {arXiv},
	author = {Gao, Luyu and Ma, Xueguang and Lin, Jimmy and Callan, Jamie},
	month = dec,
	year = {2022},
	note = {arXiv:2212.10496 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/AKZBE79R/2212.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/7YHIGEM2/Gao et al. - 2022 - Precise Zero-Shot Dense Retrieval without Relevanc.pdf:application/pdf},
}

@misc{xia_transact_2023,
	title = {{TransAct}: {Transformer}-based {Realtime} {User} {Action} {Model} for {Recommendation} at {Pinterest}},
	shorttitle = {{TransAct}},
	url = {http://arxiv.org/abs/2306.00248},
	abstract = {Sequential models that encode user activity for next action prediction have become a popular design choice for building web-scale personalized recommendation systems. Traditional methods of sequential recommendation either utilize end-to-end learning on realtime user actions, or learn user representations separately in an offline batch-generated manner. This paper (1) presents Pinterest's ranking architecture for Homefeed, our personalized recommendation product and the largest engagement surface; (2) proposes TransAct, a sequential model that extracts users' short-term preferences from their realtime activities; (3) describes our hybrid approach to ranking, which combines end-to-end sequential modeling via TransAct with batch-generated user embeddings. The hybrid approach allows us to combine the advantages of responsiveness from learning directly on realtime user activity with the cost-effectiveness of batch user representations learned over a longer time period. We describe the results of ablation studies, the challenges we faced during productionization, and the outcome of an online A/B experiment, which validates the effectiveness of our hybrid ranking model. We further demonstrate the effectiveness of TransAct on other surfaces such as contextual recommendations and search. Our model has been deployed to production in Homefeed, Related Pins, Notifications, and Search at Pinterest.},
	urldate = {2023-07-07},
	publisher = {arXiv},
	author = {Xia, Xue and Eksombatchai, Pong and Pancha, Nikil and Badani, Dhruvil Deven and Wang, Po-Wei and Gu, Neng and Joshi, Saurabh Vishwas and Farahpour, Nazanin and Zhang, Zhiyuan and Zhai, Andrew},
	month = may,
	year = {2023},
	note = {arXiv:2306.00248 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/GP3CXQF3/2306.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/WWKJLCPA/Xia et al. - 2023 - TransAct Transformer-based Realtime User Action M.pdf:application/pdf},
}

@misc{zhao_survey_2023,
	title = {A {Survey} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2303.18223},
	abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
	urldate = {2023-07-08},
	publisher = {arXiv},
	author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
	month = jun,
	year = {2023},
	note = {arXiv:2303.18223 [cs]
version: 10},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/BIF3FJ62/2303.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/NII8BMRK/Zhao et al. - 2023 - A Survey of Large Language Models.pdf:application/pdf},
}

@misc{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	urldate = {2023-07-09},
	publisher = {arXiv},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv:1707.06347 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/HVV29QJG/1707.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/EHNDG4ZD/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2023-07-11},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/FINQL4KV/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/8PNGQA9U/2005.html:text/html},
}

@misc{yang_large_2020,
	title = {Large {Scale} {Product} {Graph} {Construction} for {Recommendation} in {E}-commerce},
	url = {http://arxiv.org/abs/2010.05525},
	abstract = {Building a recommendation system that serves billions of users on daily basis is a challenging problem, as the system needs to make astronomical number of predictions per second based on real-time user behaviors with O(1) time complexity. Such kind of large scale recommendation systems usually rely heavily on pre-built index of products to speedup the recommendation service so that online user waiting time is un-noticeable. One important indexing structure is the product-product index, where one can retrieval a list of ranked products given a seed product. The index can be viewed as a weighted product-product graph. In this paper, we present our novel technologies to efficiently build such kind of indexed product graphs. In particular, we propose the Swing algorithm to capture the substitute relationships between products, which can utilize the substructures of user-item click bi-partitive graph. Then we propose the Surprise algorithm for the modeling of complementary product relationships, which utilizes product category information and solves the sparsity problem of user co-purchasing graph via clustering technique. Base on these two approaches, we can build the basis product graph for recommendation in Taobao. The approaches are evaluated comprehensively with both offline and online experiments, and the results demonstrate the effectiveness and efficiency of the work.},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {Yang, Xiaoyong and Zhu, Yadong and Zhang, Yi and Wang, Xiaobo and Yuan, Quan},
	month = oct,
	year = {2020},
	note = {arXiv:2010.05525 [cs]},
	keywords = {Computer Science - Information Retrieval},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/M9GGYNQI/2010.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/ABC354MA/Yang et al. - 2020 - Large Scale Product Graph Construction for Recomme.pdf:application/pdf},
}

@misc{muennighoff_sgpt_2022,
	title = {{SGPT}: {GPT} {Sentence} {Embeddings} for {Semantic} {Search}},
	shorttitle = {{SGPT}},
	url = {http://arxiv.org/abs/2202.08904},
	abstract = {Decoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7\% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {Muennighoff, Niklas},
	month = aug,
	year = {2022},
	note = {arXiv:2202.08904 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/3SI2UW4G/2202.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/Q8T8JYEG/Muennighoff - 2022 - SGPT GPT Sentence Embeddings for Semantic Search.pdf:application/pdf},
}

@misc{touvron_llama_2023-2,
	title = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
	shorttitle = {Llama 2},
	url = {http://arxiv.org/abs/2307.09288},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
	urldate = {2023-07-25},
	publisher = {arXiv},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09288 [cs]
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/AEIG3ZX8/2307.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/8KAKDRDP/Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf:application/pdf},
}

@inproceedings{hidasi_recurrent_2018,
	title = {Recurrent {Neural} {Networks} with {Top}-k {Gains} for {Session}-based {Recommendations}},
	url = {http://arxiv.org/abs/1706.03847},
	doi = {10.1145/3269206.3271761},
	abstract = {RNNs have been shown to be excellent models for sequential data and in particular for data that is generated by users in an session-based manner. The use of RNNs provides impressive performance benefits over classical methods in session-based recommendations. In this work we introduce novel ranking loss functions tailored to RNNs in the recommendation setting. The improved performance of these losses over alternatives, along with further tricks and refinements described in this work, allow for an overall improvement of up to 35\% in terms of MRR and Recall@20 over previous session-based RNN solutions and up to 53\% over classical collaborative filtering approaches. Unlike data augmentation-based improvements, our method does not increase training times significantly. We further demonstrate the performance gain of the RNN over baselines in an online A/B test.},
	urldate = {2023-08-01},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	author = {Hidasi, Balázs and Karatzoglou, Alexandros},
	month = oct,
	year = {2018},
	note = {arXiv:1706.03847 [cs]},
	keywords = {Computer Science - Machine Learning},
	pages = {843--852},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/7Z5T7HXA/1706.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/IY2JVH4U/Hidasi and Karatzoglou - 2018 - Recurrent Neural Networks with Top-k Gains for Ses.pdf:application/pdf},
}

@inproceedings{de_souza_pereira_moreira_transformers4rec_2021,
	address = {Amsterdam Netherlands},
	title = {{Transformers4Rec}: {Bridging} the {Gap} between {NLP} and {Sequential} / {Session}-{Based} {Recommendation}},
	isbn = {978-1-4503-8458-2},
	shorttitle = {{Transformers4Rec}},
	url = {https://dl.acm.org/doi/10.1145/3460231.3474255},
	doi = {10.1145/3460231.3474255},
	abstract = {Much of the recent progress in sequential and session-based recommendation has been driven by improvements in model architecture and pretraining techniques originating in the field of Natural Language Processing. Transformer architectures in particular have facilitated building higher-capacity models and provided data augmentation and training techniques which demonstrably improve the effectiveness of sequential recommendation. But with a thousandfold more research going on in NLP, the application of transformers for recommendation understandably lags behind. To remedy this we introduce Transformers4Rec, an open-source library built upon HuggingFace’s Transformers library with a similar goal of opening up the advances of NLP based Transformers to the recommender system community and making these advancements immediately accessible for the tasks of sequential and session-based recommendation. Like its core dependency, Transformers4Rec is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments.},
	language = {en},
	urldate = {2023-08-01},
	booktitle = {Fifteenth {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {De Souza Pereira Moreira, Gabriel and Rabhi, Sara and Lee, Jeong Min and Ak, Ronay and Oldridge, Even},
	month = sep,
	year = {2021},
	pages = {143--153},
	file = {De Souza Pereira Moreira et al. - 2021 - Transformers4Rec Bridging the Gap between NLP and.pdf:/Users/yisu/Zotero/storage/6NX7XX6Z/De Souza Pereira Moreira et al. - 2021 - Transformers4Rec Bridging the Gap between NLP and.pdf:application/pdf},
}

@misc{zhou_deep_2018,
	title = {Deep {Interest} {Network} for {Click}-{Through} {Rate} {Prediction}},
	url = {http://arxiv.org/abs/1706.06978},
	doi = {10.48550/arXiv.1706.06978},
	abstract = {Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding{\textbackslash}\&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features. In this way, user features are compressed into a fixed-length representation vector, in regardless of what candidate ads are. The use of fixed-length vector will be a bottleneck, which brings difficulty for Embedding{\textbackslash}\&MLP methods to capture user's diverse interests effectively from rich historical behaviors. In this paper, we propose a novel model: Deep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad. This representation vector varies over different ads, improving the expressive ability of model greatly. Besides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters. Experiments on two public datasets as well as an Alibaba real production dataset with over 2 billion samples demonstrate the effectiveness of proposed approaches, which achieve superior performance compared with state-of-the-art methods. DIN now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic.},
	urldate = {2023-08-01},
	publisher = {arXiv},
	author = {Zhou, Guorui and Song, Chengru and Zhu, Xiaoqiang and Fan, Ying and Zhu, Han and Ma, Xiao and Yan, Yanghui and Jin, Junqi and Li, Han and Gai, Kun},
	month = sep,
	year = {2018},
	note = {arXiv:1706.06978 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, I.2.6, H.3.2},
	file = {arXiv Fulltext PDF:/Users/yisu/Zotero/storage/74EFBFVI/Zhou et al. - 2018 - Deep Interest Network for Click-Through Rate Predi.pdf:application/pdf;arXiv.org Snapshot:/Users/yisu/Zotero/storage/7JM7UXW5/1706.html:text/html},
}

@misc{copet_simple_2023,
	title = {Simple and {Controllable} {Music} {Generation}},
	url = {http://arxiv.org/abs/2306.05284},
	abstract = {We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/facebookresearch/audiocraft.},
	urldate = {2023-08-06},
	publisher = {arXiv},
	author = {Copet, Jade and Kreuk, Felix and Gat, Itai and Remez, Tal and Kant, David and Synnaeve, Gabriel and Adi, Yossi and Défossez, Alexandre},
	month = jun,
	year = {2023},
	note = {arXiv:2306.05284 [cs, eess]
version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/LASXMVJU/2306.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/28PGVM9A/Copet et al. - 2023 - Simple and Controllable Music Generation.pdf:application/pdf},
}

@misc{schick_toolformer_2023,
	title = {Toolformer: {Language} {Models} {Can} {Teach} {Themselves} to {Use} {Tools}},
	shorttitle = {Toolformer},
	url = {http://arxiv.org/abs/2302.04761},
	abstract = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q{\textbackslash}\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
	urldate = {2023-08-10},
	publisher = {arXiv},
	author = {Schick, Timo and Dwivedi-Yu, Jane and Dessì, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04761 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/JYNCRSCW/2302.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/JVVQI8J6/Schick et al. - 2023 - Toolformer Language Models Can Teach Themselves t.pdf:application/pdf},
}

@inproceedings{hewitt_backpack_2023,
	address = {Toronto, Canada},
	title = {Backpack {Language} {Models}},
	url = {https://aclanthology.org/2023.acl-long.506},
	doi = {10.18653/v1/2023.acl-long.506},
	abstract = {We present Backpacks: a new neural architecture that marries strong modeling performancewith an interface for interpretability and control. Backpacks learn multiple non-contextual sense vectors for each word in a vocabulary, and represent a word in a sequence as a context-dependent, non-negative linear combination ofsense vectors in this sequence. We find that, after training, sense vectors specialize, each encoding a different aspect of a word. We can interpret a sense vector by inspecting its (non-contextual, linear) projection onto the output space, and intervene on these interpretable hooks to change the model's behavior in predictable ways. We train a 170M-parameter Backpack language model on OpenWebText, matching the loss of a GPT-2 small (124Mparameter) Transformer. On lexical similarity evaluations, we find that Backpack sense vectors outperform even a 6B-parameter Transformer LM's word embeddings. Finally, we present simple algorithms that intervene on sense vectors to perform controllable text generation and debiasing. For example, we can edit the sense vocabulary to tend more towards a topic, or localize a source of gender bias to a sense vector and globally suppress that sense.},
	urldate = {2023-08-15},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hewitt, John and Thickstun, John and Manning, Christopher and Liang, Percy},
	month = jul,
	year = {2023},
	pages = {9103--9125},
	file = {Full Text PDF:/Users/yisu/Zotero/storage/AUQ2PWXL/Hewitt et al. - 2023 - Backpack Language Models.pdf:application/pdf},
}

@misc{huang_raven_2023,
	title = {{RAVEN}: {In}-{Context} {Learning} with {Retrieval} {Augmented} {Encoder}-{Decoder} {Language} {Models}},
	shorttitle = {{RAVEN}},
	url = {http://arxiv.org/abs/2308.07922},
	abstract = {In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of the state-of-the-art ATLAS model and identify its limitations in in-context learning, primarily due to a mismatch between pretraining and testing, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training or model modifications. Through extensive experiments, we demonstrate that RAVEN significantly outperforms ATLAS and achieves results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder language models for in-context learning and encourages further research in this direction.},
	urldate = {2023-08-18},
	publisher = {arXiv},
	author = {Huang, Jie and Ping, Wei and Xu, Peng and Shoeybi, Mohammad and Chang, Kevin Chen-Chuan and Catanzaro, Bryan},
	month = aug,
	year = {2023},
	note = {arXiv:2308.07922 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/BCN5J8CS/2308.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/YYZY3KHM/Huang et al. - 2023 - RAVEN In-Context Learning with Retrieval Augmente.pdf:application/pdf},
}

@misc{ramesh_zero-shot_2021,
	title = {Zero-{Shot} {Text}-to-{Image} {Generation}},
	url = {http://arxiv.org/abs/2102.12092},
	abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
	urldate = {2023-08-15},
	publisher = {arXiv},
	author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {arXiv:2102.12092 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/2KPNH6FA/2102.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/ITJ995MR/Ramesh et al. - 2021 - Zero-Shot Text-to-Image Generation.pdf:application/pdf},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
	urldate = {2023-08-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
	file = {Full Text PDF:/Users/yisu/Zotero/storage/VBMYLPG8/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf},
}

@misc{loshchilov_sgdr_2017,
	title = {{SGDR}: {Stochastic} {Gradient} {Descent} with {Warm} {Restarts}},
	shorttitle = {{SGDR}},
	url = {http://arxiv.org/abs/1608.03983},
	abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
	urldate = {2023-08-20},
	publisher = {arXiv},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = may,
	year = {2017},
	note = {arXiv:1608.03983 [cs, math]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/C3UD92DN/1608.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/MP9ZACPY/Loshchilov and Hutter - 2017 - SGDR Stochastic Gradient Descent with Warm Restar.pdf:application/pdf},
}

@article{sutskever_importance_nodate,
	title = {On the importance of initialization and momentum in deep learning},
	abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We ﬁnd that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.},
	language = {en},
	author = {Sutskever, Ilya and Martens, James and Dahl, George},
	keywords = {momentum},
	file = {Sutskever et al. - On the importance of initialization and momentum i.pdf:/Users/yisu/Zotero/storage/WCNMDBPV/Sutskever et al. - On the importance of initialization and momentum i.pdf:application/pdf},
}

@techreport{athey_surrogate_2019,
	address = {Cambridge, MA},
	title = {The {Surrogate} {Index}: {Combining} {Short}-{Term} {Proxies} to {Estimate} {Long}-{Term} {Treatment} {Effects} {More} {Rapidly} and {Precisely}},
	shorttitle = {The {Surrogate} {Index}},
	url = {http://www.nber.org/papers/w26463.pdf},
	abstract = {A common challenge in estimating the long-term impacts of treatments (e.g., job training programs) is that the outcomes of interest (e.g., lifetime earnings) are observed with a long delay. We address this problem by combining several short-term outcomes (e.g., short-run earnings) into a “surrogate index,” the predicted value of the long-term outcome given the short-term outcomes. We show that the average treatment effect on the surrogate index equals the treatment effect on the long-term outcome under the assumption that the long-term outcome is independent of the treatment conditional on the surrogate index. We then characterize the bias that arises from violations of this assumption, deriving feasible bounds on the degree of bias and providing simple methods to validate the key assumption using additional outcomes. Finally, we develop efficient estimators for the surrogate index and show that even in settings where the long-term outcome is observed, using a surrogate index can increase precision. We apply our method to analyze the long-term impacts of a multi-site job training experiment in California. Using short-term employment rates as surrogates, one could have estimated the program's impacts on mean employment rates over a 9 year horizon within 1.5 years, with a 35\% reduction in standard errors. Our empirical results suggest that the long-term impacts of programs on labor market outcomes can be predicted accurately by combining their short-term treatment effects into a surrogate index.},
	language = {en},
	number = {w26463},
	urldate = {2023-08-21},
	institution = {National Bureau of Economic Research},
	author = {Athey, Susan and Chetty, Raj and Imbens, Guido and Kang, Hyunseung},
	month = nov,
	year = {2019},
	doi = {10.3386/w26463},
	pages = {w26463},
	file = {Athey et al. - 2019 - The Surrogate Index Combining Short-Term Proxies .pdf:/Users/yisu/Zotero/storage/48SNPDVI/Athey et al. - 2019 - The Surrogate Index Combining Short-Term Proxies .pdf:application/pdf},
}

@misc{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {http://arxiv.org/abs/1711.05101},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
	urldate = {2023-08-21},
	publisher = {arXiv},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = jan,
	year = {2019},
	note = {arXiv:1711.05101 [cs, math]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/I5MG8BGJ/1711.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/NR6WYXV8/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf:application/pdf},
}

@inproceedings{egg_online_2021,
	address = {Amsterdam Netherlands},
	title = {Online {Learning} for {Recommendations} at {Grubhub}},
	isbn = {978-1-4503-8458-2},
	url = {https://dl.acm.org/doi/10.1145/3460231.3474599},
	doi = {10.1145/3460231.3474599},
	abstract = {We propose a method to easily modify existing offline Recommender Systems to run online using Transfer Learning. Online Learning for Recommender Systems has two main advantages: quality and scale. Like many Machine Learning algorithms in production if not regularly retrained will suffer from Concept Drift. A policy that is updated frequently online can adapt to drift faster than a batch system. This is especially true for user-interaction systems like recommenders where the underlying distribution can shift drastically to follow user behaviour. As a platform grows rapidly like Grubhub, the cost of running batch training jobs becomes material. A shift from stateless batch learning offline to stateful incremental learning online can recover, for example, at Grubhub, up to a 45x cost savings and a +20\% metrics increase. There are a few challenges to overcome with the transition to online stateful learning, namely convergence, non-stationary embeddings and offpolicy evaluation, which we explore from our experiences running this system in production.},
	language = {en},
	urldate = {2023-08-21},
	booktitle = {Fifteenth {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Egg, Alex},
	month = sep,
	year = {2021},
	pages = {569--571},
	file = {Egg - 2021 - Online Learning for Recommendations at Grubhub.pdf:/Users/yisu/Zotero/storage/PM7FD54J/Egg - 2021 - Online Learning for Recommendations at Grubhub.pdf:application/pdf},
}

@inproceedings{gutierrez_granada_recommendations_2021-1,
	address = {Amsterdam Netherlands},
	title = {Recommendations at {Videoland}},
	isbn = {978-1-4503-8458-2},
	url = {https://dl.acm.org/doi/10.1145/3460231.3474617},
	doi = {10.1145/3460231.3474617},
	abstract = {With the largest commercial TV channels in the Netherlands, RTL plays an important role in society. We reach over 85\% of Dutch people on a weekly basis, spending on average around 45 minutes a day with us and our content. We feel a part of all of the Netherlands. We actively pick up the challenge to show a diverse and representative view in the media.},
	language = {en},
	urldate = {2023-08-21},
	booktitle = {Fifteenth {ACM} {Conference} on {Recommender} {Systems}},
	publisher = {ACM},
	author = {Gutierrez Granada, Mateo and Odijk, Daan},
	month = sep,
	year = {2021},
	keywords = {Recsys, SeqRec},
	pages = {580--582},
	file = {Gutierrez Granada and Odijk - 2021 - Recommendations at Videoland.pdf:/Users/yisu/Zotero/storage/X7CHT3MP/Gutierrez Granada and Odijk - 2021 - Recommendations at Videoland.pdf:application/pdf},
}

@inproceedings{breck_ml_2017-1,
	address = {Boston, MA},
	title = {The {ML} test score: {A} rubric for {ML} production readiness and technical debt reduction},
	isbn = {978-1-5386-2715-0},
	shorttitle = {The {ML} test score},
	url = {http://ieeexplore.ieee.org/document/8258038/},
	doi = {10.1109/BigData.2017.8258038},
	abstract = {Creating reliable, production-level machine learning systems brings on a host of concerns not found in small toy examples or even large ofﬂine research experiments. Testing and monitoring are key considerations for ensuring the production-readiness of an ML system, and for reducing technical debt of ML systems. But it can be difﬁcult to formulate speciﬁc tests, given that the actual prediction behavior of any given model is difﬁcult to specify a priori. In this paper, we present 28 speciﬁc tests and monitoring needs, drawn from experience with a wide range of production ML systems to help quantify these issues and present an easy to follow road-map to improve production readiness and pay down ML technical debt.},
	language = {en},
	urldate = {2023-08-21},
	booktitle = {2017 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	publisher = {IEEE},
	author = {Breck, Eric and Cai, Shanqing and Nielsen, Eric and Salib, Michael and Sculley, D.},
	month = dec,
	year = {2017},
	pages = {1123--1132},
	file = {Breck et al. - 2017 - The ML test score A rubric for ML production read.pdf:/Users/yisu/Zotero/storage/C3VSMCZA/Breck et al. - 2017 - The ML test score A rubric for ML production read.pdf:application/pdf},
}

@misc{ozdayi_fair_2022,
	title = {Fair {Machine} {Learning} under {Limited} {Demographically} {Labeled} {Data}},
	url = {http://arxiv.org/abs/2106.04757},
	abstract = {Prior studies have shown that, training machine learning models via empirical loss minimization to maximize a utility metric (e.g., accuracy), might yield models that make discriminatory predictions. To alleviate this issue, we develop a new training algorithm, named BiFair, which jointly minimizes for a utility, and a fairness loss of interest. Crucially, we do so without directly modifying the training objective, e.g., by adding regularization terms. Rather, we learn a set of weights on the training dataset, such that, training on the weighted dataset ensures both good utility, and fairness. The dataset weights are learned in concurrence to the model training, which is done by solving a bilevel optimization problem using a held-out validation dataset. Overall, this approach yields models with better fairness-utility trade-offs. Particularly, we compare our algorithm with three other state-of-the-art fair training algorithms over three real-world datasets, and demonstrate that, BiFair consistently performs better, i.e., we reach to better values of a given fairness metric under same, or higher accuracy. Further, our algorithm is scalable. It is applicable both to simple models, such as logistic regression, as well as more complex models, such as deep neural networks, as evidenced by our experimental analysis.},
	language = {en},
	urldate = {2023-08-22},
	publisher = {arXiv},
	author = {Ozdayi, Mustafa Safa and Kantarcioglu, Murat and Iyer, Rishabh},
	month = apr,
	year = {2022},
	note = {arXiv:2106.04757 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computers and Society},
	file = {Ozdayi et al. - 2022 - Fair Machine Learning under Limited Demographicall.pdf:/Users/yisu/Zotero/storage/VDAVL7GE/Ozdayi et al. - 2022 - Fair Machine Learning under Limited Demographicall.pdf:application/pdf},
}

@book{odersky_programming_2019,
	address = {Walnut Creek, California},
	edition = {Fourth edition},
	title = {Programming in {Scala}},
	isbn = {978-0-9815316-1-8},
	language = {en},
	publisher = {Artima, Inc},
	author = {Odersky, Martin and Spoon, Lex and Venners, Bill},
	year = {2019},
	keywords = {Computer programming, Programming languages (Electronic computers), Scala (Computer program language)},
	file = {Odersky et al. - 2019 - Programming in Scala.pdf:/Users/yisu/Zotero/storage/NQUHFLKK/Odersky et al. - 2019 - Programming in Scala.pdf:application/pdf},
}

@article{chambers_spark_nodate,
	title = {Spark: {The} {Definitive} {Guide}},
	language = {en},
	author = {Chambers, Bill and Zaharia, Matei},
	file = {Chambers and Zaharia - Spark The Definitive Guide.pdf:/Users/yisu/Zotero/storage/U6PWB92X/Chambers and Zaharia - Spark The Definitive Guide.pdf:application/pdf},
}

@misc{kirchenbauer_watermark_2023,
	title = {A {Watermark} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2301.10226},
	abstract = {Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.},
	urldate = {2023-08-23},
	publisher = {arXiv},
	author = {Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
	month = jun,
	year = {2023},
	note = {arXiv:2301.10226 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/BSTE2Y6N/2301.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/Z7DHRZ5F/Kirchenbauer et al. - 2023 - A Watermark for Large Language Models.pdf:application/pdf},
}

@misc{puigcerver_sparse_2023,
	title = {From {Sparse} to {Soft} {Mixtures} of {Experts}},
	url = {http://arxiv.org/abs/2308.00951},
	abstract = {Sparse mixture of expert architectures (MoEs) scale model capacity without large increases in training or inference costs. Despite their success, MoEs suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In this work, we propose Soft MoE, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of MoEs. Soft MoE performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert. As in other MoE works, experts in Soft MoE only process a subset of the (combined) tokens, enabling larger model capacity at lower inference cost. In the context of visual recognition, Soft MoE greatly outperforms standard Transformers (ViTs) and popular MoE variants (Tokens Choice and Experts Choice). For example, Soft MoE-Base/16 requires 10.5× lower inference cost (5.7× lower wall-clock time) than ViT-Huge/14 while matching its performance after similar training. Soft MoE also scales well: Soft MoE Huge/14 with 128 experts in 16 MoE layers has over 40× more parameters than ViT Huge/14, while inference time cost grows by only 2\%, and it performs substantially better.},
	language = {en},
	urldate = {2023-08-22},
	publisher = {arXiv},
	author = {Puigcerver, Joan and Riquelme, Carlos and Mustafa, Basil and Houlsby, Neil},
	month = aug,
	year = {2023},
	note = {arXiv:2308.00951 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Puigcerver et al. - 2023 - From Sparse to Soft Mixtures of Experts.pdf:/Users/yisu/Zotero/storage/23KMWVFI/Puigcerver et al. - 2023 - From Sparse to Soft Mixtures of Experts.pdf:application/pdf},
}

@book{bhargava_grokking_2016,
	address = {Shelter Island},
	title = {Grokking algorithms: an illustrated guide for programmers and other curious people},
	isbn = {978-1-61729-223-1},
	shorttitle = {Grokking algorithms},
	abstract = {"Grokking Algorithms is a fully illustrated, friendly guide that teaches you how to apply common algorithms to the practical problems you face every day as a programmer. You'll start with sorting and searching and, as you build up your skills in thinking algorithmically, you'll tackle more complex concerns such as data compression and artificial intelligence. Each carefully presented example includes helpful diagrams and fully annotated code samples in Python."--Publisher's desription},
	language = {en},
	publisher = {Manning},
	author = {Bhargava, Aditya Y.},
	year = {2016},
	keywords = {Computer programming, Computer algorithms, Handbooks, manuals, etc},
	file = {Bhargava - 2016 - Grokking algorithms an illustrated guide for prog.pdf:/Users/yisu/Zotero/storage/KQ3ETVR4/Bhargava - 2016 - Grokking algorithms an illustrated guide for prog.pdf:application/pdf},
}

@article{roziere_code_nodate,
	title = {Code {Llama}: {Open} {Foundation} {Models} for {Code}},
	language = {en},
	author = {Rozière, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Ellen, Xiaoqing and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, Jérémy and Kozhevnikov, Artyom and Evtimov, Ivan and Bitton, Joanna and Bhatt, Manish and Ferrer, Cristian Canton and Grattafiori, Aaron and Xiong, Wenhan and Défossez, Alexandre and Copet, Jade and Azhar, Faisal and Touvron, Hugo and Martin, Louis and Usunier, Nicolas and Scialom, Thomas and Synnaeve, Gabriel},
	file = {Rozière et al. - Code Llama Open Foundation Models for Code.pdf:/Users/yisu/Zotero/storage/UJEEYBTD/Rozière et al. - Code Llama Open Foundation Models for Code.pdf:application/pdf},
}

@inproceedings{oramas_deep_2017,
	address = {Como Italy},
	title = {A {Deep} {Multimodal} {Approach} for {Cold}-start {Music} {Recommendation}},
	isbn = {978-1-4503-5353-3},
	url = {https://dl.acm.org/doi/10.1145/3125486.3125492},
	doi = {10.1145/3125486.3125492},
	abstract = {An increasing amount of digital music is being published daily. Music streaming services o en ingest all available music, but this poses a challenge: how to recommend new artists for which prior knowledge is scarce? In this work we aim to address this so-called cold-start problem by combining text and audio information with user feedback data using deep network architectures. Our method is divided into three steps. First, artist embeddings are learned from biographies by combining semantics, text features, and aggregated usage data. Second, track embeddings are learned from the audio signal and available feedback data. Finally, artist and track embeddings are combined in a multimodal network. Results suggest that both spli ing the recommendation problem between feature levels (i.e., artist metadata and audio track), and merging feature embeddings in a multimodal approach improve the accuracy of the recommendations.},
	language = {en},
	urldate = {2023-08-25},
	booktitle = {Proceedings of the 2nd {Workshop} on {Deep} {Learning} for {Recommender} {Systems}},
	publisher = {ACM},
	author = {Oramas, Sergio and Nieto, Oriol and Sordo, Mohamed and Serra, Xavier},
	month = aug,
	year = {2017},
	pages = {32--37},
	file = {Oramas et al. - 2017 - A Deep Multimodal Approach for Cold-start Music Re.pdf:/Users/yisu/Zotero/storage/46922YUL/Oramas et al. - 2017 - A Deep Multimodal Approach for Cold-start Music Re.pdf:application/pdf},
}

@article{damji_learning_nodate,
	title = {Learning {Spark}, {Second} {Edition}},
	language = {en},
	author = {Damji, Jules S and Wenig, Brooke and Das, Tathagata and Lee, Denny},
	file = {Damji et al. - Learning Spark, Second Edition.pdf:/Users/yisu/Zotero/storage/FQ62FRA9/Damji et al. - Learning Spark, Second Edition.pdf:application/pdf},
}

@inproceedings{schluter_limits_2017,
	address = {Valencia, Spain},
	title = {The limits of automatic summarisation according to {ROUGE}},
	url = {https://aclanthology.org/E17-2007},
	abstract = {This paper discusses some central caveats of summarisation, incurred in the use of the ROUGE metric for evaluation, with respect to optimal solutions. The task is NP-hard, of which we give the first proof. Still, as we show empirically for three central benchmark datasets for the task, greedy algorithms empirically seem to perform optimally according to the metric. Additionally, overall quality assurance is problematic: there is no natural upper bound on the quality of summarisation systems, and even humans are excluded from performing optimal summarisation.},
	urldate = {2023-08-31},
	booktitle = {Proceedings of the 15th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Volume} 2, {Short} {Papers}},
	publisher = {Association for Computational Linguistics},
	author = {Schluter, Natalie},
	month = apr,
	year = {2017},
	pages = {41--45},
	file = {Full Text PDF:/Users/yisu/Zotero/storage/MHLZP6B4/Schluter - 2017 - The limits of automatic summarisation according to.pdf:application/pdf},
}

@inproceedings{lin_rouge_2004,
	address = {Barcelona, Spain},
	title = {{ROUGE}: {A} {Package} for {Automatic} {Evaluation} of {Summaries}},
	shorttitle = {{ROUGE}},
	url = {https://aclanthology.org/W04-1013},
	urldate = {2023-08-31},
	booktitle = {Text {Summarization} {Branches} {Out}},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Chin-Yew},
	month = jul,
	year = {2004},
	pages = {74--81},
	file = {Full Text PDF:/Users/yisu/Zotero/storage/XWC7G5EM/Lin - 2004 - ROUGE A Package for Automatic Evaluation of Summa.pdf:application/pdf},
}

@misc{shazeer_fast_2019,
	title = {Fast {Transformer} {Decoding}: {One} {Write}-{Head} is {All} {You} {Need}},
	shorttitle = {Fast {Transformer} {Decoding}},
	url = {http://arxiv.org/abs/1911.02150},
	abstract = {Multi-head attention layers, as used in the Transformer neural sequence model, are a powerful alternative to RNNs for moving information across and between sequences. While training these layers is generally fast and simple, due to parallelizability across the length of the sequence, incremental inference (where such paralleization is impossible) is often slow, due to the memory-bandwidth cost of repeatedly loading the large "keys" and "values" tensors. We propose a variant called multi-query attention, where the keys and values are shared across all of the different attention "heads", greatly reducing the size of these tensors and hence the memory bandwidth requirements of incremental decoding. We verify experimentally that the resulting models can indeed be much faster to decode, and incur only minor quality degradation from the baseline.},
	urldate = {2023-09-05},
	publisher = {arXiv},
	author = {Shazeer, Noam},
	month = nov,
	year = {2019},
	note = {arXiv:1911.02150 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/GV26IDAE/1911.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/6MYHDR3S/Shazeer - 2019 - Fast Transformer Decoding One Write-Head is All Y.pdf:application/pdf},
}

@misc{kopf_openassistant_2023,
	title = {{OpenAssistant} {Conversations} -- {Democratizing} {Large} {Language} {Model} {Alignment}},
	url = {http://arxiv.org/abs/2304.07327},
	abstract = {Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages distributed across 66,497 conversation trees, in 35 different languages, annotated with 461,292 quality ratings. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers. To demonstrate the OpenAssistant Conversations dataset's effectiveness, we present OpenAssistant, the first fully open-source large-scale instruction-tuned model to be trained on human data. A preference study revealed that OpenAssistant replies are comparably preferred to GPT-3.5-turbo (ChatGPT) with a relative winrate of 48.3\% vs. 51.7\% respectively. We release our code and data under fully permissive licenses.},
	urldate = {2023-09-06},
	publisher = {arXiv},
	author = {Köpf, Andreas and Kilcher, Yannic and von Rütte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi-Rui and Stevens, Keith and Barhoum, Abdullah and Duc, Nguyen Minh and Stanley, Oliver and Nagyfi, Richárd and ES, Shahul and Suri, Sameer and Glushkov, David and Dantuluri, Arnav and Maguire, Andrew and Schuhmann, Christoph and Nguyen, Huu and Mattick, Alexander},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07327 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, I.2},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/KCRQEIZZ/2304.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/ICXHY3UL/Köpf et al. - 2023 - OpenAssistant Conversations -- Democratizing Large.pdf:application/pdf},
}

@misc{su_one_2023,
	title = {One {Embedder}, {Any} {Task}: {Instruction}-{Finetuned} {Text} {Embeddings}},
	shorttitle = {One {Embedder}, {Any} {Task}},
	url = {http://arxiv.org/abs/2212.09741},
	abstract = {We introduce INSTRUCTOR, a new method for computing text embeddings given task instructions: every text input is embedded together with instructions explaining the use case (e.g., task and domain descriptions). Unlike encoders from prior work that are more specialized, INSTRUCTOR is a single embedder that can generate text embeddings tailored to different downstream tasks and domains, without any further training. We first annotate instructions for 330 diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive loss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are unseen during training), ranging from classification and information retrieval to semantic textual similarity and text generation evaluation. INSTRUCTOR, while having an order of magnitude fewer parameters than the previous best model, achieves state-of-the-art performance, with an average improvement of 3.4\% compared to the previous best results on the 70 diverse datasets. Our analysis suggests that INSTRUCTOR is robust to changes in instructions, and that instruction finetuning mitigates the challenge of training a single model on diverse datasets. Our model, code, and data are available at https://instructor-embedding.github.io.},
	urldate = {2023-09-07},
	publisher = {arXiv},
	author = {Su, Hongjin and Shi, Weijia and Kasai, Jungo and Wang, Yizhong and Hu, Yushi and Ostendorf, Mari and Yih, Wen-tau and Smith, Noah A. and Zettlemoyer, Luke and Yu, Tao},
	month = may,
	year = {2023},
	note = {arXiv:2212.09741 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/K8DT3M6Y/2212.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/JEYZ8QMU/Su et al. - 2023 - One Embedder, Any Task Instruction-Finetuned Text.pdf:application/pdf},
}

@article{kwiatkowski_natural_2019,
	title = {Natural {Questions}: {A} {Benchmark} for {Question} {Answering} {Research}},
	volume = {7},
	shorttitle = {Natural {Questions}},
	url = {https://aclanthology.org/Q19-1026},
	doi = {10.1162/tacl_a_00276},
	abstract = {We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.},
	urldate = {2023-09-08},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
	year = {2019},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {452--466},
	file = {Full Text PDF:/Users/yisu/Zotero/storage/SSFZA4XC/Kwiatkowski et al. - 2019 - Natural Questions A Benchmark for Question Answer.pdf:application/pdf},
}

@article{siriwardhana_improving_2023,
	title = {Improving the {Domain} {Adaptation} of {Retrieval} {Augmented} {Generation} ({RAG}) {Models} for {Open} {Domain} {Question} {Answering}},
	volume = {11},
	issn = {2307-387X},
	url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00530/114590/Improving-the-Domain-Adaptation-of-Retrieval},
	doi = {10.1162/tacl_a_00530},
	abstract = {Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain Question Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news. In this paper, we evaluate the impact of joint training of the retriever and generator components of RAG for the task of domain adaptation in ODQA. We propose RAG-end2end, an extension to RAG that can adapt to a domain-specific knowledge base by updating all components of the external knowledge base during training. In addition, we introduce an auxiliary training signal to inject more domain-specific knowledge. This auxiliary signal forces RAG-end2end to reconstruct a given sentence by accessing the relevant information from the external knowledge base. Our novel contribution is that, unlike RAG, RAG-end2end does joint training of the retriever and generator for the end QA task and domain adaptation. We evaluate our approach with datasets from three domains: COVID-19, News, and Conversations, and achieve significant performance improvements compared to the original RAG model. Our work has been open-sourced through the HuggingFace Transformers library, attesting to our work’s credibility and technical consistency.},
	language = {en},
	urldate = {2023-09-08},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Siriwardhana, Shamane and Weerasekera, Rivindu and Wen, Elliott and Kaluarachchi, Tharindu and Rana, Rajib and Nanayakkara, Suranga},
	month = jan,
	year = {2023},
	pages = {1--17},
	file = {Siriwardhana et al. - 2023 - Improving the Domain Adaptation of Retrieval Augme.pdf:/Users/yisu/Zotero/storage/EPJURXDT/Siriwardhana et al. - 2023 - Improving the Domain Adaptation of Retrieval Augme.pdf:application/pdf},
}

@misc{guu_realm_2020,
	title = {{REALM}: {Retrieval}-{Augmented} {Language} {Model} {Pre}-{Training}},
	shorttitle = {{REALM}},
	url = {http://arxiv.org/abs/2002.08909},
	abstract = {Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16\% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.},
	urldate = {2023-09-09},
	publisher = {arXiv},
	author = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
	month = feb,
	year = {2020},
	note = {arXiv:2002.08909 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/C3FQ2GEL/2002.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/HF2V43YZ/Guu et al. - 2020 - REALM Retrieval-Augmented Language Model Pre-Trai.pdf:application/pdf},
}

@misc{malkov_efficient_2018,
	title = {Efficient and robust approximate nearest neighbor search using {Hierarchical} {Navigable} {Small} {World} graphs},
	url = {http://arxiv.org/abs/1603.09320},
	abstract = {We present a new approach for the approximate K-nearest neighbor search based on navigable small world graphs with controllable hierarchy (Hierarchical NSW, HNSW). The proposed solution is fully graph-based, without any need for additional search structures, which are typically used at the coarse search stage of the most proximity graph techniques. Hierarchical NSW incrementally builds a multi-layer structure consisting from hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. The maximum layer in which an element is present is selected randomly with an exponentially decaying probability distribution. This allows producing graphs similar to the previously studied Navigable Small World (NSW) structures while additionally having the links separated by their characteristic distance scales. Starting search from the upper layer together with utilizing the scale separation boosts the performance compared to NSW and allows a logarithmic complexity scaling. Additional employment of a heuristic for selecting proximity graph neighbors significantly increases performance at high recall and in case of highly clustered data. Performance evaluation has demonstrated that the proposed general metric space search index is able to strongly outperform previous opensource state-of-the-art vector-only approaches. Similarity of the algorithm to the skip list structure allows straightforward balanced distributed implementation.},
	urldate = {2023-09-09},
	publisher = {arXiv},
	author = {Malkov, Yu A. and Yashunin, D. A.},
	month = aug,
	year = {2018},
	note = {arXiv:1603.09320 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval, Computer Science - Data Structures and Algorithms, Computer Science - Social and Information Networks},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/YS8SCBS5/1603.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/98CRELJE/Malkov and Yashunin - 2018 - Efficient and robust approximate nearest neighbor .pdf:application/pdf},
}

@article{li_competition-level_2022,
	title = {Competition-{Level} {Code} {Generation} with {AlphaCode}},
	volume = {378},
	issn = {0036-8075, 1095-9203},
	url = {http://arxiv.org/abs/2203.07814},
	doi = {10.1126/science.abq1158},
	abstract = {Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3\% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.},
	number = {6624},
	urldate = {2023-09-09},
	journal = {Science},
	author = {Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, Rémi and Eccles, Tom and Keeling, James and Gimeno, Felix and Lago, Agustin Dal and Hubert, Thomas and Choy, Peter and d'Autume, Cyprien de Masson and Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and Gowal, Sven and Cherepanov, Alexey and Molloy, James and Mankowitz, Daniel J. and Robson, Esme Sutherland and Kohli, Pushmeet and de Freitas, Nando and Kavukcuoglu, Koray and Vinyals, Oriol},
	month = dec,
	year = {2022},
	note = {arXiv:2203.07814 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Programming Languages},
	pages = {1092--1097},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/CGHEDWLD/2203.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/8DSAFWKF/Li et al. - 2022 - Competition-Level Code Generation with AlphaCode.pdf:application/pdf},
}

@inproceedings{bulian_tomayto_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Tomayto, {Tomahto}. {Beyond} {Token}-level {Answer} {Equivalence} for {Question} {Answering} {Evaluation}},
	url = {https://aclanthology.org/2022.emnlp-main.20},
	doi = {10.18653/v1/2022.emnlp-main.20},
	abstract = {The predictions of question answering (QA) systems are typically evaluated against manually annotated finite sets of one or more answers. This leads to a coverage limitation that results in underestimating the true performance of systems, and is typically addressed by extending over exact match (EM) with predefined rules or with the token-level F1 measure.In this paper, we present the first systematic conceptual and data-driven analysis to examine the shortcomings of token-level equivalence measures.To this end, we define the asymmetric notion of answer equivalence (AE), accepting answers that are equivalent to or improve over the reference, and publish over 23k human judgements for candidates produced by multiple QA systems on SQuAD.Through a careful analysis of this data, we reveal and quantify several concrete limitations of the F1 measure, such as a false impression of graduality, or missing dependence on the question.Since collecting AE annotations for each evaluated model is expensive, we learn a BERT matching (BEM) measure to approximate this task. Being a simpler task than QA, we find BEM to provide significantly better AE approximations than F1, and to more accurately reflect the performance of systems.Finally, we demonstrate the practical utility of AE and BEM on the concrete application of minimal accurate prediction sets, reducing the number of required answers by up to X2.6.},
	urldate = {2023-09-11},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Bulian, Jannis and Buck, Christian and Gajewski, Wojciech and Börschinger, Benjamin and Schuster, Tal},
	month = dec,
	year = {2022},
	pages = {291--305},
	file = {Full Text PDF:/Users/yisu/Zotero/storage/ZYSQCB4T/Bulian et al. - 2022 - Tomayto, Tomahto. Beyond Token-level Answer Equiva.pdf:application/pdf},
}

@misc{peinl_evaluation_2023,
	title = {Evaluation of medium-large {Language} {Models} at zero-shot closed book generative question answering},
	url = {http://arxiv.org/abs/2305.11991},
	abstract = {Large language models (LLMs) have garnered significant attention, but the definition of "large" lacks clarity. This paper focuses on medium-sized language models (MLMs), defined as having at least six billion parameters but less than 100 billion. The study evaluates MLMs regarding zero-shot generative question answering, which requires models to provide elaborate answers without external document retrieval. The paper introduces an own test dataset and presents results from human evaluation. Results show that combining the best answers from different MLMs yielded an overall correct answer rate of 82.7\% which is better than the 60.9\% of ChatGPT. The best MLM achieved 71.8\% and has 33B parameters, which highlights the importance of using appropriate training data for fine-tuning rather than solely relying on the number of parameters. More fine-grained feedback should be used to further improve the quality of answers. The open source community is quickly closing the gap to the best commercial models.},
	urldate = {2023-09-11},
	publisher = {arXiv},
	author = {Peinl, René and Wirth, Johannes},
	month = jul,
	year = {2023},
	note = {arXiv:2305.11991 [cs]},
	keywords = {Computer Science - Computation and Language, I.2.7},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/MRR4YQ37/2305.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/EBFY6TEH/Peinl and Wirth - 2023 - Evaluation of medium-large Language Models at zero.pdf:application/pdf},
}

@misc{adlakha_evaluating_2023,
	title = {Evaluating {Correctness} and {Faithfulness} of {Instruction}-{Following} {Models} for {Question} {Answering}},
	url = {http://arxiv.org/abs/2307.16877},
	abstract = {Retriever-augmented instruction-following models are attractive alternatives to fine-tuned approaches for information-seeking tasks such as question answering (QA). By simply prepending retrieved documents in its input along with an instruction, these models can be adapted to various information domains and tasks without additional fine-tuning. While the model responses tend to be natural and fluent, the additional verbosity makes traditional QA evaluation metrics such as exact match (EM) and F1 unreliable for accurately quantifying model performance. In this work, we investigate the performance of instruction-following models across three information-seeking QA tasks. We use both automatic and human evaluation to evaluate these models along two dimensions: 1) how well they satisfy the user's information need (correctness), and 2) whether they produce a response based on the provided knowledge (faithfulness). Guided by human evaluation and analysis, we highlight the shortcomings of traditional metrics for both correctness and faithfulness. We then propose simple token-overlap based and model-based metrics that reflect the true performance of these models. Our analysis reveals that instruction-following models are competitive, and sometimes even outperform fine-tuned models for correctness. However, these models struggle to stick to the provided knowledge and often hallucinate in their responses. We hope our work encourages a more holistic evaluation of instruction-following models for QA. Our code and data is available at https://github.com/McGill-NLP/instruct-qa},
	urldate = {2023-09-11},
	publisher = {arXiv},
	author = {Adlakha, Vaibhav and BehnamGhader, Parishad and Lu, Xing Han and Meade, Nicholas and Reddy, Siva},
	month = jul,
	year = {2023},
	note = {arXiv:2307.16877 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/9K6V33TL/2307.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/RTSHRP6T/Adlakha et al. - 2023 - Evaluating Correctness and Faithfulness of Instruc.pdf:application/pdf},
}

@misc{liang_holistic_2022,
	title = {Holistic {Evaluation} of {Language} {Models}},
	url = {http://arxiv.org/abs/2211.09110},
	abstract = {Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5\% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9\% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0\%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.},
	urldate = {2023-09-11},
	publisher = {arXiv},
	author = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and Ré, Christopher and Acosta-Navas, Diana and Hudson, Drew A. and Zelikman, Eric and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel and Zheng, Lucia and Yuksekgonul, Mert and Suzgun, Mirac and Kim, Nathan and Guha, Neel and Chatterji, Niladri and Khattab, Omar and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
	month = nov,
	year = {2022},
	note = {arXiv:2211.09110 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/5XJBQSPF/2211.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/7P6RLUE5/Liang et al. - 2022 - Holistic Evaluation of Language Models.pdf:application/pdf},
}

@misc{zhang_bertscore_2020,
	title = {{BERTScore}: {Evaluating} {Text} {Generation} with {BERT}},
	shorttitle = {{BERTScore}},
	url = {http://arxiv.org/abs/1904.09675},
	abstract = {We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics.},
	urldate = {2023-09-11},
	publisher = {arXiv},
	author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
	month = feb,
	year = {2020},
	note = {arXiv:1904.09675 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/F29XUN7K/1904.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/WYYM7FAT/Zhang et al. - 2020 - BERTScore Evaluating Text Generation with BERT.pdf:application/pdf},
}

@misc{shi_replug_2023,
	title = {{REPLUG}: {Retrieval}-{Augmented} {Black}-{Box} {Language} {Models}},
	shorttitle = {{REPLUG}},
	url = {http://arxiv.org/abs/2301.12652},
	abstract = {We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing retrieval and language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3\%, as well as the performance of Codex on five-shot MMLU by 5.1\%.},
	urldate = {2023-09-12},
	publisher = {arXiv},
	author = {Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Rich and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
	month = may,
	year = {2023},
	note = {arXiv:2301.12652 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/yisu/Zotero/storage/3I98GIBP/2301.html:text/html;Full Text PDF:/Users/yisu/Zotero/storage/DIE29GJQ/Shi et al. - 2023 - REPLUG Retrieval-Augmented Black-Box Language Mod.pdf:application/pdf},
}

@misc{noauthor_3-_nodate,
	title = {轩辕剑3外传-天之痕 for {Mac版本} 中文版 2021重制版 mac单机游戏 - {Mac版下载}{\textbar}{XMAC资源网}},
	url = {https://www.xmac.im/19_1760.html},
	urldate = {2023-09-16},
	file = {轩辕剑3外传-天之痕 for Mac版本 中文版 2021重制版 mac单机游戏 - Mac版下载|XMAC资源网:/Users/yisu/Zotero/storage/4YAHAXYH/19_1760.html:text/html},
}

@misc{noauthor_3-_nodate-1,
	title = {轩辕剑3外传-天之痕 for {Mac版本} 中文版 2021重制版 mac单机游戏 - {Mac版下载}{\textbar}{XMAC资源网}},
	url = {https://www.xmac.im/19_1760.html},
	urldate = {2023-09-16},
	file = {轩辕剑3外传-天之痕 for Mac版本 中文版 2021重制版 mac单机游戏 - Mac版下载|XMAC资源网:/Users/yisu/Zotero/storage/WBIDK8WJ/19_1760.html:text/html},
}

@article{yao_reac_2023,
	title = {{REAC} {T}: {SYNERGIZING} {REASONING} {AND} {ACTING} {IN} {LANGUAGE} {MODELS}},
	abstract = {While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-speciﬁc actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact veriﬁcation (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples.},
	language = {en},
	author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
	year = {2023},
	file = {Yao et al. - 2023 - REAC T SYNERGIZING REASONING AND ACTING IN LANGUA.pdf:/Users/yisu/Zotero/storage/R7I62X4X/Yao et al. - 2023 - REAC T SYNERGIZING REASONING AND ACTING IN LANGUA.pdf:application/pdf},
}
